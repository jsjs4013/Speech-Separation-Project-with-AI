{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-imperial",
   "metadata": {},
   "source": [
    "# Make wav list\n",
    "- 파일을 tr, cv, tt 폴더별로 무지성으로 읽어온 다음 각 폴더별 list를 .lst 파일로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sound-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tr_wav.lst format:\n",
    "...\n",
    "447o030v_0.1232_050c0109_-0.1232.wav\n",
    "447o030v_1.7882_444o0310_-1.7882.wav\n",
    "447o030w_0.52605_446o030e_-0.52605.wav\n",
    "447o030w_1.9272_420c0203_-1.9272.wav\n",
    "447o030x_0.03457_441c0209_-0.03457.wav\n",
    "447o030x_0.70879_420o0307_-0.70879.wav\n",
    "447o030x_0.98832_441o0308_-0.98832.wav\n",
    "447o030x_1.4783_422o030p_-1.4783.wav\n",
    "...\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "list_dir = './mycode/wsj0_2mix/use_this/lists/'\n",
    "\n",
    "wav_dir = wav_dir\n",
    "output_lst = list_dir\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-desperate",
   "metadata": {},
   "source": [
    "# Make TFRecord file\n",
    "\n",
    "- 여기서는 위에서 만든 리스트 파일을 가지고 tfrecord data로 변환함\n",
    "- 이 섹션의 맨 아래 블럭의 코드가 본 코드인데, 읽어들인 raw data를 stft하고, stft한거를 입력으로 세팅함\n",
    "- 그리고 mix된 stft data를 통해서 label들을 뽑아내고, 따로 gender값도 읽어들임\n",
    "- 이렇게 얻게된 3개의 값(mix_stft, labels, gender)을 TFRecord 형식으로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eastern-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy import signal\n",
    "import argparse\n",
    "import os, sys\n",
    "from numpy.fft import rfft, irfft\n",
    "from scipy.io.wavfile import write as wav_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entitled-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_axis(a, length, overlap=0, axis=None, end='cut', endvalue=0):\n",
    "    \"\"\"Generate a new array that chops the given array along the given axis into overlapping frames.\n",
    "    example:\n",
    "    >>> segment_axis(np.arange(10), 4, 2)\n",
    "    array([[0, 1, 2, 3],\n",
    "           [2, 3, 4, 5],\n",
    "           [4, 5, 6, 7],\n",
    "           [6, 7, 8, 9]])\n",
    "    arguments:\n",
    "    a       The array to segment\n",
    "    length  The length of each frame\n",
    "    overlap The number of array elements by which the frames should overlap\n",
    "    axis    The axis to operate on; if None, act on the flattened array\n",
    "    end     What to do with the last frame, if the array is not evenly\n",
    "            divisible into pieces. Options are:\n",
    "            'cut'   Simply discard the extra values\n",
    "            'wrap'  Copy values from the beginning of the array\n",
    "            'pad'   Pad with a constant value\n",
    "    endvalue    The value to use for end='pad'\n",
    "    The array is not copied unless necessary (either because it is\n",
    "    unevenly strided and being flattened or because end is set to\n",
    "    'pad' or 'wrap').\n",
    "    \"\"\"\n",
    "\n",
    "    if axis is None:\n",
    "        a = np.ravel(a)  # may copy\n",
    "        axis = 0\n",
    "\n",
    "    l = a.shape[axis]\n",
    "\n",
    "    if overlap >= length: raise ValueError(\n",
    "        \"frames cannot overlap by more than 100%\")\n",
    "    if overlap < 0 or length <= 0: raise ValueError(\n",
    "        \"overlap must be nonnegative and length must be positive\")\n",
    "\n",
    "    if l < length or (l - length) % (length - overlap):\n",
    "        if l > length:\n",
    "            roundup = length + (1 + (l - length) // (length - overlap)) * (\n",
    "                    length - overlap)\n",
    "            rounddown = length + ((l - length) // (length - overlap)) * (\n",
    "                    length - overlap)\n",
    "        else:\n",
    "            roundup = length\n",
    "            rounddown = 0\n",
    "        assert rounddown < l < roundup\n",
    "        assert roundup == rounddown + (length - overlap) or (\n",
    "                roundup == length and rounddown == 0)\n",
    "        a = a.swapaxes(-1, axis)\n",
    "\n",
    "        if end == 'cut':\n",
    "            a = a[..., :rounddown]\n",
    "        elif end in ['pad', 'wrap']:  # copying will be necessary\n",
    "            s = list(a.shape)\n",
    "            s[-1] = roundup\n",
    "            b = np.empty(s, dtype=a.dtype)\n",
    "            b[..., :l] = a\n",
    "            if end == 'pad':\n",
    "                b[..., l:] = endvalue\n",
    "            elif end == 'wrap':\n",
    "                b[..., l:] = a[..., :roundup - l]\n",
    "            a = b\n",
    "\n",
    "        a = a.swapaxes(-1, axis)\n",
    "\n",
    "    l = a.shape[axis]\n",
    "    if l == 0: raise ValueError(\n",
    "        \"Not enough data points to segment array in 'cut' mode; try 'pad' or 'wrap'\")\n",
    "    assert l >= length\n",
    "    assert (l - length) % (length - overlap) == 0\n",
    "    n = 1 + (l - length) // (length - overlap)\n",
    "    s = a.strides[axis]\n",
    "    newshape = a.shape[:axis] + (n, length) + a.shape[axis + 1:]\n",
    "    newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "\n",
    "    if not a.flags.contiguous:\n",
    "        a = a.copy()\n",
    "        newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)\n",
    "\n",
    "    try:\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)\n",
    "    except TypeError or ValueError:\n",
    "        warnings.warn(\"Problem with ndarray creation forces copy.\")\n",
    "        a = a.copy()\n",
    "        # Shape doesn't change but strides does\n",
    "        newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _samples_to_stft_frames(samples, size, shift):\n",
    "    \"\"\"\n",
    "    Calculates STFT frames from samples in time domain.\n",
    "    :param samples: Number of samples in time domain.\n",
    "    :param size: FFT size.\n",
    "    :param shift: Hop in samples.\n",
    "    :return: Number of STFT frames.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.ceil((float(samples) - size + shift) / shift).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stft_frames_to_samples(frames, size, shift):\n",
    "    \"\"\"\n",
    "    Calculates samples in time domain from STFT frames\n",
    "    :param frames: Number of STFT frames.\n",
    "    :param size: FFT size.\n",
    "    :param shift: Hop in samples.\n",
    "    :return: Number of samples in time domain.\n",
    "    \"\"\"\n",
    "    return frames * shift + size - shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handled-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(time_signal, time_dim=None, size=1024, shift=256,\n",
    "         window=signal.blackman, fading=True, window_length=None):\n",
    "    \"\"\"\n",
    "    Calculates the short time Fourier transformation of a multi channel multi\n",
    "    speaker time signal. It is able to add additional zeros for fade-in and\n",
    "    fade out and should yield an STFT signal which allows perfect\n",
    "    reconstruction.\n",
    "    :param time_signal: multi channel time signal.\n",
    "    :param time_dim: Scalar dim of time.\n",
    "        Default: None means the biggest dimension\n",
    "    :param size: Scalar FFT-size.\n",
    "    :param shift: Scalar FFT-shift. Typically shift is a fraction of size.\n",
    "    :param window: Window function handle.\n",
    "    :param fading: Pads the signal with zeros for better reconstruction.\n",
    "    :param window_length: Sometimes one desires to use a shorter window than\n",
    "        the fft size. In that case, the window is padded with zeros.\n",
    "        The default is to use the fft-size as a window size.\n",
    "    :return: Single channel complex STFT signal\n",
    "        with dimensions frames times size/2+1.\n",
    "    \"\"\"\n",
    "    if time_dim is None:\n",
    "        time_dim = np.argmax(time_signal.shape)\n",
    "\n",
    "    # Pad with zeros to have enough samples for the window function to fade.\n",
    "    if fading:\n",
    "        pad = [(0, 0)] * time_signal.ndim\n",
    "        pad[time_dim] = [size - shift, size - shift]\n",
    "        time_signal = np.pad(time_signal, pad, mode='constant')\n",
    "\n",
    "    # Pad with trailing zeros, to have an integral number of frames.\n",
    "    frames = _samples_to_stft_frames(time_signal.shape[time_dim], size, shift)\n",
    "    samples = _stft_frames_to_samples(frames, size, shift)\n",
    "    pad = [(0, 0)] * time_signal.ndim\n",
    "    pad[time_dim] = [0, samples - time_signal.shape[time_dim]]\n",
    "    time_signal = np.pad(time_signal, pad, mode='constant')\n",
    "    \n",
    "\n",
    "    if window_length is None:\n",
    "        window = window(size)\n",
    "    else:\n",
    "        window = window(window_length)\n",
    "        window = np.pad(window, (0, size - window_length), mode='constant')\n",
    "\n",
    "    time_signal_seg = segment_axis(time_signal, size,\n",
    "                                   size - shift, axis=time_dim)\n",
    "\n",
    "    letters = string.ascii_lowercase\n",
    "    mapping = letters[:time_signal_seg.ndim] + ',' + letters[time_dim + 1] \\\n",
    "              + '->' + letters[:time_signal_seg.ndim]\n",
    "\n",
    "    return rfft(np.einsum(mapping, time_signal_seg, window), axis=time_dim + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioread(path, offset=0.0, duration=None, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Reads a wav file, converts it to 32 bit float values and reshapes accoring\n",
    "    to the number of channels.\n",
    "    Now, this is a wrapper of librosa with our common defaults.\n",
    "    :param path: Absolute or relative file path to audio file.\n",
    "    :type: String.\n",
    "    :param offset: Begin of loaded audio.\n",
    "    :type: Scalar in seconds.\n",
    "    :param duration: Duration of loaded audio.\n",
    "    :type: Scalar in seconds.\n",
    "    :param sample_rate: Sample rate of audio\n",
    "    :type: scalar in number of samples per second\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    signal = librosa.load(path, sr=sample_rate, mono=False, offset=offset, duration=duration)\n",
    "    \n",
    "    return signal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "yellow-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import librosa.display\n",
    "from math import ceil\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "list_dir = './mycode/wsj0_2mix/use_this/lists/'\n",
    "tfrecord_dir = './mycode/tfrecords/'\n",
    "gender_list = './wsj0-train-spkrinfo.txt'\n",
    "process_num = 8\n",
    "\n",
    "CASE = 'mixed' # mixed or signal\n",
    "\n",
    "mkdir_p(tfrecord_dir) # tfrecord_dir 폴더 만드는 코드\n",
    "sample_rate = 8000\n",
    "window_size = 256\n",
    "window_shift = 128\n",
    "\n",
    "# if gender_list is not '':\n",
    "#     apply_gender_info = True\n",
    "#     gender_dict = {}\n",
    "#     fid = open(gender_list, 'r')\n",
    "#     lines = fid.readlines()\n",
    "#     fid.close()\n",
    "#     for line in lines:\n",
    "#         spk = line.strip('\\n').split(' ')[0]\n",
    "#         gender = line.strip('\\n').split(' ')[1]\n",
    "#         if gender.lower() == 'm':\n",
    "#             gender_dict[spk] = 1\n",
    "#         else:\n",
    "#             gender_dict[spk] = 0\n",
    "\n",
    "\n",
    "def max_length(file, name, mix_or_not):\n",
    "    max_len = 0\n",
    "    \n",
    "    for name in lines:\n",
    "        name = name.strip('\\n')\n",
    "\n",
    "        wav_name = wav_dir + file + '/' + mix_or_not + '/' + name\n",
    "        audio_wav = audioread(wav_name, offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        mix_len = len(audio_wav)\n",
    "\n",
    "        if mix_len > max_len:\n",
    "            max_len = mix_len\n",
    "    \n",
    "    # 초 맞춰주는 부분\n",
    "    max_len = ceil(max_len / sample_rate) * sample_rate\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "\n",
    "def make_sequence_example(inputs, labels, length, name, genders=False):\n",
    "    input_features = [tf.train.Feature(float_list=tf.train.FloatList(value=input_)) for input_ in inputs]\n",
    "    label_features = [tf.train.Feature(float_list=tf.train.FloatList(value=label)) for label in labels]\n",
    "    len_feature = [tf.train.Feature(float_list=tf.train.FloatList(value=[length]))]\n",
    "    name_feature = [tf.train.Feature(bytes_list=tf.train.BytesList(value=[name.encode('utf-8')]))]\n",
    "#     gender_features = [tf.train.Feature(float_list=tf.train.FloatList(value=genders))]\n",
    "    \n",
    "    feature_list = {\n",
    "        'inputs': tf.train.FeatureList(feature=input_features),\n",
    "        'labels': tf.train.FeatureList(feature=label_features),\n",
    "        'length': tf.train.FeatureList(feature=len_feature),\n",
    "        'name' : tf.train.FeatureList(feature=name_feature)\n",
    "#         'genders': tf.train.FeatureList(feature=gender_features)\n",
    "    }\n",
    "    feature_lists = tf.train.FeatureLists(feature_list=feature_list)\n",
    "    \n",
    "    return tf.train.SequenceExample(feature_lists=feature_lists)\n",
    "\n",
    "\n",
    "def gen_feats(wav_name, sample_rate, window_size, window_shift, file, max_len, case='mixed'):\n",
    "    mix_wav_name = wav_dir + file + '/mix/' + wav_name\n",
    "    s1_wav_name  = wav_dir + file + '/s1/' + wav_name\n",
    "    s2_wav_name  = wav_dir + file + '/s2/' + wav_name\n",
    "\n",
    "    # value initiallization\n",
    "    mix_wav = 0\n",
    "    s1_wav = 0\n",
    "    s2_wav = 0\n",
    "    mix_stft = 0\n",
    "    s1_stft = 0\n",
    "    s2_stft = 0\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        # ------- AUDIO READ -------\n",
    "        mix_wav = audioread(mix_wav_name, offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s1_wav  = audioread(s1_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s2_wav  = audioread(s2_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        # --------------------------\n",
    "        \n",
    "        # ------- AUDIO PAD -------\n",
    "        mix_wav_pad = np.pad(mix_wav, (0, max_len - len(mix_wav)), 'constant', constant_values=(0))\n",
    "        s1_wav_pad = np.pad(s1_wav, (0, max_len - len(s1_wav)), 'constant', constant_values=(0))\n",
    "        s2_wav_pad = np.pad(s2_wav, (0, max_len - len(s2_wav)), 'constant', constant_values=(0))\n",
    "        # -------------------------\n",
    "\n",
    "        # ------- STFT -------\n",
    "        mix_stft = stft(mix_wav, time_dim=0, size=window_size, shift=window_shift)\n",
    "        \n",
    "        mix_stft_pad = stft(mix_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s1_stft_pad = stft(s1_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft_pad = stft(s2_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        # --------------------\n",
    "        \n",
    "        part_name = os.path.splitext(wav_name)[0]\n",
    "        tfrecords_name = tfrecord_dir + file + '_tfrecord\\\\' + part_name + '.tfrecords'\n",
    "        \n",
    "        with tf.io.TFRecordWriter(tfrecords_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_name)\n",
    "\n",
    "            mix_abs = np.abs(mix_stft_pad)\n",
    "            mix_angle = np.angle(mix_stft_pad)\n",
    "\n",
    "            s1_abs = np.abs(s1_stft_pad)\n",
    "            s1_angle = np.angle(s1_stft_pad)\n",
    "\n",
    "            s2_abs = np.abs(s2_stft_pad)\n",
    "            s2_angle = np.angle(s2_stft_pad)\n",
    "\n",
    "            inputs = np.concatenate((mix_abs, mix_angle), axis=1)\n",
    "            labels = np.concatenate((s1_abs * np.cos(mix_angle - s1_angle), s2_abs * np.cos(mix_angle - s2_angle)), axis=1)\n",
    "            \n",
    "            ex = make_sequence_example(inputs, labels, mix_stft.shape[0], part_name)\n",
    "            writer.write(ex.SerializeToString())\n",
    "    else:\n",
    "        # ------- AUDIO READ -------\n",
    "        s1_wav  = audioread(s1_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s2_wav  = audioread(s2_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        # --------------------------\n",
    "        \n",
    "        # ------- AUDIO PAD -------\n",
    "        s1_wav_pad = np.pad(s1_wav, (0, max_len - len(s1_wav)), 'constant', constant_values=(0))\n",
    "        s2_wav_pad = np.pad(s2_wav, (0, max_len - len(s2_wav)), 'constant', constant_values=(0))\n",
    "        # -------------------------\n",
    "        \n",
    "        # ------- STFT -------\n",
    "        s1_stft  = stft(s1_wav,  time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft  = stft(s2_wav,  time_dim=0, size=window_size, shift=window_shift)\n",
    "        \n",
    "        s1_stft_pad = stft(s1_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft_pad = stft(s2_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        # --------------------\n",
    "        \n",
    "        part_name = os.path.splitext(wav_name)[0]\n",
    "        tfrecords_s1_name = tfrecord_dir + file + '_one_source_tfrecord\\\\' + part_name + '_s1.tfrecords'\n",
    "        tfrecords_s2_name = tfrecord_dir + file + '_one_source_tfrecord\\\\' + part_name + '_s2.tfrecords'\n",
    "        \n",
    "        with tf.io.TFRecordWriter(tfrecords_s1_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_s1_name)\n",
    "            \n",
    "            s1_abs = np.abs(s1_stft_pad)\n",
    "            s1_angle = np.angle(s1_stft_pad)\n",
    "            \n",
    "            ex = make_sequence_example(s1_abs, s1_angle, s1_stft.shape[0], part_name + '_s1')\n",
    "            writer.write(ex.SerializeToString())\n",
    "\n",
    "        with tf.io.TFRecordWriter(tfrecords_s2_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_s2_name)\n",
    "            \n",
    "            s2_abs = np.abs(s2_stft_pad)\n",
    "            s2_angle = np.angle(s2_stft_pad)\n",
    "            \n",
    "            ex = make_sequence_example(s2_abs, s2_angle, s2_stft.shape[0], part_name + '_s2')\n",
    "            writer.write(ex.SerializeToString())\n",
    "\n",
    "\n",
    "# 여기 멀티프로세싱 pool 적용 어케하는지 모르게씀\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    max_len = 0\n",
    "    \n",
    "    output_lst_files = list_dir + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    \n",
    "    \n",
    "    if CASE == 'mixed':\n",
    "        for name in lines:\n",
    "            name = name.strip('\\n')\n",
    "            max_len = max_length(files, name, 'mix')\n",
    "    else:\n",
    "        for name in lines:\n",
    "            name = name.strip('\\n')\n",
    "            max_len1 = max_length(files, name, 's1')\n",
    "            max_len2 = max_length(files, name, 's2')\n",
    "        \n",
    "        if max_len1 >= max_len2:\n",
    "            max_len = max_len1\n",
    "        else:\n",
    "            max_len = max_len2\n",
    "    \n",
    "    \n",
    "    mkdir_p(tfrecord_dir + files + '_tfrecord') # tfrecord_dir 폴더 만드는 코드\n",
    "    mkdir_p(tfrecord_dir + files + '_one_source_tfrecord') # one_source_tfrecord 폴더 만드는 코드\n",
    "    for name in lines:\n",
    "        name = name.strip('\\n')\n",
    "        gen_feats(name, sample_rate, window_size, window_shift, files, max_len, CASE)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-cuisine",
   "metadata": {},
   "source": [
    "# Deep learning part\n",
    "## 1. Data Loader\n",
    "- Data를 시바 읽어오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "urban-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 2\n",
    "INPUT_SIZE = 129\n",
    "OUTPUT_SIZE = 129\n",
    "\n",
    "CASE = 'mixed' # mixed or signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "guilty-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "if CASE == 'mixed':\n",
    "    tr_path = './mycode/tfrecords/tr_tfrecord/*.tfrecords'\n",
    "    val_path = './mycode/tfrecords/cv_tfrecord/*.tfrecords'\n",
    "    tt_path = './mycode/tfrecords/tt_tfrecord/*.tfrecords'\n",
    "else:\n",
    "    tr_path = './mycode/tfrecords/tr_one_source_tfrecord/*.tfrecords'\n",
    "    val_path = './mycode/tfrecords/cv_one_source_tfrecord/*.tfrecords'\n",
    "    tt_path = './mycode/tfrecords/tt_one_source_tfrecord/*.tfrecords'\n",
    "\n",
    "FILENAMES_TRAINING = tf.io.gfile.glob(tr_path)\n",
    "FILENAMES_VALIDATION = tf.io.gfile.glob(val_path)\n",
    "FILENAMES_TEST = tf.io.gfile.glob(tt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "satellite-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFRecord Files: 4\n",
      "Validation TFRecord Files: 4\n",
      "Test TFRecord Files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_0.62948_441c0212_-0.62948.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_1.3388_22ho010i_-1.3388.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_2.1067_422o030k_-2.1067.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0303_0.14144_441c0212_-0.14144.tfrecords']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train TFRecord Files:\", len(FILENAMES_TRAINING))\n",
    "print(\"Validation TFRecord Files:\", len(FILENAMES_VALIDATION))\n",
    "print(\"Test TFRecord Files:\", len(FILENAMES_TEST))\n",
    "FILENAMES_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "economic-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data, check, input_size=129*2, output_size=129*2):\n",
    "    if check == 'inputs':\n",
    "        inputs = tf.slice(data, [0, 0], [-1, input_size//2])\n",
    "        angle = tf.slice(data, [0, input_size//2], [-1, -1])\n",
    "        \n",
    "        return inputs, angle\n",
    "    \n",
    "    elif check == 'labels':\n",
    "        label1 = tf.slice(data, [0, 0], [-1, output_size//2])\n",
    "        label2 = tf.slice(data, [0, output_size//2], [-1, -1])\n",
    "        \n",
    "        return label1, label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "historic-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example, input_size=129*2, output_size=129*2, case='mixed'):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'inputs': tf.io.FixedLenSequenceFeature(shape=[input_size], dtype=tf.float32),\n",
    "            'labels': tf.io.FixedLenSequenceFeature(shape=[output_size], dtype=tf.float32),\n",
    "            'length': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32),\n",
    "            'name': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.string),\n",
    "#             'genders': tf.io.FixedLenSequenceFeature(shape=[2], dtype=tf.float32, allow_missing=True)\n",
    "        }\n",
    "    )\n",
    "    _, example = tf.io.parse_single_sequence_example(example, sequence_features=tfrecord_format)\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        inputs, angle = data_preprocessing(example[\"inputs\"], 'inputs', input_size)\n",
    "    #     label1, label2 = data_preprocessing(example[\"labels\"], 'labels', input_size)\n",
    "\n",
    "        tiled = tf.tile(tf.expand_dims(example['length'], 1), [1, input_size])\n",
    "        \n",
    "        # 여기가 변경된 부분(length와 label을 concat 함)\n",
    "        return inputs, tf.concat([example['labels'], tiled], 0)\n",
    "#         return inputs, example['labels']\n",
    "    \n",
    "    else:\n",
    "        return example['inputs'], example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "gross-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord_test(example, input_size=129*2, output_size=129*2, case='mixed'):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'inputs': tf.io.FixedLenSequenceFeature(shape=[input_size], dtype=tf.float32),\n",
    "            'labels': tf.io.FixedLenSequenceFeature(shape=[output_size], dtype=tf.float32),\n",
    "            'length': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32),\n",
    "            'name': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.string),\n",
    "#             'genders': tf.io.FixedLenSequenceFeature(shape=[2], dtype=tf.float32, allow_missing=True)\n",
    "        }\n",
    "    )\n",
    "    _, example = tf.io.parse_single_sequence_example(example, sequence_features=tfrecord_format)\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        inputs, angle = data_preprocessing(example[\"inputs\"], 'inputs', input_size)\n",
    "\n",
    "        return inputs, angle, example['labels'], example['name']\n",
    "    \n",
    "    else:\n",
    "        return example['inputs'], example['labels'], example['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "agricultural-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, input_size=129*2, output_size=129*2, check='train', case='mixed'):\n",
    "    ignore_order = tf.data.Options()\n",
    "    \n",
    "    if check == 'train':\n",
    "        ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    else:\n",
    "        ignore_order.experimental_deterministic = True\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames\n",
    "    )  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(\n",
    "        ignore_order\n",
    "    )  # uses data as soon as it streams in, rather than in its original order\n",
    "    \n",
    "    if check == 'train':\n",
    "        if case == 'mixed':\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord, input_size=input_size, output_size=output_size), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord, input_size=input_size//2, output_size=output_size//2, case=case), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "    else:\n",
    "        if case == 'mixed':\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord_test, input_size=input_size, output_size=output_size), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord_test, input_size=input_size//2, output_size=output_size//2, case=case), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "interracial-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filenames, input_size=129*2, output_size=129*2):\n",
    "    dataset = load_dataset(filenames, input_size, output_size, case=CASE)\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(None))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "essential-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_test(filenames, input_size=129*2, output_size=129*2):\n",
    "    dataset = load_dataset(filenames, input_size, output_size, check='test', case=CASE)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(None))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "comprehensive-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(FILENAMES_TRAINING, INPUT_SIZE*2, OUTPUT_SIZE*2)\n",
    "valid_dataset = get_dataset(FILENAMES_VALIDATION, INPUT_SIZE*2, OUTPUT_SIZE*2)\n",
    "\n",
    "test_dataset = get_dataset_for_test(FILENAMES_TEST, INPUT_SIZE*2, OUTPUT_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "accessory-switzerland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 627, 258) (2, 626, 258) (2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "a, b = next(iter(train_dataset))\n",
    "c1 = tf.slice(b, [0, 0, 0], [-1, b.shape[1]-1, -1])\n",
    "d1 = tf.slice(b, [0, b.shape[1]-1, 0], [-1, -1, 1])\n",
    "\n",
    "# tf.reduce_mean(tf.cast(tf.math.equal(lab, c1), dtype=tf.float32))\n",
    "print(b.shape, c1.shape, d1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-grenada",
   "metadata": {},
   "source": [
    "## 2. Building model\n",
    "\n",
    "- 이제 우리 모델을 시바 개쩔게 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "surface-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys, os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, LSTM, Concatenate, Multiply, Bidirectional, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "sunrise-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "qualified-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PIT loss\n",
    "\n",
    "def pit_with_outputsize(output_size):\n",
    "    def pit_loss(y_true, y_pred):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Label value slice\n",
    "        labels1 = tf.slice(labels, [0, 0, 0], [-1, -1, output_size])\n",
    "        labels2 = tf.slice(labels, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Predict value slice\n",
    "        pred1 = tf.slice(y_pred, [0, 0, 0], [-1, -1, output_size])\n",
    "        pred2 = tf.slice(y_pred, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Permute calculate\n",
    "        cost1 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred1 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred2 - labels2, 2), 1), 1)\n",
    "        cost2 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred2 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred1 - labels2, 2), 1), 1)\n",
    "\n",
    "        idx = tf.cast(cost1 > cost2, tf.float32) \n",
    "        pit_loss = tf.reduce_sum(idx * cost2 + (1 - idx) * cost1)\n",
    "        \n",
    "        return pit_loss\n",
    "    \n",
    "    return pit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "colonial-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "\n",
    "def uPIT(input_size, output_size, batch):\n",
    "    inputs = Input(shape=(None, input_size))\n",
    "    \n",
    "    outputs = Dense(496, activation = 'tanh')(inputs)\n",
    "    \n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True),\n",
    "                           input_shape=(None, 496,))(outputs)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    \n",
    "    pred1 = Dense(output_size, activation = 'relu')(drop)\n",
    "    pred2 = Dense(output_size, activation = 'relu')(drop)\n",
    "    \n",
    "    cleaned1 = Multiply()([pred1, inputs])\n",
    "    cleaned2 = Multiply()([pred2, inputs])\n",
    "    \n",
    "    model = Concatenate()([cleaned1, cleaned2])\n",
    "    \n",
    "    model = keras.Model(inputs, model)\n",
    "    \n",
    "    model.summary()\n",
    "    adam = tf.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=pit_with_outputsize(output_size), optimizer=adam)\n",
    "#     model.compile(loss=keras.losses.mean_squared_error, optimizer=adam)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-jumping",
   "metadata": {},
   "source": [
    "## 3. Training model\n",
    "- 구축한 모델을 기반으로 딥러닝 학습을 진행하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "ordered-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "dress-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, None, 129)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, None, 496)    64480       input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional (None, None, 992)    3940224     dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, None, 992)    0           bidirectional_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional (None, None, 992)    5908352     dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, None, 992)    0           bidirectional_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional (None, None, 992)    5908352     dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, None, 992)    0           bidirectional_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, None, 129)    128097      dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, None, 129)    128097      dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_22 (Multiply)          (None, None, 129)    0           dense_34[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_23 (Multiply)          (None, None, 129)    0           dense_35[0][0]                   \n",
      "                                                                 input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, 258)    0           multiply_22[0][0]                \n",
      "                                                                 multiply_23[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 16,077,602\n",
      "Trainable params: 16,077,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 48s 24s/step - loss: 554.3282 - val_loss: 457.1932\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 457.19324, saving model to ./CKPT\\CKP_ep_1__loss_457.19324_.h5\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 39s 22s/step - loss: 471.7826 - val_loss: 383.5308\n",
      "\n",
      "Epoch 00002: val_loss improved from 457.19324 to 383.53082, saving model to ./CKPT\\CKP_ep_2__loss_383.53082_.h5\n"
     ]
    }
   ],
   "source": [
    "# Training part\n",
    "\n",
    "epoch = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     model = load_model('./CKPT/CKP_ep_29__loss_102.63367_.h5', custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "    \n",
    "    model = uPIT(INPUT_SIZE, OUTPUT_SIZE, BATCH_SIZE)\n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-boutique",
   "metadata": {},
   "source": [
    "## 4. Training and validation loss plot\n",
    "- 학습한 모델의 loss값을 그래프로 그려봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "southeast-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and validation loss graph\n",
    "\n",
    "def graph_util(history):\n",
    "    fig = plt.figure()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.plot(history.history['loss'], c='b')\n",
    "    plt.plot(history.history['val_loss'], c='r')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training loss', 'validation loss'], loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "important-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_util(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-royal",
   "metadata": {},
   "source": [
    "## 5. Test Model\n",
    "- 학습된 모델을 사용해서 테스트를 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handed-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "window_size = 256\n",
    "window_shift = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dramatic-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "attempted-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _biorthogonal_window_loopy(analysis_window, shift):\n",
    "    \"\"\"\n",
    "    This version of the synthesis calculation is as close as possible to the\n",
    "    Matlab impelementation in terms of variable names.\n",
    "    The results are equal.\n",
    "    The implementation follows equation A.92 in\n",
    "    Krueger, A. Modellbasierte Merkmalsverbesserung zur robusten automatischen\n",
    "    Spracherkennung in Gegenwart von Nachhall und Hintergrundstoerungen\n",
    "    Paderborn, Universitaet Paderborn, Diss., 2011, 2011\n",
    "    \"\"\"\n",
    "    fft_size = len(analysis_window)\n",
    "    assert np.mod(fft_size, shift) == 0\n",
    "    number_of_shifts = len(analysis_window) // shift\n",
    "\n",
    "    sum_of_squares = np.zeros(shift)\n",
    "    for synthesis_index in range(0, shift):\n",
    "        for sample_index in range(0, number_of_shifts + 1):\n",
    "            analysis_index = synthesis_index + sample_index * shift\n",
    "\n",
    "            if analysis_index + 1 < fft_size:\n",
    "                sum_of_squares[synthesis_index] \\\n",
    "                    += analysis_window[analysis_index] ** 2\n",
    "\n",
    "    sum_of_squares = np.kron(np.ones(number_of_shifts), sum_of_squares)\n",
    "    synthesis_window = analysis_window / sum_of_squares / fft_size\n",
    "    return synthesis_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "north-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def istft(stft_signal, size=1024, shift=256,\n",
    "          window=signal.blackman, fading=True, window_length=None):\n",
    "    \"\"\"\n",
    "    Calculated the inverse short time Fourier transform to exactly reconstruct\n",
    "    the time signal.\n",
    "    :param stft_signal: Single channel complex STFT signal\n",
    "        with dimensions frames times size/2+1.\n",
    "    :param size: Scalar FFT-size.\n",
    "    :param shift: Scalar FFT-shift. Typically shift is a fraction of size.\n",
    "    :param window: Window function handle.\n",
    "    :param fading: Removes the additional padding, if done during STFT.\n",
    "    :param window_length: Sometimes one desires to use a shorter window than\n",
    "        the fft size. In that case, the window is padded with zeros.\n",
    "        The default is to use the fft-size as a window size.\n",
    "    :return: Single channel complex STFT signal\n",
    "    :return: Single channel time signal.\n",
    "    \"\"\"\n",
    "    assert stft_signal.shape[1] == size // 2 + 1\n",
    "\n",
    "    if window_length is None:\n",
    "        window = window(size)\n",
    "    else:\n",
    "        window = window(window_length)\n",
    "        window = np.pad(window, (0, size - window_length), mode='constant')\n",
    "\n",
    "    window = _biorthogonal_window_loopy(window, shift)\n",
    "\n",
    "    # Why? Line created by Hai, Lukas does not know, why it exists.\n",
    "    window *= size\n",
    "    time_signal = scipy.zeros(stft_signal.shape[0] * shift + size - shift)\n",
    "\n",
    "    for j, i in enumerate(range(0, len(time_signal) - size + shift, shift)):\n",
    "        time_signal[i:i + size] += window * np.real(irfft(stft_signal[j]))\n",
    "\n",
    "    # Compensate fade-in and fade-out\n",
    "    if fading:\n",
    "        time_signal = time_signal[size - shift:len(time_signal) - (size - shift)]\n",
    "\n",
    "    return time_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "detailed-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "textile-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6236: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _ctx, \"Mul\", name, x, y)\n",
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: scipy.zeros is deprecated and will be removed in SciPy 2.0.0, use numpy.zeros instead\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model_path = './CKPT/CKP_ep_2__loss_411.15891_.h5'\n",
    "    model = load_model(model_path, custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "\n",
    "    cnt = 0\n",
    "    check = 0\n",
    "    for batch in test_dataset:\n",
    "        input_batch, angle_batch, label_batch, name = batch\n",
    "        tf.executing_eagerly() # requires r1.7\n",
    "        angle_numpy = tf.constant(angle_batch)\n",
    "        angle_numpy = angle_numpy.numpy()\n",
    "\n",
    "        result = model.predict(input_batch)\n",
    "        label1 = tf.slice(result, [0, 0, 0], [-1, -1, OUTPUT_SIZE])\n",
    "        label2 = tf.slice(result, [0, 0, OUTPUT_SIZE], [-1, -1, -1])\n",
    "        spec1 = label1 * np.exp(angle_numpy * 1j)\n",
    "        spec2 = label2 * np.exp(angle_numpy * 1j)\n",
    "\n",
    "        num = cnt * BATCH_SIZE\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if i >= input_batch.shape[0]:\n",
    "                check = -1\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                wav_name = name[i][0].numpy().decode('utf-8')\n",
    "\n",
    "                wav_name1 = './test_wav/' + wav_name + '_s1.wav'\n",
    "                wav_name2 = './test_wav/' + wav_name + '_s2.wav'\n",
    "                wav1 = istft(spec1[i, 0:input_batch[i].shape[0], :], size=window_size, shift=window_shift)\n",
    "                wav2 = istft(spec2[i, 0:input_batch[i].shape[0], :], size=window_size, shift=window_shift)\n",
    "                audiowrite(wav1, wav_name1, sample_rate, True, True)\n",
    "                audiowrite(wav2, wav_name2, sample_rate, True, True)\n",
    "        \n",
    "        if check == -1:\n",
    "            break\n",
    "\n",
    "        if (cnt + 1) % 10 == 0:\n",
    "            print((cnt + 1) * BATCH_SIZE)\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-africa",
   "metadata": {},
   "source": [
    "- 원본(ref)과 모델을 통해 만들어진 파일(est)을 SI-SDR SDR과 같이 정확도를 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "natural-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "agreed-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "test_dir = './test_wav/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liable-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SI-SDR (db) : -7.791285775601864\n",
      "The SDR (db) : -2.3582835316785915\n"
     ]
    }
   ],
   "source": [
    "si_sdr = evaluate_metrics.eval_si_sdr(wav_dir, test_dir)\n",
    "sdr = evaluate_metrics.eval_sdr(wav_dir, test_dir)\n",
    "\n",
    "print(\"The SI-SDR (db) :\", si_sdr)\n",
    "print(\"The SDR (db) :\", sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-naples",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
