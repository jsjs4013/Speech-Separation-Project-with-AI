{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, use_enc=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.use_enc = use_enc\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax()\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        if self.use_enc:\n",
    "            one_hot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "            \n",
    "            return one_hot\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_1 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "2/2 [==============================] - 4s 2s/step - loss: 629.2358 - Si-sdr: -199.2693 - val_loss: 627.7835 - val_Si-sdr: -221.8050\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 52.84876\n",
      "Epoch 2/200\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 627.9673 - Si-sdr: -215.4507 - val_loss: 628.0581 - val_Si-sdr: -232.8337\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 52.84876\n",
      "Epoch 3/200\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 627.9014 - Si-sdr: -266.4165 - val_loss: 627.3989 - val_Si-sdr: -224.1950\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 52.84876\n",
      "Epoch 4/200\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 627.3158 - Si-sdr: -219.8372 - val_loss: 627.2649 - val_Si-sdr: -197.4704\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 52.84876\n",
      "Epoch 5/200\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 627.2963 - Si-sdr: -226.2106 - val_loss: 627.3264 - val_Si-sdr: -202.3037\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 52.84876\n",
      "Epoch 6/200\n",
      "2/2 [==============================] - 1s 503ms/step - loss: 627.2686 - Si-sdr: -210.6120 - val_loss: 627.2240 - val_Si-sdr: -211.8574\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 52.84876\n",
      "Epoch 7/200\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 627.2390 - Si-sdr: -238.8459 - val_loss: 627.2020 - val_Si-sdr: -190.7103\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 52.84876\n",
      "Epoch 8/200\n",
      "2/2 [==============================] - 1s 529ms/step - loss: 627.2277 - Si-sdr: -210.7080 - val_loss: 627.2249 - val_Si-sdr: -211.1942\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 52.84876\n",
      "Epoch 9/200\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 627.1656 - Si-sdr: -187.0074 - val_loss: 627.1806 - val_Si-sdr: -215.5965\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 52.84876\n",
      "Epoch 10/200\n",
      "2/2 [==============================] - 1s 497ms/step - loss: 627.1755 - Si-sdr: -189.6000 - val_loss: 627.1656 - val_Si-sdr: -187.8720\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 52.84876\n",
      "Epoch 11/200\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 627.1411 - Si-sdr: -163.3238 - val_loss: 627.1680 - val_Si-sdr: -191.8503\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 52.84876\n",
      "Epoch 12/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 627.1582 - Si-sdr: -198.0799 - val_loss: 627.1392 - val_Si-sdr: -171.6288\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 52.84876\n",
      "Epoch 13/200\n",
      "2/2 [==============================] - 1s 391ms/step - loss: 627.1024 - Si-sdr: -150.0146 - val_loss: 626.9885 - val_Si-sdr: -124.2735\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 52.84876\n",
      "Epoch 14/200\n",
      "2/2 [==============================] - 1s 488ms/step - loss: 626.9291 - Si-sdr: -118.3885 - val_loss: 626.8167 - val_Si-sdr: -107.0628\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 52.84876\n",
      "Epoch 15/200\n",
      "2/2 [==============================] - 1s 500ms/step - loss: 626.7246 - Si-sdr: -103.4139 - val_loss: 626.2587 - val_Si-sdr: -88.0463\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 52.84876\n",
      "Epoch 16/200\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 626.0638 - Si-sdr: -83.8552 - val_loss: 624.8025 - val_Si-sdr: -69.7152\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 52.84876\n",
      "Epoch 17/200\n",
      "2/2 [==============================] - 1s 526ms/step - loss: 623.9321 - Si-sdr: -65.1951 - val_loss: 620.7914 - val_Si-sdr: -52.7991\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 52.84876\n",
      "Epoch 18/200\n",
      "2/2 [==============================] - 1s 514ms/step - loss: 618.6952 - Si-sdr: -51.7923 - val_loss: 611.7799 - val_Si-sdr: -45.5331\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 52.84876\n",
      "Epoch 19/200\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 608.8275 - Si-sdr: -44.1484 - val_loss: 593.6638 - val_Si-sdr: -37.5729\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 52.84876\n",
      "Epoch 20/200\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 588.0369 - Si-sdr: -36.1907 - val_loss: 564.2756 - val_Si-sdr: -31.5758\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 52.84876\n",
      "Epoch 21/200\n",
      "2/2 [==============================] - 1s 597ms/step - loss: 554.5052 - Si-sdr: -31.2429 - val_loss: 526.8017 - val_Si-sdr: -26.4548\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 52.84876\n",
      "Epoch 22/200\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 519.1837 - Si-sdr: -25.0550 - val_loss: 480.4177 - val_Si-sdr: -19.6282\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 52.84876\n",
      "Epoch 23/200\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 466.4188 - Si-sdr: -18.0230 - val_loss: 435.2813 - val_Si-sdr: -14.1522\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 52.84876\n",
      "Epoch 24/200\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 421.8240 - Si-sdr: -12.8559 - val_loss: 391.3010 - val_Si-sdr: -8.8819\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 52.84876\n",
      "Epoch 25/200\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 373.8323 - Si-sdr: -6.6405 - val_loss: 335.7817 - val_Si-sdr: -2.1104\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 52.84876\n",
      "Epoch 26/200\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 320.8287 - Si-sdr: -0.6046 - val_loss: 294.9134 - val_Si-sdr: 2.7743\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 52.84876\n",
      "Epoch 27/200\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 285.1693 - Si-sdr: 3.7363 - val_loss: 264.8890 - val_Si-sdr: 5.7728\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 52.84876\n",
      "Epoch 28/200\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 257.5986 - Si-sdr: 6.8880 - val_loss: 246.4772 - val_Si-sdr: 7.9048\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 52.84876\n",
      "Epoch 29/200\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 237.4365 - Si-sdr: 9.1722 - val_loss: 226.3925 - val_Si-sdr: 10.1042\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 52.84876\n",
      "Epoch 30/200\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 224.3307 - Si-sdr: 10.4708 - val_loss: 210.9193 - val_Si-sdr: 11.7152\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 52.84876\n",
      "Epoch 31/200\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 205.4604 - Si-sdr: 12.5328 - val_loss: 197.7529 - val_Si-sdr: 13.4427\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 52.84876\n",
      "Epoch 32/200\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 194.4873 - Si-sdr: 13.8960 - val_loss: 190.2312 - val_Si-sdr: 14.3214\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 52.84876\n",
      "Epoch 33/200\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 187.3694 - Si-sdr: 14.6046 - val_loss: 182.3662 - val_Si-sdr: 15.5856\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 52.84876\n",
      "Epoch 34/200\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 179.1438 - Si-sdr: 15.8831 - val_loss: 173.7746 - val_Si-sdr: 16.6547\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 52.84876\n",
      "Epoch 35/200\n",
      "2/2 [==============================] - 1s 475ms/step - loss: 170.8872 - Si-sdr: 17.0886 - val_loss: 167.5074 - val_Si-sdr: 17.2596\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 52.84876\n",
      "Epoch 36/200\n",
      "2/2 [==============================] - 1s 494ms/step - loss: 165.2259 - Si-sdr: 17.8246 - val_loss: 164.4354 - val_Si-sdr: 17.8053\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 52.84876\n",
      "Epoch 37/200\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 162.1940 - Si-sdr: 18.3461 - val_loss: 157.3920 - val_Si-sdr: 18.7936\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 52.84876\n",
      "Epoch 38/200\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 155.3301 - Si-sdr: 19.2195 - val_loss: 151.3411 - val_Si-sdr: 19.6842\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 52.84876\n",
      "Epoch 39/200\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 148.7450 - Si-sdr: 20.1133 - val_loss: 150.1316 - val_Si-sdr: 19.9113\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 52.84876\n",
      "Epoch 40/200\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 148.8557 - Si-sdr: 20.1615 - val_loss: 143.0562 - val_Si-sdr: 20.8977\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 52.84876\n",
      "Epoch 41/200\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 141.5978 - Si-sdr: 21.1261 - val_loss: 139.0487 - val_Si-sdr: 21.6835\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 52.84876\n",
      "Epoch 42/200\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 139.6287 - Si-sdr: 21.5273 - val_loss: 137.4462 - val_Si-sdr: 21.7906\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 52.84876\n",
      "Epoch 43/200\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 136.6070 - Si-sdr: 22.0587 - val_loss: 131.9555 - val_Si-sdr: 22.7707\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 52.84876\n",
      "Epoch 44/200\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 132.6855 - Si-sdr: 22.6454 - val_loss: 131.6695 - val_Si-sdr: 22.7309\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 52.84876\n",
      "Epoch 45/200\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 131.4022 - Si-sdr: 22.8064 - val_loss: 127.1716 - val_Si-sdr: 23.5508\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 52.84876\n",
      "Epoch 46/200\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 126.7995 - Si-sdr: 23.7110 - val_loss: 124.1415 - val_Si-sdr: 24.0170\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 52.84876\n",
      "Epoch 47/200\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 125.1866 - Si-sdr: 23.8390 - val_loss: 120.6602 - val_Si-sdr: 24.5713\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 52.84876\n",
      "Epoch 48/200\n",
      "2/2 [==============================] - 1s 527ms/step - loss: 120.5779 - Si-sdr: 24.6351 - val_loss: 119.5978 - val_Si-sdr: 24.7902\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 52.84876\n",
      "Epoch 49/200\n",
      "2/2 [==============================] - 1s 493ms/step - loss: 119.4131 - Si-sdr: 24.8347 - val_loss: 117.0817 - val_Si-sdr: 25.3321\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 52.84876\n",
      "Epoch 50/200\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 117.2804 - Si-sdr: 25.1867 - val_loss: 115.8286 - val_Si-sdr: 25.4708\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 52.84876\n",
      "Epoch 51/200\n",
      "2/2 [==============================] - 1s 511ms/step - loss: 115.5919 - Si-sdr: 25.4873 - val_loss: 113.7151 - val_Si-sdr: 25.8361\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 52.84876\n",
      "Epoch 52/200\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 113.0141 - Si-sdr: 25.9535 - val_loss: 112.8726 - val_Si-sdr: 26.0102\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 52.84876\n",
      "Epoch 53/200\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 111.8119 - Si-sdr: 26.2494 - val_loss: 111.2605 - val_Si-sdr: 26.3001\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 52.84876\n",
      "Epoch 54/200\n",
      "2/2 [==============================] - 1s 504ms/step - loss: 109.9122 - Si-sdr: 26.5956 - val_loss: 109.0709 - val_Si-sdr: 26.6192\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 52.84876\n",
      "Epoch 55/200\n",
      "2/2 [==============================] - 1s 476ms/step - loss: 108.3473 - Si-sdr: 26.7926 - val_loss: 107.0914 - val_Si-sdr: 27.0080\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 52.84876\n",
      "Epoch 56/200\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 105.9095 - Si-sdr: 27.2766 - val_loss: 107.3533 - val_Si-sdr: 27.0043\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 52.84876\n",
      "Epoch 57/200\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 104.6764 - Si-sdr: 27.4984 - val_loss: 104.6086 - val_Si-sdr: 27.4842\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 52.84876\n",
      "Epoch 58/200\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 105.1966 - Si-sdr: 27.4157 - val_loss: 104.7614 - val_Si-sdr: 27.3392\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 52.84876\n",
      "Epoch 59/200\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 102.0156 - Si-sdr: 28.0222 - val_loss: 102.5511 - val_Si-sdr: 27.9076\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 52.84876\n",
      "Epoch 60/200\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 103.0979 - Si-sdr: 27.7782 - val_loss: 99.6251 - val_Si-sdr: 28.4863\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 52.84876\n",
      "Epoch 61/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 99.3444 - Si-sdr: 28.5103 - val_loss: 98.9529 - val_Si-sdr: 28.5947\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 52.84876\n",
      "Epoch 62/200\n",
      "2/2 [==============================] - 1s 531ms/step - loss: 99.5994 - Si-sdr: 28.5009 - val_loss: 97.3475 - val_Si-sdr: 28.9190\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 52.84876\n",
      "Epoch 63/200\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 97.6298 - Si-sdr: 28.9089 - val_loss: 95.0604 - val_Si-sdr: 29.3989\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 52.84876\n",
      "Epoch 64/200\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 97.3771 - Si-sdr: 28.8730 - val_loss: 97.2360 - val_Si-sdr: 28.9640\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 52.84876\n",
      "Epoch 65/200\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 95.0235 - Si-sdr: 29.4078 - val_loss: 95.3427 - val_Si-sdr: 29.3376\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 52.84876\n",
      "Epoch 66/200\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 93.4161 - Si-sdr: 29.8014 - val_loss: 93.1427 - val_Si-sdr: 29.8032\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 52.84876\n",
      "Epoch 67/200\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 92.1542 - Si-sdr: 30.1033 - val_loss: 93.0959 - val_Si-sdr: 29.8523\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 52.84876\n",
      "Epoch 68/200\n",
      "2/2 [==============================] - 1s 597ms/step - loss: 91.7097 - Si-sdr: 30.1387 - val_loss: 90.3943 - val_Si-sdr: 30.4035\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 52.84876\n",
      "Epoch 69/200\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 91.5537 - Si-sdr: 30.2331 - val_loss: 91.7383 - val_Si-sdr: 30.1247\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 52.84876\n",
      "Epoch 70/200\n",
      "2/2 [==============================] - 1s 494ms/step - loss: 90.9707 - Si-sdr: 30.3648 - val_loss: 88.5755 - val_Si-sdr: 30.8906\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 52.84876\n",
      "Epoch 71/200\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 91.0961 - Si-sdr: 30.3157 - val_loss: 87.7775 - val_Si-sdr: 31.1036\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 52.84876\n",
      "Epoch 72/200\n",
      "2/2 [==============================] - 1s 476ms/step - loss: 88.4537 - Si-sdr: 30.8424 - val_loss: 89.5082 - val_Si-sdr: 30.6961\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 52.84876\n",
      "Epoch 73/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 88.5984 - Si-sdr: 31.0065 - val_loss: 87.9056 - val_Si-sdr: 31.0309\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 52.84876\n",
      "Epoch 74/200\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 87.5472 - Si-sdr: 31.0908 - val_loss: 86.3657 - val_Si-sdr: 31.3890\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 52.84876\n",
      "Epoch 75/200\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 86.6545 - Si-sdr: 31.4084 - val_loss: 84.1273 - val_Si-sdr: 31.9723\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 52.84876\n",
      "Epoch 76/200\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 84.8545 - Si-sdr: 31.7674 - val_loss: 83.8119 - val_Si-sdr: 31.8601\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 52.84876\n",
      "Epoch 77/200\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 84.8528 - Si-sdr: 31.6955 - val_loss: 84.8397 - val_Si-sdr: 31.6443\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 52.84876\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 476ms/step - loss: 85.0157 - Si-sdr: 31.6534 - val_loss: 83.3907 - val_Si-sdr: 32.0091\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 52.84876\n",
      "Epoch 79/200\n",
      "2/2 [==============================] - 1s 496ms/step - loss: 82.7203 - Si-sdr: 32.1130 - val_loss: 81.4593 - val_Si-sdr: 32.5094\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 52.84876\n",
      "Epoch 80/200\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 81.5037 - Si-sdr: 32.5729 - val_loss: 81.2026 - val_Si-sdr: 32.5588\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 52.84876\n",
      "Epoch 81/200\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 80.7992 - Si-sdr: 32.6360 - val_loss: 81.0676 - val_Si-sdr: 32.6135\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 52.84876\n",
      "Epoch 82/200\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 80.9746 - Si-sdr: 32.5844 - val_loss: 80.9644 - val_Si-sdr: 32.5460\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 52.84876\n",
      "Epoch 83/200\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 80.4515 - Si-sdr: 32.7150 - val_loss: 78.9299 - val_Si-sdr: 33.0497\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 52.84876\n",
      "Epoch 84/200\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 79.6919 - Si-sdr: 32.8505 - val_loss: 77.1447 - val_Si-sdr: 33.5268\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 52.84876\n",
      "Epoch 85/200\n",
      "2/2 [==============================] - 1s 544ms/step - loss: 76.8413 - Si-sdr: 33.6542 - val_loss: 77.2549 - val_Si-sdr: 33.4910\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 52.84876\n",
      "Epoch 86/200\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 78.2087 - Si-sdr: 33.3517 - val_loss: 75.9431 - val_Si-sdr: 33.7877\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 52.84876\n",
      "Epoch 87/200\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 76.6380 - Si-sdr: 33.7055 - val_loss: 78.1449 - val_Si-sdr: 33.4419\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 52.84876\n",
      "Epoch 88/200\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 78.0020 - Si-sdr: 33.3558 - val_loss: 76.1158 - val_Si-sdr: 33.7712\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 52.84876\n",
      "Epoch 89/200\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 76.4684 - Si-sdr: 33.6833 - val_loss: 74.3784 - val_Si-sdr: 34.2323\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 52.84876\n",
      "Epoch 90/200\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 75.3106 - Si-sdr: 33.9890 - val_loss: 74.5896 - val_Si-sdr: 34.2275\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 52.84876\n",
      "Epoch 91/200\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 76.0207 - Si-sdr: 33.7518 - val_loss: 75.5802 - val_Si-sdr: 33.8866\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 52.84876\n",
      "Epoch 92/200\n",
      "2/2 [==============================] - 1s 375ms/step - loss: 73.8805 - Si-sdr: 34.3354 - val_loss: 74.5529 - val_Si-sdr: 34.2576\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 52.84876\n",
      "Epoch 93/200\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 75.8543 - Si-sdr: 34.0148 - val_loss: 73.5880 - val_Si-sdr: 34.4599\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 52.84876\n",
      "Epoch 94/200\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 75.0437 - Si-sdr: 34.1375 - val_loss: 72.5535 - val_Si-sdr: 34.6963\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 52.84876\n",
      "Epoch 95/200\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 73.4285 - Si-sdr: 34.4111 - val_loss: 75.1474 - val_Si-sdr: 34.2129\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 52.84876\n",
      "Epoch 96/200\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 75.7162 - Si-sdr: 34.0225 - val_loss: 72.6190 - val_Si-sdr: 34.6554\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 52.84876\n",
      "Epoch 97/200\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 73.7433 - Si-sdr: 34.3999 - val_loss: 73.2165 - val_Si-sdr: 34.5522\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 52.84876\n",
      "Epoch 98/200\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 73.3345 - Si-sdr: 34.6981 - val_loss: 73.5666 - val_Si-sdr: 34.4877\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 52.84876\n",
      "Epoch 99/200\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 73.0482 - Si-sdr: 34.6402 - val_loss: 72.7088 - val_Si-sdr: 34.6428\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 52.84876\n",
      "Epoch 100/200\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 72.0682 - Si-sdr: 34.7844 - val_loss: 70.6962 - val_Si-sdr: 35.2267\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 52.84876\n",
      "Epoch 101/200\n",
      "2/2 [==============================] - 1s 554ms/step - loss: 71.3513 - Si-sdr: 35.0435 - val_loss: 70.4049 - val_Si-sdr: 35.3478\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 52.84876\n",
      "Epoch 102/200\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 71.6316 - Si-sdr: 35.0584 - val_loss: 71.0090 - val_Si-sdr: 35.1512\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 52.84876\n",
      "Epoch 103/200\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 70.9431 - Si-sdr: 35.1502 - val_loss: 70.2019 - val_Si-sdr: 35.3505\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 52.84876\n",
      "Epoch 104/200\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 70.0927 - Si-sdr: 35.3358 - val_loss: 70.0260 - val_Si-sdr: 35.2721\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 52.84876\n",
      "Epoch 105/200\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 69.6026 - Si-sdr: 35.4801 - val_loss: 68.1811 - val_Si-sdr: 35.8958\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 52.84876\n",
      "Epoch 106/200\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 67.8344 - Si-sdr: 35.9860 - val_loss: 66.9601 - val_Si-sdr: 36.2650\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 52.84876\n",
      "Epoch 107/200\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 68.2084 - Si-sdr: 35.8327 - val_loss: 67.6626 - val_Si-sdr: 36.0258\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 52.84876\n",
      "Epoch 108/200\n",
      "2/2 [==============================] - 1s 515ms/step - loss: 67.5898 - Si-sdr: 36.0290 - val_loss: 68.2303 - val_Si-sdr: 35.8934\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 52.84876\n",
      "Epoch 109/200\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 67.9477 - Si-sdr: 35.9101 - val_loss: 67.9733 - val_Si-sdr: 36.0309\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 52.84876\n",
      "Epoch 110/200\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 67.6661 - Si-sdr: 36.0102 - val_loss: 66.9912 - val_Si-sdr: 36.2420\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 52.84876\n",
      "Epoch 111/200\n",
      "2/2 [==============================] - 1s 502ms/step - loss: 66.2622 - Si-sdr: 36.4610 - val_loss: 66.7682 - val_Si-sdr: 36.2367\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 52.84876\n",
      "Epoch 112/200\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 67.3677 - Si-sdr: 36.2188 - val_loss: 67.2673 - val_Si-sdr: 36.1182\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 52.84876\n",
      "Epoch 113/200\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 66.2638 - Si-sdr: 36.4405 - val_loss: 64.5045 - val_Si-sdr: 36.9688\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 52.84876\n",
      "Epoch 114/200\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 64.9419 - Si-sdr: 36.9871 - val_loss: 65.5107 - val_Si-sdr: 36.6284\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 52.84876\n",
      "Epoch 115/200\n",
      "2/2 [==============================] - 1s 497ms/step - loss: 65.6996 - Si-sdr: 36.5998 - val_loss: 65.5202 - val_Si-sdr: 36.6883\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 52.84876\n",
      "Epoch 116/200\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 64.7431 - Si-sdr: 36.9613 - val_loss: 66.1781 - val_Si-sdr: 36.4635\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 52.84876\n",
      "Epoch 117/200\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 65.2127 - Si-sdr: 36.7644 - val_loss: 67.0295 - val_Si-sdr: 36.2294\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 52.84876\n",
      "Epoch 118/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 65.0873 - Si-sdr: 36.6721 - val_loss: 65.2351 - val_Si-sdr: 36.7222\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 52.84876\n",
      "Epoch 119/200\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 64.1956 - Si-sdr: 37.0129 - val_loss: 63.6311 - val_Si-sdr: 37.2726\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 52.84876\n",
      "Epoch 120/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 423ms/step - loss: 64.3087 - Si-sdr: 37.0991 - val_loss: 64.3097 - val_Si-sdr: 37.0571\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 52.84876\n",
      "Epoch 121/200\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 63.1999 - Si-sdr: 37.4132 - val_loss: 64.4115 - val_Si-sdr: 36.9595\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 52.84876\n",
      "Epoch 122/200\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 64.3868 - Si-sdr: 36.9989 - val_loss: 65.7454 - val_Si-sdr: 36.5524\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 52.84876\n",
      "Epoch 123/200\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 63.9689 - Si-sdr: 37.1403 - val_loss: 63.7839 - val_Si-sdr: 37.2580\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 52.84876\n",
      "Epoch 124/200\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 63.3284 - Si-sdr: 37.3974 - val_loss: 64.0381 - val_Si-sdr: 37.1139\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 52.84876\n",
      "Epoch 125/200\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 64.8431 - Si-sdr: 36.9682 - val_loss: 65.0618 - val_Si-sdr: 36.7525\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 52.84876\n",
      "Epoch 126/200\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 64.1780 - Si-sdr: 37.0773 - val_loss: 63.6363 - val_Si-sdr: 37.1546\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 52.84876\n",
      "Epoch 127/200\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 63.4475 - Si-sdr: 37.4479 - val_loss: 63.6335 - val_Si-sdr: 37.2600\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 52.84876\n",
      "Epoch 128/200\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 63.7577 - Si-sdr: 37.1864 - val_loss: 63.3501 - val_Si-sdr: 37.2406\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 52.84876\n",
      "Epoch 129/200\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 62.8817 - Si-sdr: 37.5805 - val_loss: 63.5634 - val_Si-sdr: 37.3165\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 52.84876\n",
      "Epoch 130/200\n",
      "2/2 [==============================] - 1s 483ms/step - loss: 64.1420 - Si-sdr: 37.2042 - val_loss: 63.0508 - val_Si-sdr: 37.3659\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 52.84876\n",
      "Epoch 131/200\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 61.3028 - Si-sdr: 38.0426 - val_loss: 62.7194 - val_Si-sdr: 37.5100\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 52.84876\n",
      "Epoch 132/200\n",
      "2/2 [==============================] - 1s 524ms/step - loss: 61.8998 - Si-sdr: 37.7255 - val_loss: 62.0502 - val_Si-sdr: 37.7178\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 52.84876\n",
      "Epoch 133/200\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 61.5474 - Si-sdr: 37.7984 - val_loss: 60.6747 - val_Si-sdr: 38.2078\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 52.84876\n",
      "Epoch 134/200\n",
      "2/2 [==============================] - 1s 481ms/step - loss: 61.9212 - Si-sdr: 37.8264 - val_loss: 61.0613 - val_Si-sdr: 38.0112\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 52.84876\n",
      "Epoch 135/200\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 61.5167 - Si-sdr: 37.8768 - val_loss: 60.2557 - val_Si-sdr: 38.2607\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 52.84876\n",
      "Epoch 136/200\n",
      "2/2 [==============================] - 1s 532ms/step - loss: 61.5168 - Si-sdr: 37.9287 - val_loss: 61.9932 - val_Si-sdr: 37.6806\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 52.84876\n",
      "Epoch 137/200\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 61.0689 - Si-sdr: 38.0253 - val_loss: 61.6353 - val_Si-sdr: 37.7960\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 52.84876\n",
      "Epoch 138/200\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 60.9650 - Si-sdr: 37.9507 - val_loss: 60.8393 - val_Si-sdr: 38.1027\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 52.84876\n",
      "Epoch 139/200\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 60.4802 - Si-sdr: 38.2193 - val_loss: 59.9939 - val_Si-sdr: 38.4103\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 52.84876\n",
      "Epoch 140/200\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 60.6076 - Si-sdr: 38.2157 - val_loss: 60.7511 - val_Si-sdr: 38.0686\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 52.84876\n",
      "Epoch 141/200\n",
      "2/2 [==============================] - 1s 381ms/step - loss: 62.0232 - Si-sdr: 37.8709 - val_loss: 62.3490 - val_Si-sdr: 37.6898\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 52.84876\n",
      "Epoch 142/200\n",
      "2/2 [==============================] - 1s 543ms/step - loss: 62.8518 - Si-sdr: 37.4986 - val_loss: 59.4085 - val_Si-sdr: 38.5197\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 52.84876\n",
      "Epoch 143/200\n",
      "2/2 [==============================] - 1s 483ms/step - loss: 62.0692 - Si-sdr: 37.9934 - val_loss: 59.3086 - val_Si-sdr: 38.6088\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 52.84876\n",
      "Epoch 144/200\n",
      "2/2 [==============================] - 1s 386ms/step - loss: 59.1073 - Si-sdr: 38.8282 - val_loss: 61.9665 - val_Si-sdr: 37.9208\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 52.84876\n",
      "Epoch 145/200\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 60.9411 - Si-sdr: 38.2672 - val_loss: 57.7282 - val_Si-sdr: 39.0386\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 52.84876\n",
      "Epoch 146/200\n",
      "2/2 [==============================] - 1s 491ms/step - loss: 57.8945 - Si-sdr: 38.9326 - val_loss: 60.2399 - val_Si-sdr: 38.3958\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 52.84876\n",
      "Epoch 147/200\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 59.2816 - Si-sdr: 38.6467 - val_loss: 58.3522 - val_Si-sdr: 38.8995\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 52.84876\n",
      "Epoch 148/200\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 58.9852 - Si-sdr: 38.8849 - val_loss: 58.0683 - val_Si-sdr: 39.0120\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 52.84876\n",
      "Epoch 149/200\n",
      "2/2 [==============================] - 1s 496ms/step - loss: 57.7625 - Si-sdr: 39.0389 - val_loss: 57.1724 - val_Si-sdr: 39.3020\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 52.84876\n",
      "Epoch 150/200\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 58.4180 - Si-sdr: 38.9032 - val_loss: 58.2472 - val_Si-sdr: 38.8745\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 52.84876\n",
      "Epoch 151/200\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 58.5090 - Si-sdr: 39.0640 - val_loss: 57.1244 - val_Si-sdr: 39.3995\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 52.84876\n",
      "Epoch 152/200\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 57.3829 - Si-sdr: 39.2371 - val_loss: 57.2168 - val_Si-sdr: 39.3377\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 52.84876\n",
      "Epoch 153/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 56.6053 - Si-sdr: 39.6181 - val_loss: 56.9877 - val_Si-sdr: 39.2959\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 52.84876\n",
      "Epoch 154/200\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 58.0132 - Si-sdr: 38.9852 - val_loss: 57.1351 - val_Si-sdr: 39.2515\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 52.84876\n",
      "Epoch 155/200\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 57.2218 - Si-sdr: 39.2196 - val_loss: 57.1483 - val_Si-sdr: 39.3039\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 52.84876\n",
      "Epoch 156/200\n",
      "2/2 [==============================] - 1s 535ms/step - loss: 57.3622 - Si-sdr: 39.1956 - val_loss: 57.7463 - val_Si-sdr: 39.0354\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 52.84876\n",
      "Epoch 157/200\n",
      "2/2 [==============================] - 1s 380ms/step - loss: 57.5327 - Si-sdr: 39.2891 - val_loss: 58.0991 - val_Si-sdr: 39.0622\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 52.84876\n",
      "Epoch 158/200\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 59.7595 - Si-sdr: 38.4508 - val_loss: 56.1700 - val_Si-sdr: 39.6149\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 52.84876\n",
      "Epoch 159/200\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 58.0878 - Si-sdr: 38.8815 - val_loss: 57.1269 - val_Si-sdr: 39.3280\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 52.84876\n",
      "Epoch 160/200\n",
      "2/2 [==============================] - 1s 501ms/step - loss: 57.8411 - Si-sdr: 39.1370 - val_loss: 58.7938 - val_Si-sdr: 38.7352\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 52.84876\n",
      "Epoch 161/200\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 57.5488 - Si-sdr: 39.1767 - val_loss: 58.6625 - val_Si-sdr: 38.7104\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 52.84876\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 412ms/step - loss: 60.0343 - Si-sdr: 38.3775 - val_loss: 56.7287 - val_Si-sdr: 39.4518\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 52.84876\n",
      "Epoch 163/200\n",
      "2/2 [==============================] - 1s 554ms/step - loss: 56.9330 - Si-sdr: 39.4034 - val_loss: 56.5650 - val_Si-sdr: 39.5110\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 52.84876\n",
      "Epoch 164/200\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 56.1389 - Si-sdr: 39.5646 - val_loss: 55.0139 - val_Si-sdr: 40.0388\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 52.84876\n",
      "Epoch 165/200\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 55.6714 - Si-sdr: 39.8187 - val_loss: 55.1481 - val_Si-sdr: 39.9197\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 52.84876\n",
      "Epoch 166/200\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 55.4261 - Si-sdr: 39.8844 - val_loss: 54.7671 - val_Si-sdr: 40.0852\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 52.84876\n",
      "Epoch 167/200\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 54.9146 - Si-sdr: 40.0304 - val_loss: 55.8453 - val_Si-sdr: 39.6821\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 52.84876\n",
      "Epoch 168/200\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 54.4164 - Si-sdr: 40.1236 - val_loss: 56.0329 - val_Si-sdr: 39.6403\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 52.84876\n",
      "Epoch 169/200\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 55.3437 - Si-sdr: 39.9486 - val_loss: 54.3658 - val_Si-sdr: 40.2849\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 52.84876\n",
      "Epoch 170/200\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 55.1474 - Si-sdr: 39.9965 - val_loss: 54.3960 - val_Si-sdr: 40.2584\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 52.84876\n",
      "Epoch 171/200\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 55.5699 - Si-sdr: 39.9204 - val_loss: 54.4508 - val_Si-sdr: 40.2858\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 52.84876\n",
      "Epoch 172/200\n",
      "2/2 [==============================] - 1s 511ms/step - loss: 54.7712 - Si-sdr: 40.0096 - val_loss: 54.2180 - val_Si-sdr: 40.3592\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 52.84876\n",
      "Epoch 173/200\n",
      "2/2 [==============================] - 1s 475ms/step - loss: 54.1535 - Si-sdr: 40.3912 - val_loss: 54.1839 - val_Si-sdr: 40.2538\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 52.84876\n",
      "Epoch 174/200\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 54.5652 - Si-sdr: 40.0681 - val_loss: 53.4492 - val_Si-sdr: 40.5776\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 52.84876\n",
      "Epoch 175/200\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 53.2648 - Si-sdr: 40.6701 - val_loss: 53.1845 - val_Si-sdr: 40.5862\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 52.84876\n",
      "Epoch 176/200\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 54.4111 - Si-sdr: 40.2030 - val_loss: 52.8307 - val_Si-sdr: 40.7406\n",
      "\n",
      "Epoch 00176: val_loss improved from 52.84876 to 52.83071, saving model to ./CKPT\\CKP_ep_176__loss_52.83071_.h5\n",
      "Epoch 177/200\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 53.5016 - Si-sdr: 40.5537 - val_loss: 54.0254 - val_Si-sdr: 40.2829\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 52.83071\n",
      "Epoch 178/200\n",
      "2/2 [==============================] - 1s 536ms/step - loss: 55.6143 - Si-sdr: 39.8496 - val_loss: 53.7691 - val_Si-sdr: 40.4479\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 52.83071\n",
      "Epoch 179/200\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 53.3799 - Si-sdr: 40.5357 - val_loss: 53.2625 - val_Si-sdr: 40.6822\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 52.83071\n",
      "Epoch 180/200\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 53.6557 - Si-sdr: 40.4494 - val_loss: 55.0174 - val_Si-sdr: 40.0223\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 52.83071\n",
      "Epoch 181/200\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 54.2078 - Si-sdr: 40.2437 - val_loss: 53.3719 - val_Si-sdr: 40.5444\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 52.83071\n",
      "Epoch 182/200\n",
      "2/2 [==============================] - 1s 517ms/step - loss: 52.5736 - Si-sdr: 40.9627 - val_loss: 53.2444 - val_Si-sdr: 40.5965\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 52.83071\n",
      "Epoch 183/200\n",
      "2/2 [==============================] - 1s 510ms/step - loss: 53.3946 - Si-sdr: 40.5200 - val_loss: 53.1559 - val_Si-sdr: 40.6098\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 52.83071\n",
      "Epoch 184/200\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 53.1862 - Si-sdr: 40.6598 - val_loss: 52.2678 - val_Si-sdr: 40.9978\n",
      "\n",
      "Epoch 00184: val_loss improved from 52.83071 to 52.26781, saving model to ./CKPT\\CKP_ep_184__loss_52.26781_.h5\n",
      "Epoch 185/200\n",
      "2/2 [==============================] - 1s 475ms/step - loss: 52.3150 - Si-sdr: 41.0003 - val_loss: 52.6396 - val_Si-sdr: 40.8331\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 52.26781\n",
      "Epoch 186/200\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 52.7178 - Si-sdr: 40.7878 - val_loss: 52.5798 - val_Si-sdr: 40.7877\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 52.26781\n",
      "Epoch 187/200\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 51.5163 - Si-sdr: 41.2198 - val_loss: 51.4925 - val_Si-sdr: 41.2159\n",
      "\n",
      "Epoch 00187: val_loss improved from 52.26781 to 51.49252, saving model to ./CKPT\\CKP_ep_187__loss_51.49252_.h5\n",
      "Epoch 188/200\n",
      "2/2 [==============================] - 1s 483ms/step - loss: 50.9113 - Si-sdr: 41.4579 - val_loss: 51.2653 - val_Si-sdr: 41.3200\n",
      "\n",
      "Epoch 00188: val_loss improved from 51.49252 to 51.26528, saving model to ./CKPT\\CKP_ep_188__loss_51.26528_.h5\n",
      "Epoch 189/200\n",
      "2/2 [==============================] - 1s 481ms/step - loss: 52.2338 - Si-sdr: 40.8929 - val_loss: 50.6355 - val_Si-sdr: 41.5141\n",
      "\n",
      "Epoch 00189: val_loss improved from 51.26528 to 50.63554, saving model to ./CKPT\\CKP_ep_189__loss_50.63554_.h5\n",
      "Epoch 190/200\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 50.0143 - Si-sdr: 41.9600 - val_loss: 50.8836 - val_Si-sdr: 41.5171\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 50.63554\n",
      "Epoch 191/200\n",
      "2/2 [==============================] - 1s 485ms/step - loss: 51.3665 - Si-sdr: 41.4889 - val_loss: 51.7028 - val_Si-sdr: 41.1965\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 50.63554\n",
      "Epoch 192/200\n",
      "2/2 [==============================] - 1s 491ms/step - loss: 51.0859 - Si-sdr: 41.3739 - val_loss: 50.8493 - val_Si-sdr: 41.4524\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 50.63554\n",
      "Epoch 193/200\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 50.9865 - Si-sdr: 41.4858 - val_loss: 50.8734 - val_Si-sdr: 41.5364\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 50.63554\n",
      "Epoch 194/200\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 51.4849 - Si-sdr: 41.3703 - val_loss: 50.3135 - val_Si-sdr: 41.7363\n",
      "\n",
      "Epoch 00194: val_loss improved from 50.63554 to 50.31346, saving model to ./CKPT\\CKP_ep_194__loss_50.31346_.h5\n",
      "Epoch 195/200\n",
      "2/2 [==============================] - 1s 525ms/step - loss: 50.9185 - Si-sdr: 41.5584 - val_loss: 50.0515 - val_Si-sdr: 41.8020\n",
      "\n",
      "Epoch 00195: val_loss improved from 50.31346 to 50.05148, saving model to ./CKPT\\CKP_ep_195__loss_50.05148_.h5\n",
      "Epoch 196/200\n",
      "2/2 [==============================] - 1s 496ms/step - loss: 50.3889 - Si-sdr: 41.7782 - val_loss: 52.6510 - val_Si-sdr: 40.8211\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 50.05148\n",
      "Epoch 197/200\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 52.0942 - Si-sdr: 41.0627 - val_loss: 49.9374 - val_Si-sdr: 41.7966\n",
      "\n",
      "Epoch 00197: val_loss improved from 50.05148 to 49.93743, saving model to ./CKPT\\CKP_ep_197__loss_49.93743_.h5\n",
      "Epoch 198/200\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 50.9104 - Si-sdr: 41.4092 - val_loss: 49.5117 - val_Si-sdr: 42.0483\n",
      "\n",
      "Epoch 00198: val_loss improved from 49.93743 to 49.51171, saving model to ./CKPT\\CKP_ep_198__loss_49.51171_.h5\n",
      "Epoch 199/200\n",
      "2/2 [==============================] - 1s 475ms/step - loss: 50.6804 - Si-sdr: 41.5430 - val_loss: 50.0856 - val_Si-sdr: 41.7940\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 49.51171\n",
      "Epoch 200/200\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 49.9890 - Si-sdr: 41.8565 - val_loss: 50.3985 - val_Si-sdr: 41.6984\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 49.51171\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_2 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_198__loss_49.51171_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "        tf.executing_eagerly() # requires r1.7\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)\n",
    "        \n",
    "#         result = vq_vae.predict(input_batch)\n",
    "#         print(result.shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.14930329 -0.05488385 -0.03165275  0.02511186]\n",
      "  [-0.04538564 -0.02340828 -0.13736942  0.03185076]\n",
      "  [-0.18286747 -0.01420305  0.02583414  0.20739384]\n",
      "  [-0.15128048 -0.15295134  0.00826486 -0.03651741]\n",
      "  [-0.19406578 -0.14820886 -0.24136184  0.01954714]\n",
      "  [-0.11608941  0.05977409 -0.05805521  0.08262399]\n",
      "  [-0.10437049  0.00281959  0.00744648  0.06939161]\n",
      "  [-0.32889026 -0.11109954 -0.06513101  0.02347144]\n",
      "  [-0.10914627  0.04005037 -0.11194003  0.34588984]\n",
      "  [-0.55321944 -0.24241613 -0.17234027  0.2255739 ]]\n",
      "\n",
      " [[-0.12665638 -0.02560038 -0.07357763 -0.03386865]\n",
      "  [-0.00296582  0.05382628 -0.02431939 -0.00124017]\n",
      "  [-0.152773    0.01729362 -0.02049777  0.00555693]\n",
      "  [-0.01630396 -0.0508009   0.00338947 -0.00536075]\n",
      "  [-0.11710438 -0.0411608   0.01888775  0.10048383]\n",
      "  [-0.1000828   0.06160894 -0.04584875  0.14583868]\n",
      "  [-0.29426795 -0.04820506 -0.1607864   0.03541403]\n",
      "  [-0.14156486  0.07812893 -0.13114211  0.12421213]\n",
      "  [-0.20461929  0.08133916 -0.08211927  0.03100684]\n",
      "  [-0.00948744 -0.11452891  0.03942408 -0.1281477 ]]]\n",
      "(2, 10, 4)\n",
      "(2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
