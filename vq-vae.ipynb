{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "forward-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ready to excute...\n",
    "\n",
    "# def vae_loss(q_y, latent_dim):\n",
    "#     def si_sdr_loss(y_true, y_pred):\n",
    "#         ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "#         # Label & Length divide\n",
    "#         labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "#         lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "#         target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "#         noise = y_pred - target\n",
    "#         si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "#         si_sdr = tf.reduce_mean(si_sdr)\n",
    "\n",
    "#         return si_sdr\n",
    "    \n",
    "#     log_q_y = tf.math.log(q_y+1e-20)\n",
    "#     kl_loss = tf.reduce_sum(q_y*(log_q_y-tf.math.log(1.0/latent_dim)), axis=[1,2])\n",
    "    \n",
    "#     return si_sdr_loss - kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.reduce_max(y, 2, keep_dims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax()\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_37 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 4s 2s/step - loss: 629.0644 - Si-sdr: -240.9962 - val_loss: 627.8399 - val_Si-sdr: -246.9993\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 66.26788\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 488ms/step - loss: 627.5936 - Si-sdr: -242.5529 - val_loss: 627.3467 - val_Si-sdr: -223.7982\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 66.26788\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 627.4501 - Si-sdr: -240.9507 - val_loss: 627.3632 - val_Si-sdr: -198.1993\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 66.26788\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 546ms/step - loss: 627.3498 - Si-sdr: -242.8647 - val_loss: 627.2504 - val_Si-sdr: -216.7136\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 66.26788\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 547ms/step - loss: 627.2675 - Si-sdr: -218.7871 - val_loss: 627.2163 - val_Si-sdr: -202.0917\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 66.26788\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 516ms/step - loss: 627.2003 - Si-sdr: -203.6017 - val_loss: 627.2188 - val_Si-sdr: -282.8951\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 66.26788\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 627.2023 - Si-sdr: -200.0470 - val_loss: 627.1664 - val_Si-sdr: -181.3293\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 66.26788\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 798ms/step - loss: 627.1940 - Si-sdr: -212.0339 - val_loss: 627.1722 - val_Si-sdr: -204.2834\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 66.26788\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 627.2484 - Si-sdr: -233.4420 - val_loss: 627.1430 - val_Si-sdr: -168.6665\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 66.26788\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 627.1583 - Si-sdr: -176.5443 - val_loss: 627.1522 - val_Si-sdr: -183.9020\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 66.26788\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 627.1374 - Si-sdr: -166.1599 - val_loss: 627.0836 - val_Si-sdr: -146.3899\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 66.26788\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 627.0723 - Si-sdr: -142.1175 - val_loss: 626.9891 - val_Si-sdr: -126.3755\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 66.26788\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 626.9138 - Si-sdr: -118.3720 - val_loss: 626.7141 - val_Si-sdr: -107.0118\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 66.26788\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 510ms/step - loss: 626.6249 - Si-sdr: -103.6942 - val_loss: 626.2405 - val_Si-sdr: -92.4285\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 66.26788\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 626.0134 - Si-sdr: -86.0600 - val_loss: 624.6203 - val_Si-sdr: -68.9649\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 66.26788\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 507ms/step - loss: 623.6003 - Si-sdr: -61.8963 - val_loss: 620.2551 - val_Si-sdr: -52.5377\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 66.26788\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 618.1890 - Si-sdr: -48.9145 - val_loss: 608.8843 - val_Si-sdr: -44.7612\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 66.26788\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 488ms/step - loss: 603.9341 - Si-sdr: -43.0770 - val_loss: 588.1905 - val_Si-sdr: -39.7600\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 66.26788\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 581.1053 - Si-sdr: -39.4654 - val_loss: 563.6321 - val_Si-sdr: -36.2653\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 66.26788\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 560.0414 - Si-sdr: -36.6586 - val_loss: 546.9717 - val_Si-sdr: -33.6633\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 66.26788\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 540.6272 - Si-sdr: -32.4077 - val_loss: 518.9417 - val_Si-sdr: -26.9853\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 66.26788\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 512.1701 - Si-sdr: -25.3983 - val_loss: 495.6811 - val_Si-sdr: -22.8646\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 66.26788\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 488.2096 - Si-sdr: -21.2665 - val_loss: 463.4045 - val_Si-sdr: -17.9029\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 66.26788\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 454.2349 - Si-sdr: -16.2309 - val_loss: 419.0494 - val_Si-sdr: -11.6346\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 66.26788\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 407.4963 - Si-sdr: -10.4060 - val_loss: 373.5775 - val_Si-sdr: -6.3042\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 66.26788\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 371.3606 - Si-sdr: -6.5304 - val_loss: 335.2293 - val_Si-sdr: -1.7634\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 66.26788\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 330.3213 - Si-sdr: -1.2361 - val_loss: 303.1205 - val_Si-sdr: 1.2748\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 66.26788\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 290.0594 - Si-sdr: 3.3548 - val_loss: 265.8669 - val_Si-sdr: 5.9070\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 66.26788\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 478ms/step - loss: 262.2946 - Si-sdr: 6.5035 - val_loss: 244.3250 - val_Si-sdr: 8.2658\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 66.26788\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 237.4464 - Si-sdr: 9.2899 - val_loss: 225.0696 - val_Si-sdr: 10.6069\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 66.26788\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 579ms/step - loss: 218.2243 - Si-sdr: 11.1727 - val_loss: 208.3107 - val_Si-sdr: 12.5275\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 66.26788\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 204.6794 - Si-sdr: 13.0016 - val_loss: 196.4420 - val_Si-sdr: 13.7472\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 66.26788\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 196.5500 - Si-sdr: 13.8803 - val_loss: 189.5901 - val_Si-sdr: 14.6680\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 66.26788\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 187.6755 - Si-sdr: 15.0227 - val_loss: 179.2234 - val_Si-sdr: 16.0881\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 66.26788\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 182.1496 - Si-sdr: 15.6775 - val_loss: 171.9246 - val_Si-sdr: 17.0410\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 66.26788\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 508ms/step - loss: 171.3608 - Si-sdr: 17.0472 - val_loss: 167.7225 - val_Si-sdr: 17.4845\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 66.26788\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 166.3492 - Si-sdr: 17.7270 - val_loss: 161.9364 - val_Si-sdr: 18.2781\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 66.26788\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 539ms/step - loss: 161.7646 - Si-sdr: 18.3435 - val_loss: 156.5202 - val_Si-sdr: 19.2608\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 66.26788\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 157.7810 - Si-sdr: 19.0538 - val_loss: 152.0102 - val_Si-sdr: 19.7276\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 66.26788\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 151.3697 - Si-sdr: 19.9512 - val_loss: 147.2350 - val_Si-sdr: 20.4373\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 66.26788\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 147.0500 - Si-sdr: 20.4930 - val_loss: 143.2835 - val_Si-sdr: 20.9728\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 66.26788\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 141.2637 - Si-sdr: 21.4296 - val_loss: 139.6126 - val_Si-sdr: 21.6229\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 66.26788\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 138.4275 - Si-sdr: 22.1716 - val_loss: 134.3126 - val_Si-sdr: 22.4460\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 66.26788\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 134.2444 - Si-sdr: 22.3834 - val_loss: 131.3866 - val_Si-sdr: 22.8466\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 66.26788\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 132.1918 - Si-sdr: 22.7984 - val_loss: 131.8647 - val_Si-sdr: 22.9328\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 66.26788\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 128.7865 - Si-sdr: 23.4069 - val_loss: 123.8934 - val_Si-sdr: 24.2122\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 66.26788\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 124.4276 - Si-sdr: 24.1498 - val_loss: 122.5208 - val_Si-sdr: 24.3847\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 66.26788\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 122.7889 - Si-sdr: 24.3155 - val_loss: 119.2747 - val_Si-sdr: 24.8877\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 66.26788\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 119.8790 - Si-sdr: 24.9655 - val_loss: 118.6403 - val_Si-sdr: 25.0611\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 66.26788\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 116.3311 - Si-sdr: 25.4882 - val_loss: 114.7839 - val_Si-sdr: 25.8184\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 66.26788\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 1s 494ms/step - loss: 114.8146 - Si-sdr: 25.8042 - val_loss: 114.8285 - val_Si-sdr: 25.6752\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 66.26788\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 114.3974 - Si-sdr: 25.8808 - val_loss: 112.6723 - val_Si-sdr: 26.0864\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 66.26788\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 1s 523ms/step - loss: 110.8144 - Si-sdr: 26.5234 - val_loss: 109.4279 - val_Si-sdr: 26.6018\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 66.26788\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 1s 512ms/step - loss: 108.2328 - Si-sdr: 26.7904 - val_loss: 111.0170 - val_Si-sdr: 26.5696\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 66.26788\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 107.5763 - Si-sdr: 27.2561 - val_loss: 105.7132 - val_Si-sdr: 27.4239\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 66.26788\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 1s 485ms/step - loss: 106.6708 - Si-sdr: 27.2181 - val_loss: 104.1890 - val_Si-sdr: 27.7573\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 66.26788\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 1s 538ms/step - loss: 104.1074 - Si-sdr: 27.7585 - val_loss: 102.7053 - val_Si-sdr: 27.8933\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 66.26788\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 1s 523ms/step - loss: 103.5511 - Si-sdr: 27.9015 - val_loss: 103.3735 - val_Si-sdr: 27.8700\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 66.26788\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 102.9897 - Si-sdr: 28.0320 - val_loss: 99.4505 - val_Si-sdr: 28.8084\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 66.26788\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 1s 532ms/step - loss: 100.3454 - Si-sdr: 28.5465 - val_loss: 98.6734 - val_Si-sdr: 28.7331\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 66.26788\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 1s 541ms/step - loss: 101.4063 - Si-sdr: 28.3844 - val_loss: 98.1217 - val_Si-sdr: 28.8734\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 66.26788\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 98.5042 - Si-sdr: 29.0028 - val_loss: 99.8722 - val_Si-sdr: 28.6340\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 66.26788\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 97.8980 - Si-sdr: 28.9696 - val_loss: 96.3561 - val_Si-sdr: 29.3288\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 66.26788\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 1s 533ms/step - loss: 96.7614 - Si-sdr: 29.2674 - val_loss: 95.3054 - val_Si-sdr: 29.4495\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 66.26788\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 95.5701 - Si-sdr: 29.4626 - val_loss: 92.8420 - val_Si-sdr: 29.8649\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 66.26788\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 92.4749 - Si-sdr: 29.9999 - val_loss: 90.6500 - val_Si-sdr: 30.4098\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 66.26788\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 1s 504ms/step - loss: 92.1150 - Si-sdr: 30.2985 - val_loss: 90.1186 - val_Si-sdr: 30.5617\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 66.26788\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 91.8369 - Si-sdr: 30.4083 - val_loss: 89.0293 - val_Si-sdr: 30.8568\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 66.26788\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 88.9306 - Si-sdr: 30.8718 - val_loss: 88.7721 - val_Si-sdr: 30.9741\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 66.26788\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 1s 527ms/step - loss: 87.5111 - Si-sdr: 31.1462 - val_loss: 86.8654 - val_Si-sdr: 31.2600\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 66.26788\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 86.2773 - Si-sdr: 31.4316 - val_loss: 85.3872 - val_Si-sdr: 31.6457\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 66.26788\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 86.5745 - Si-sdr: 31.2838 - val_loss: 85.7000 - val_Si-sdr: 31.4321\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 66.26788\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 83.6640 - Si-sdr: 31.9623 - val_loss: 84.9777 - val_Si-sdr: 31.5466\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 66.26788\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 83.2569 - Si-sdr: 32.0385 - val_loss: 83.1414 - val_Si-sdr: 32.0009\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 66.26788\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 1s 477ms/step - loss: 83.3154 - Si-sdr: 32.0996 - val_loss: 82.1286 - val_Si-sdr: 32.2641\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 66.26788\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 1s 481ms/step - loss: 83.3929 - Si-sdr: 32.0182 - val_loss: 82.4495 - val_Si-sdr: 32.2837\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 66.26788\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 1s 564ms/step - loss: 82.8910 - Si-sdr: 32.1121 - val_loss: 80.1667 - val_Si-sdr: 32.7948\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 66.26788\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 454ms/step - loss: 80.6361 - Si-sdr: 32.6592 - val_loss: 81.3985 - val_Si-sdr: 32.4848\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 66.26788\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 79.4996 - Si-sdr: 32.9513 - val_loss: 78.6391 - val_Si-sdr: 33.1604\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 66.26788\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 78.3604 - Si-sdr: 33.2349 - val_loss: 79.9459 - val_Si-sdr: 32.7816\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 66.26788\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 79.4323 - Si-sdr: 32.9750 - val_loss: 77.9629 - val_Si-sdr: 33.3304\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 66.26788\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 1s 520ms/step - loss: 78.2348 - Si-sdr: 33.3201 - val_loss: 79.1346 - val_Si-sdr: 32.9659\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 66.26788\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 78.7386 - Si-sdr: 33.0842 - val_loss: 77.1051 - val_Si-sdr: 33.4778\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 66.26788\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 76.5117 - Si-sdr: 33.6882 - val_loss: 75.1334 - val_Si-sdr: 34.0289\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 66.26788\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 1s 487ms/step - loss: 75.8321 - Si-sdr: 33.9119 - val_loss: 74.7903 - val_Si-sdr: 34.1194\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 66.26788\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 74.8677 - Si-sdr: 34.0830 - val_loss: 74.7441 - val_Si-sdr: 34.1449\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 66.26788\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 74.2201 - Si-sdr: 34.3064 - val_loss: 73.4036 - val_Si-sdr: 34.5187\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 66.26788\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 1s 539ms/step - loss: 72.5507 - Si-sdr: 34.7189 - val_loss: 72.9984 - val_Si-sdr: 34.6245\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 66.26788\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 74.4490 - Si-sdr: 34.2240 - val_loss: 72.7211 - val_Si-sdr: 34.5604\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 66.26788\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 1s 530ms/step - loss: 71.9693 - Si-sdr: 34.7966 - val_loss: 72.6242 - val_Si-sdr: 34.7027\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 66.26788\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 72.2060 - Si-sdr: 34.7035 - val_loss: 70.6361 - val_Si-sdr: 35.1893\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 66.26788\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 71.7954 - Si-sdr: 34.8244 - val_loss: 70.7824 - val_Si-sdr: 35.1117\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 66.26788\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 1s 548ms/step - loss: 70.5708 - Si-sdr: 35.2740 - val_loss: 70.2188 - val_Si-sdr: 35.3780\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 66.26788\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 1s 514ms/step - loss: 70.6708 - Si-sdr: 35.2828 - val_loss: 69.2838 - val_Si-sdr: 35.5589\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 66.26788\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 70.4946 - Si-sdr: 35.2842 - val_loss: 68.8158 - val_Si-sdr: 35.7432\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 66.26788\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 1s 530ms/step - loss: 69.4917 - Si-sdr: 35.5141 - val_loss: 68.2879 - val_Si-sdr: 35.8492\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 66.26788\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 1s 385ms/step - loss: 69.0316 - Si-sdr: 35.6144 - val_loss: 70.0114 - val_Si-sdr: 35.4137\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 66.26788\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 68.5117 - Si-sdr: 35.7154 - val_loss: 68.3429 - val_Si-sdr: 35.8557\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 66.26788\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 1s 476ms/step - loss: 68.0554 - Si-sdr: 35.9999 - val_loss: 68.0187 - val_Si-sdr: 35.8903\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 66.26788\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 67.9666 - Si-sdr: 35.9046 - val_loss: 66.8580 - val_Si-sdr: 36.2757\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 66.26788\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=custom_mse, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=1, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.28700954]\n",
      "  [-0.4718614 ]\n",
      "  [-0.3124033 ]\n",
      "  [-0.2531465 ]\n",
      "  [-0.17277378]\n",
      "  [-0.13993308]\n",
      "  [-0.23689504]\n",
      "  [-0.15480173]\n",
      "  [-0.15366751]\n",
      "  [-0.21193263]]\n",
      "\n",
      " [[ 0.00840923]\n",
      "  [ 0.05616647]\n",
      "  [-0.17278165]\n",
      "  [-0.39486957]\n",
      "  [-0.24275026]\n",
      "  [-0.14337276]\n",
      "  [-0.0620722 ]\n",
      "  [-0.33342713]\n",
      "  [-0.12811692]\n",
      "  [-0.41662824]]]\n",
      "(2, 10, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23794276, 0.3758409 , 0.20595631, 0.18026009],\n",
       "        [0.16875331, 0.20575501, 0.26814145, 0.35735017],\n",
       "        [0.08178392, 0.23333283, 0.32606637, 0.35881683],\n",
       "        [0.13777426, 0.4296873 , 0.2569556 , 0.17558284],\n",
       "        [0.2141833 , 0.3373918 , 0.2515016 , 0.19692335]],\n",
       "\n",
       "       [[0.2109271 , 0.3640199 , 0.20789267, 0.21716031],\n",
       "        [0.25047418, 0.33341768, 0.23042291, 0.18568526],\n",
       "        [0.22128512, 0.30281523, 0.21816052, 0.25773916],\n",
       "        [0.17393257, 0.30159497, 0.2893706 , 0.23510183],\n",
       "        [0.29052198, 0.27541766, 0.23140128, 0.2026591 ]]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
