{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 112000, 1)\n",
      "(2, 112001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 112000, 1)\n",
      "(2, 112001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 104000, 1)\n",
      "(2, 104001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 96000, 1)\n",
      "(2, 96001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 88000, 1)\n",
      "(2, 88001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 80000, 1)\n",
      "(2, 80001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 72000, 1)\n",
      "(2, 72001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 56000, 1)\n",
      "(2, 56001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 32000, 1)\n",
      "(2, 32001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n",
      "(2, 40000, 1)\n",
      "(2, 40001, 1)\n",
      "(2, 48000, 1)\n",
      "(2, 48001, 1)\n",
      "(2, 64000, 1)\n",
      "(2, 64001, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)\n",
    "for a, b in train_dataset:\n",
    "    print(a.shape)\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=64, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=128, kernel_size=3, strides=1, padding='valid', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=1, activation=None, padding='valid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=1, activation='relu', padding='valid')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=1, activation='relu', padding='valid')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=1, activation='relu', padding='valid')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=1, padding='valid', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2), axis=[1, 2])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-10)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "        kl_loss = tf.reduce_mean(kl_loss) * 0.2\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-johns",
   "metadata": {},
   "source": [
    "# 이렇게 GradientTape 를 사용해서 프로그램 해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "italian-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 1024\n",
    "epochs = 100\n",
    "\n",
    "filePath = \"./CKPT/CKP_ep_{0}__loss_{1:.5f}_.h5\"\n",
    "model_path = './CKPT/CKP_ep_145__loss_66.64056_.h5'\n",
    "\n",
    "loss_fun = custom_mse\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "sisdr_Metric = SiSdr()\n",
    "val_sisdr_Metric = SiSdr()\n",
    "\n",
    "# Model 불러오는 부분이다\n",
    "vq_vae = Vq_vae(latent_size, gumbel_hard=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worldwide-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Call model\n",
    "        results = vq_vae(x)\n",
    "        \n",
    "        # Calculate losses\n",
    "        loss_value = loss_fun(y, results)\n",
    "        loss_value += sum(vq_vae.losses) # Add KL loss\n",
    "    \n",
    "    # Update weights\n",
    "    grads = tape.gradient(loss_value, vq_vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vq_vae.trainable_weights))\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "#     train_loss.update_state(sum(vq_vae.losses))\n",
    "    train_loss.update_state(loss_value)\n",
    "    sisdr_Metric.update_state(y, results)\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    # Call model\n",
    "    val_results = vq_vae(x)\n",
    "    \n",
    "    # Calculate losses\n",
    "    val_loss_value = loss_fun(y, val_results)\n",
    "    val_loss_value += sum(vq_vae.losses) # Add KL loss\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "    valid_loss.update_state(val_loss_value)\n",
    "    val_sisdr_Metric.update_state(y, val_results)\n",
    "    \n",
    "    return val_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surprising-promise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 0.0058\n",
      "Training Si-sdr (for one batch) at step 0: -48.0414\n",
      "Seen so far: 2 samples\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x000001F0B6A95598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training loss (for one batch) at step 10: 0.0040\n",
      "Training Si-sdr (for one batch) at step 10: -52.2784\n",
      "Seen so far: 22 samples\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_step at 0x000001F0B6A95598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Training loss (for one batch) at step 20: 0.0037\n",
      "Training Si-sdr (for one batch) at step 20: -51.5374\n",
      "Seen so far: 42 samples\n",
      "Training loss (for one batch) at step 30: 0.0040\n",
      "Training Si-sdr (for one batch) at step 30: -52.8708\n",
      "Seen so far: 62 samples\n",
      "Training loss (for one batch) at step 40: 0.0039\n",
      "Training Si-sdr (for one batch) at step 40: -52.6228\n",
      "Seen so far: 82 samples\n",
      "Training loss (for one batch) at step 50: 0.0038\n",
      "Training Si-sdr (for one batch) at step 50: -52.5265\n",
      "Seen so far: 102 samples\n",
      "Training loss (for one batch) at step 60: 0.0037\n",
      "Training Si-sdr (for one batch) at step 60: -52.4100\n",
      "Seen so far: 122 samples\n",
      "Training loss (for one batch) at step 70: 0.0036\n",
      "Training Si-sdr (for one batch) at step 70: -51.8596\n",
      "Seen so far: 142 samples\n",
      "Training loss (for one batch) at step 80: 0.0035\n",
      "Training Si-sdr (for one batch) at step 80: -51.7297\n",
      "Seen so far: 162 samples\n",
      "Training loss (for one batch) at step 90: 0.0034\n",
      "Training Si-sdr (for one batch) at step 90: -51.5903\n",
      "Seen so far: 182 samples\n",
      "Training loss (for one batch) at step 100: 0.0034\n",
      "Training Si-sdr (for one batch) at step 100: -51.7935\n",
      "Seen so far: 202 samples\n",
      "Training loss (for one batch) at step 110: 0.0034\n",
      "Training Si-sdr (for one batch) at step 110: -51.4719\n",
      "Seen so far: 222 samples\n",
      "Training loss (for one batch) at step 120: 0.0033\n",
      "Training Si-sdr (for one batch) at step 120: -51.5213\n",
      "Seen so far: 242 samples\n",
      "Training loss (for one batch) at step 130: 0.0033\n",
      "Training Si-sdr (for one batch) at step 130: -51.6435\n",
      "Seen so far: 262 samples\n",
      "Training loss (for one batch) at step 140: 0.0032\n",
      "Training Si-sdr (for one batch) at step 140: -51.6571\n",
      "Seen so far: 282 samples\n",
      "Training loss (for one batch) at step 150: 0.0032\n",
      "Training Si-sdr (for one batch) at step 150: -51.4560\n",
      "Seen so far: 302 samples\n",
      "Training loss (for one batch) at step 160: 0.0032\n",
      "Training Si-sdr (for one batch) at step 160: -51.6130\n",
      "Seen so far: 322 samples\n",
      "Training loss (for one batch) at step 170: 0.0033\n",
      "Training Si-sdr (for one batch) at step 170: -51.5921\n",
      "Seen so far: 342 samples\n",
      "Training loss (for one batch) at step 180: 0.0032\n",
      "Training Si-sdr (for one batch) at step 180: -51.7256\n",
      "Seen so far: 362 samples\n",
      "Training loss (for one batch) at step 190: 0.0032\n",
      "Training Si-sdr (for one batch) at step 190: -51.7676\n",
      "Seen so far: 382 samples\n",
      "Training loss (for one batch) at step 200: 0.0032\n",
      "Training Si-sdr (for one batch) at step 200: -51.7392\n",
      "Seen so far: 402 samples\n",
      "Training loss (for one batch) at step 210: 0.0032\n",
      "Training Si-sdr (for one batch) at step 210: -51.7087\n",
      "Seen so far: 422 samples\n",
      "Training loss (for one batch) at step 220: 0.0033\n",
      "Training Si-sdr (for one batch) at step 220: -51.6701\n",
      "Seen so far: 442 samples\n",
      "Training loss (for one batch) at step 230: 0.0033\n",
      "Training Si-sdr (for one batch) at step 230: -51.5380\n",
      "Seen so far: 462 samples\n",
      "Training loss (for one batch) at step 240: 0.0033\n",
      "Training Si-sdr (for one batch) at step 240: -51.6301\n",
      "Seen so far: 482 samples\n",
      "Training loss (for one batch) at step 250: 0.0034\n",
      "Training Si-sdr (for one batch) at step 250: -51.5862\n",
      "Seen so far: 502 samples\n",
      "Training loss (for one batch) at step 260: 0.0033\n",
      "Training Si-sdr (for one batch) at step 260: -51.4773\n",
      "Seen so far: 522 samples\n",
      "Training loss (for one batch) at step 270: 0.0033\n",
      "Training Si-sdr (for one batch) at step 270: -51.3401\n",
      "Seen so far: 542 samples\n",
      "Training loss (for one batch) at step 280: 0.0034\n",
      "Training Si-sdr (for one batch) at step 280: -51.4571\n",
      "Seen so far: 562 samples\n",
      "Training loss (for one batch) at step 290: 0.0034\n",
      "Training Si-sdr (for one batch) at step 290: -51.3738\n",
      "Seen so far: 582 samples\n",
      "Training loss (for one batch) at step 300: 0.0034\n",
      "Training Si-sdr (for one batch) at step 300: -51.2599\n",
      "Seen so far: 602 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ec398d808a50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my_batch_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# Log every 1 batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "previous_loss = float('inf')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            x_batch_train = tf.cast(x_batch_train, dtype=tf.float32)\n",
    "            y_batch_train = tf.cast(y_batch_train, dtype=tf.float32)\n",
    "\n",
    "            loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "            # Log every 1 batches\n",
    "            if step % 10 == 0:\n",
    "                print(\"Training loss (for one batch) at step %d: %.4f\" % (step, train_loss.result()))\n",
    "                print(\"Training Si-sdr (for one batch) at step %d: %.4f\" % (step, sisdr_Metric.result()))\n",
    "                print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        for x_batch_val, y_batch_val in valid_dataset:\n",
    "            x_batch_val = tf.cast(x_batch_val, dtype=tf.float32)\n",
    "            y_batch_val = tf.cast(y_batch_val, dtype=tf.float32)\n",
    "\n",
    "            val_loss_value = test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        print()\n",
    "        print('----------------------------------------------------------------------------------')\n",
    "        print(\"Time taken >>> %.2fs <<<\" % (time.time() - start_time))\n",
    "        print('epoch: {}, Train_loss: {}, Train_Si-sdr: {} \\n\\\n",
    "        Valid_loss: {}, Valid_Si-sdr: {}'.format(\n",
    "            epoch+1,\n",
    "            train_loss.result(),\n",
    "            sisdr_Metric.result(),\n",
    "            valid_loss.result(),\n",
    "            val_sisdr_Metric.result()))\n",
    "        print('----------------------------------------------------------------------------------')\n",
    "\n",
    "        # Save Model\n",
    "        if valid_loss.result() < previous_loss:\n",
    "            filePath_temp = filePath.format(epoch+1, valid_loss.result())\n",
    "\n",
    "            vq_vae.save_weights(filePath_temp)\n",
    "            print('Epoch {}: val_loss improved from {} to {}, saving model to {}'.format(\n",
    "                epoch+1,\n",
    "                previous_loss,\n",
    "                valid_loss.result(),\n",
    "                filePath_temp))\n",
    "\n",
    "            previous_loss = valid_loss.result()\n",
    "        else:\n",
    "            print('Epoch {}: val_loss did not improve from {}'.format(\n",
    "                epoch+1,\n",
    "                previous_loss))\n",
    "        print()\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_loss.reset_states()\n",
    "        sisdr_Metric.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        val_sisdr_Metric.reset_states()\n",
    "\n",
    "        # Data shuffle at the end of each epoch\n",
    "        train_dataset.on_epoch_end()\n",
    "        valid_dataset.on_epoch_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-rescue",
   "metadata": {},
   "source": [
    "# 여기는 기존의 .fit() 함수를 사용해서 학습하는 부분임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax (Softmax)            (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 1024)        431552    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           430529    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 862,081\n",
      "Trainable params: 862,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 320.3536 - Si-sdr: -50.4173"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 16s 9s/step - loss: 320.3536 - Si-sdr: -50.4173 - val_loss: 319.5135 - val_Si-sdr: -54.4356\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 319.51349, saving model to ./CKPT\\CKP_ep_1__loss_319.51349_.h5\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 12s 8s/step - loss: 319.5452 - Si-sdr: -63.0467 - val_loss: 319.3685 - val_Si-sdr: -46.8890\n",
      "\n",
      "Epoch 00002: val_loss improved from 319.51349 to 319.36847, saving model to ./CKPT\\CKP_ep_2__loss_319.36847_.h5\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-26b44021cd1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_cb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_size = 1024\n",
    "epoch = 100\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_145__loss_66.64056_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "annoying-architect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_25 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_29 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "baking-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.9297633, 3.2949905], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(output_array, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b6c058c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-3.1354942, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-6.362304  -6.2342653 -6.2153444 ... -6.2276506 -6.2363305 -6.285157 ]\n",
      "  [-6.251078  -6.241573  -6.2366467 ... -6.352452  -6.4054537 -6.2555346]\n",
      "  [-6.2467375 -6.2661476 -6.127265  ... -6.364452  -6.3529105 -6.2357635]\n",
      "  ...\n",
      "  [-6.239136  -6.2338786 -6.2443104 ... -6.28445   -6.3654785 -6.2102475]\n",
      "  [-6.1631    -6.2846904 -6.1518383 ... -6.377404  -6.3104362 -6.228723 ]\n",
      "  [-6.2362022 -6.244498  -6.208463  ... -6.244081  -6.254401  -6.1157103]]\n",
      "\n",
      " [[-6.209783  -6.238675  -6.2208915 ... -6.280893  -6.160482  -6.2770023]\n",
      "  [-6.331647  -6.239924  -6.275844  ... -6.2127542 -6.210114  -6.258029 ]\n",
      "  [-6.229417  -6.2830906 -6.189748  ... -6.3235126 -6.3971066 -6.254447 ]\n",
      "  ...\n",
      "  [-6.2464485 -6.260875  -6.203493  ... -6.3047094 -6.3277907 -6.26268  ]\n",
      "  [-6.252694  -6.22891   -6.2109118 ... -6.2986064 -6.3151474 -6.2147765]\n",
      "  [-6.260398  -6.2353897 -6.1643496 ... -6.269767  -6.280548  -6.2419047]]], shape=(2, 10, 512), dtype=float32)\n",
      "tf.Tensor(0.003453666, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "qy = tf.nn.softmax(output_array)\n",
    "log_qy = tf.math.log(qy + 1e-10)\n",
    "log_uniform = qy * (log_qy - tf.math.log(1.0 / 512))\n",
    "kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "kl_loss = tf.reduce_mean(kl_loss) * 0.2\n",
    "print(tf.math.log(1.0 / 23))\n",
    "print(log_qy)\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00017277  0.04284319 -0.01188984 ... -0.01771124  0.01196702\n",
      "   -0.02395774]\n",
      "  [ 0.02425058  0.04362426  0.11081367 ... -0.04624727 -0.04631329\n",
      "   -0.01426021]\n",
      "  [ 0.01577644  0.09434726  0.13945295 ... -0.08329912 -0.03699685\n",
      "    0.068454  ]\n",
      "  ...\n",
      "  [-0.00547882  0.02288815  0.09831841 ... -0.04134181  0.00146184\n",
      "   -0.01383288]\n",
      "  [-0.02747884  0.06731428  0.06846403 ... -0.02201608  0.03026687\n",
      "    0.00815233]\n",
      "  [-0.07040555  0.03081928  0.01979109 ... -0.00238249  0.03648268\n",
      "    0.00656926]]\n",
      "\n",
      " [[-0.00376744  0.00843956 -0.00248334 ...  0.01591834 -0.00258182\n",
      "   -0.00995107]\n",
      "  [ 0.00349312  0.01588573  0.00413785 ...  0.00235852 -0.0046939\n",
      "   -0.00892225]\n",
      "  [ 0.00890357  0.01778383  0.04185111 ... -0.01746964 -0.01684867\n",
      "    0.01134531]\n",
      "  ...\n",
      "  [-0.07673643  0.13272269  0.04816541 ...  0.15960737 -0.06908371\n",
      "   -0.0568694 ]\n",
      "  [-0.01228268  0.10151526  0.04475912 ... -0.001128   -0.02404596\n",
      "    0.00599594]\n",
      "  [ 0.05445984  0.05042699  0.09397413 ... -0.03536536  0.00931732\n",
      "    0.01412403]]]\n",
      "(2, 10, 512)\n",
      "(2, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bacterial-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.012074914, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "qy = tf.nn.softmax(output_array)\n",
    "log_qy = tf.math.log(qy + 1e-10)\n",
    "log_uniform = qy * (log_qy - tf.math.log(1.0 / 512))\n",
    "kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n",
      "tf.Tensor(2.8309882, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "noise = output_array2 - target\n",
    "si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "si_sdr = tf.reduce_mean(si_sdr)\n",
    "print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[-0.03009652, -0.03612775, -0.06680483, -0.03670201],\n",
       "        [-0.04768711, -0.12344762, -0.03924457, -0.11762322],\n",
       "        [ 0.01808495, -0.16106637, -0.19467078, -0.15282159],\n",
       "        [-0.0986427 , -0.08625205, -0.12661007, -0.16366175],\n",
       "        [-0.09758376, -0.08886974, -0.0433558 , -0.19985165],\n",
       "        [-0.06933096, -0.03154394, -0.13725929, -0.20143284],\n",
       "        [ 0.03375649,  0.00182091, -0.01022564, -0.35924646],\n",
       "        [-0.01645333, -0.10466891, -0.13975918, -0.12066491],\n",
       "        [-0.13588801, -0.08173112, -0.00253745, -0.28615874]],\n",
       "\n",
       "       [[ 0.04865369, -0.02880372, -0.06414615, -0.07730438],\n",
       "        [-0.08225074, -0.03192509, -0.06216412, -0.08035193],\n",
       "        [-0.09515338,  0.04221668,  0.14230826, -0.23082384],\n",
       "        [-0.00094383,  0.05597762, -0.09290768, -0.08630683],\n",
       "        [-0.09894791, -0.04727853, -0.01004983, -0.30325216],\n",
       "        [ 0.01705559, -0.16948727, -0.08829505, -0.16453639],\n",
       "        [-0.07230186, -0.15348263, -0.06832955, -0.09588489],\n",
       "        [-0.1258373 ,  0.02068143,  0.07559443, -0.19079593],\n",
       "        [-0.08558049, -0.07712768, -0.07924994, -0.07352428]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
