{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_7 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 5s 2s/step - loss: 631.1898 - Si-sdr: -54.7975 - val_loss: 629.3751 - val_Si-sdr: -55.9275\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 627.62396\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 628.9716 - Si-sdr: -54.7156 - val_loss: 627.9314 - val_Si-sdr: -51.3649\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 627.62396\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 627.9067 - Si-sdr: -67.6253 - val_loss: 627.8530 - val_Si-sdr: -52.3506\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 627.62396\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 627.7100 - Si-sdr: -54.1356 - val_loss: 627.5006 - val_Si-sdr: -54.8263\n",
      "\n",
      "Epoch 00004: val_loss improved from 627.62396 to 627.50061, saving model to ./CKPT\\CKP_ep_4__loss_627.50061_.h5\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 627.4500 - Si-sdr: -50.3746 - val_loss: 627.3827 - val_Si-sdr: -56.9690\n",
      "\n",
      "Epoch 00005: val_loss improved from 627.50061 to 627.38269, saving model to ./CKPT\\CKP_ep_5__loss_627.38269_.h5\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 627.3099 - Si-sdr: -50.0483 - val_loss: 627.3123 - val_Si-sdr: -55.0616\n",
      "\n",
      "Epoch 00006: val_loss improved from 627.38269 to 627.31226, saving model to ./CKPT\\CKP_ep_6__loss_627.31226_.h5\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 627.3239 - Si-sdr: -78.4496 - val_loss: 627.3573 - val_Si-sdr: -47.1200\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 627.31226\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 627.2870 - Si-sdr: -59.0879 - val_loss: 627.2938 - val_Si-sdr: -61.3042\n",
      "\n",
      "Epoch 00008: val_loss improved from 627.31226 to 627.29382, saving model to ./CKPT\\CKP_ep_8__loss_627.29382_.h5\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 627.2976 - Si-sdr: -52.4802 - val_loss: 627.2897 - val_Si-sdr: -53.7101\n",
      "\n",
      "Epoch 00009: val_loss improved from 627.29382 to 627.28967, saving model to ./CKPT\\CKP_ep_9__loss_627.28967_.h5\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 627.2460 - Si-sdr: -61.3583 - val_loss: 627.2060 - val_Si-sdr: -54.5386\n",
      "\n",
      "Epoch 00010: val_loss improved from 627.28967 to 627.20599, saving model to ./CKPT\\CKP_ep_10__loss_627.20599_.h5\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 627.2313 - Si-sdr: -53.8131 - val_loss: 627.2098 - val_Si-sdr: -50.9370\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 627.20599\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 627.2132 - Si-sdr: -54.4431 - val_loss: 627.2305 - val_Si-sdr: -56.9221\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 627.20599\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 627.2261 - Si-sdr: -59.2952 - val_loss: 627.1964 - val_Si-sdr: -55.7114\n",
      "\n",
      "Epoch 00013: val_loss improved from 627.20599 to 627.19635, saving model to ./CKPT\\CKP_ep_13__loss_627.19635_.h5\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 627.1755 - Si-sdr: -47.5653 - val_loss: 627.2165 - val_Si-sdr: -62.7962\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 627.19635\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 627.2290 - Si-sdr: -57.9188 - val_loss: 627.1980 - val_Si-sdr: -56.3639\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 627.19635\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 627.2061 - Si-sdr: -53.6971 - val_loss: 627.1783 - val_Si-sdr: -57.3026\n",
      "\n",
      "Epoch 00016: val_loss improved from 627.19635 to 627.17834, saving model to ./CKPT\\CKP_ep_16__loss_627.17834_.h5\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 1s 778ms/step - loss: 627.1763 - Si-sdr: -47.8989 - val_loss: 627.1667 - val_Si-sdr: -50.6853\n",
      "\n",
      "Epoch 00017: val_loss improved from 627.17834 to 627.16675, saving model to ./CKPT\\CKP_ep_17__loss_627.16675_.h5\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 1s 808ms/step - loss: 627.1747 - Si-sdr: -49.2180 - val_loss: 627.1895 - val_Si-sdr: -51.0462\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 627.16675\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 627.1453 - Si-sdr: -41.2775 - val_loss: 627.1389 - val_Si-sdr: -41.6522\n",
      "\n",
      "Epoch 00019: val_loss improved from 627.16675 to 627.13892, saving model to ./CKPT\\CKP_ep_19__loss_627.13892_.h5\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 627.1606 - Si-sdr: -47.0757 - val_loss: 627.1543 - val_Si-sdr: -43.3702\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 627.13892\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 627.1583 - Si-sdr: -44.2397 - val_loss: 627.1001 - val_Si-sdr: -36.8657\n",
      "\n",
      "Epoch 00021: val_loss improved from 627.13892 to 627.10010, saving model to ./CKPT\\CKP_ep_21__loss_627.10010_.h5\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 1s 753ms/step - loss: 627.1241 - Si-sdr: -39.8790 - val_loss: 626.9805 - val_Si-sdr: -30.4495\n",
      "\n",
      "Epoch 00022: val_loss improved from 627.10010 to 626.98053, saving model to ./CKPT\\CKP_ep_22__loss_626.98053_.h5\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 626.9758 - Si-sdr: -30.7646 - val_loss: 626.7018 - val_Si-sdr: -25.0132\n",
      "\n",
      "Epoch 00023: val_loss improved from 626.98053 to 626.70184, saving model to ./CKPT\\CKP_ep_23__loss_626.70184_.h5\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 626.6146 - Si-sdr: -24.2910 - val_loss: 625.8381 - val_Si-sdr: -20.3539\n",
      "\n",
      "Epoch 00024: val_loss improved from 626.70184 to 625.83813, saving model to ./CKPT\\CKP_ep_24__loss_625.83813_.h5\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 1s 761ms/step - loss: 625.1802 - Si-sdr: -19.3019 - val_loss: 622.8867 - val_Si-sdr: -18.0815\n",
      "\n",
      "Epoch 00025: val_loss improved from 625.83813 to 622.88672, saving model to ./CKPT\\CKP_ep_25__loss_622.88672_.h5\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 1s 656ms/step - loss: 621.8976 - Si-sdr: -18.0031 - val_loss: 617.3434 - val_Si-sdr: -16.6258\n",
      "\n",
      "Epoch 00026: val_loss improved from 622.88672 to 617.34338, saving model to ./CKPT\\CKP_ep_26__loss_617.34338_.h5\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 1s 759ms/step - loss: 615.0672 - Si-sdr: -15.3887 - val_loss: 604.3138 - val_Si-sdr: -11.2392\n",
      "\n",
      "Epoch 00027: val_loss improved from 617.34338 to 604.31384, saving model to ./CKPT\\CKP_ep_27__loss_604.31384_.h5\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 599.4983 - Si-sdr: -10.5000 - val_loss: 582.9121 - val_Si-sdr: -9.1882\n",
      "\n",
      "Epoch 00028: val_loss improved from 604.31384 to 582.91211, saving model to ./CKPT\\CKP_ep_28__loss_582.91211_.h5\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 1s 662ms/step - loss: 577.8798 - Si-sdr: -9.2729 - val_loss: 558.9431 - val_Si-sdr: -8.8905\n",
      "\n",
      "Epoch 00029: val_loss improved from 582.91211 to 558.94312, saving model to ./CKPT\\CKP_ep_29__loss_558.94312_.h5\n",
      "Epoch 30/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 635ms/step - loss: 551.9090 - Si-sdr: -8.4797 - val_loss: 539.4868 - val_Si-sdr: -7.9204\n",
      "\n",
      "Epoch 00030: val_loss improved from 558.94312 to 539.48676, saving model to ./CKPT\\CKP_ep_30__loss_539.48676_.h5\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 1s 761ms/step - loss: 529.1852 - Si-sdr: -7.4093 - val_loss: 500.5460 - val_Si-sdr: -5.8928\n",
      "\n",
      "Epoch 00031: val_loss improved from 539.48676 to 500.54599, saving model to ./CKPT\\CKP_ep_31__loss_500.54599_.h5\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 495.7227 - Si-sdr: -5.5905 - val_loss: 470.5332 - val_Si-sdr: -4.2987\n",
      "\n",
      "Epoch 00032: val_loss improved from 500.54599 to 470.53323, saving model to ./CKPT\\CKP_ep_32__loss_470.53323_.h5\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 461.1436 - Si-sdr: -4.0021 - val_loss: 434.5842 - val_Si-sdr: -3.4026\n",
      "\n",
      "Epoch 00033: val_loss improved from 470.53323 to 434.58417, saving model to ./CKPT\\CKP_ep_33__loss_434.58417_.h5\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 1s 739ms/step - loss: 425.8848 - Si-sdr: -3.2436 - val_loss: 400.8071 - val_Si-sdr: -2.3767\n",
      "\n",
      "Epoch 00034: val_loss improved from 434.58417 to 400.80707, saving model to ./CKPT\\CKP_ep_34__loss_400.80707_.h5\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 394.6528 - Si-sdr: -2.2120 - val_loss: 373.8625 - val_Si-sdr: -1.5691\n",
      "\n",
      "Epoch 00035: val_loss improved from 400.80707 to 373.86249, saving model to ./CKPT\\CKP_ep_35__loss_373.86249_.h5\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 368.3984 - Si-sdr: -1.4856 - val_loss: 349.8377 - val_Si-sdr: -0.8945\n",
      "\n",
      "Epoch 00036: val_loss improved from 373.86249 to 349.83771, saving model to ./CKPT\\CKP_ep_36__loss_349.83771_.h5\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 348.1884 - Si-sdr: -0.8382 - val_loss: 336.3120 - val_Si-sdr: -0.5056\n",
      "\n",
      "Epoch 00037: val_loss improved from 349.83771 to 336.31198, saving model to ./CKPT\\CKP_ep_37__loss_336.31198_.h5\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 331.2685 - Si-sdr: -0.3380 - val_loss: 320.5056 - val_Si-sdr: -0.0701\n",
      "\n",
      "Epoch 00038: val_loss improved from 336.31198 to 320.50565, saving model to ./CKPT\\CKP_ep_38__loss_320.50565_.h5\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 316.1465 - Si-sdr: 0.0348 - val_loss: 305.6684 - val_Si-sdr: 0.3432\n",
      "\n",
      "Epoch 00039: val_loss improved from 320.50565 to 305.66840, saving model to ./CKPT\\CKP_ep_39__loss_305.66840_.h5\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 1s 758ms/step - loss: 303.2109 - Si-sdr: 0.4300 - val_loss: 295.0023 - val_Si-sdr: 0.5820\n",
      "\n",
      "Epoch 00040: val_loss improved from 305.66840 to 295.00226, saving model to ./CKPT\\CKP_ep_40__loss_295.00226_.h5\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 293.3299 - Si-sdr: 0.6578 - val_loss: 290.4803 - val_Si-sdr: 0.7226\n",
      "\n",
      "Epoch 00041: val_loss improved from 295.00226 to 290.48035, saving model to ./CKPT\\CKP_ep_41__loss_290.48035_.h5\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 285.5725 - Si-sdr: 0.8946 - val_loss: 278.3190 - val_Si-sdr: 1.0745\n",
      "\n",
      "Epoch 00042: val_loss improved from 290.48035 to 278.31903, saving model to ./CKPT\\CKP_ep_42__loss_278.31903_.h5\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 277.4320 - Si-sdr: 1.0930 - val_loss: 270.4144 - val_Si-sdr: 1.2694\n",
      "\n",
      "Epoch 00043: val_loss improved from 278.31903 to 270.41443, saving model to ./CKPT\\CKP_ep_43__loss_270.41443_.h5\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 269.2655 - Si-sdr: 1.3151 - val_loss: 263.5759 - val_Si-sdr: 1.4786\n",
      "\n",
      "Epoch 00044: val_loss improved from 270.41443 to 263.57590, saving model to ./CKPT\\CKP_ep_44__loss_263.57590_.h5\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 261.4705 - Si-sdr: 1.5543 - val_loss: 257.6462 - val_Si-sdr: 1.6579\n",
      "\n",
      "Epoch 00045: val_loss improved from 263.57590 to 257.64624, saving model to ./CKPT\\CKP_ep_45__loss_257.64624_.h5\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 255.2706 - Si-sdr: 1.7468 - val_loss: 249.4207 - val_Si-sdr: 1.8906\n",
      "\n",
      "Epoch 00046: val_loss improved from 257.64624 to 249.42075, saving model to ./CKPT\\CKP_ep_46__loss_249.42075_.h5\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 243.9739 - Si-sdr: 2.0458 - val_loss: 239.5410 - val_Si-sdr: 2.1626\n",
      "\n",
      "Epoch 00047: val_loss improved from 249.42075 to 239.54099, saving model to ./CKPT\\CKP_ep_47__loss_239.54099_.h5\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 238.8018 - Si-sdr: 2.1687 - val_loss: 230.4987 - val_Si-sdr: 2.4186\n",
      "\n",
      "Epoch 00048: val_loss improved from 239.54099 to 230.49875, saving model to ./CKPT\\CKP_ep_48__loss_230.49875_.h5\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 227.6813 - Si-sdr: 2.5481 - val_loss: 222.8942 - val_Si-sdr: 2.6746\n",
      "\n",
      "Epoch 00049: val_loss improved from 230.49875 to 222.89420, saving model to ./CKPT\\CKP_ep_49__loss_222.89420_.h5\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 220.9048 - Si-sdr: 2.7209 - val_loss: 214.0045 - val_Si-sdr: 2.9278\n",
      "\n",
      "Epoch 00050: val_loss improved from 222.89420 to 214.00455, saving model to ./CKPT\\CKP_ep_50__loss_214.00455_.h5\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 213.8071 - Si-sdr: 2.9036 - val_loss: 212.7130 - val_Si-sdr: 2.9490\n",
      "\n",
      "Epoch 00051: val_loss improved from 214.00455 to 212.71304, saving model to ./CKPT\\CKP_ep_51__loss_212.71304_.h5\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 209.2203 - Si-sdr: 3.0809 - val_loss: 203.9549 - val_Si-sdr: 3.2017\n",
      "\n",
      "Epoch 00052: val_loss improved from 212.71304 to 203.95490, saving model to ./CKPT\\CKP_ep_52__loss_203.95490_.h5\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 203.1577 - Si-sdr: 3.2508 - val_loss: 198.6606 - val_Si-sdr: 3.3666\n",
      "\n",
      "Epoch 00053: val_loss improved from 203.95490 to 198.66058, saving model to ./CKPT\\CKP_ep_53__loss_198.66058_.h5\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 198.1950 - Si-sdr: 3.3895 - val_loss: 192.9608 - val_Si-sdr: 3.5329\n",
      "\n",
      "Epoch 00054: val_loss improved from 198.66058 to 192.96082, saving model to ./CKPT\\CKP_ep_54__loss_192.96082_.h5\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 192.7311 - Si-sdr: 3.5449 - val_loss: 188.2294 - val_Si-sdr: 3.6771\n",
      "\n",
      "Epoch 00055: val_loss improved from 192.96082 to 188.22945, saving model to ./CKPT\\CKP_ep_55__loss_188.22945_.h5\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 187.0555 - Si-sdr: 3.7517 - val_loss: 183.8413 - val_Si-sdr: 3.8365\n",
      "\n",
      "Epoch 00056: val_loss improved from 188.22945 to 183.84131, saving model to ./CKPT\\CKP_ep_56__loss_183.84131_.h5\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 183.4398 - Si-sdr: 3.8373 - val_loss: 180.1599 - val_Si-sdr: 3.9630\n",
      "\n",
      "Epoch 00057: val_loss improved from 183.84131 to 180.15988, saving model to ./CKPT\\CKP_ep_57__loss_180.15988_.h5\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 180.1946 - Si-sdr: 3.9594 - val_loss: 175.9924 - val_Si-sdr: 4.0861\n",
      "\n",
      "Epoch 00058: val_loss improved from 180.15988 to 175.99242, saving model to ./CKPT\\CKP_ep_58__loss_175.99242_.h5\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 175.1042 - Si-sdr: 4.1367 - val_loss: 172.8824 - val_Si-sdr: 4.2208\n",
      "\n",
      "Epoch 00059: val_loss improved from 175.99242 to 172.88242, saving model to ./CKPT\\CKP_ep_59__loss_172.88242_.h5\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 172.2459 - Si-sdr: 4.2490 - val_loss: 171.5109 - val_Si-sdr: 4.2325\n",
      "\n",
      "Epoch 00060: val_loss improved from 172.88242 to 171.51089, saving model to ./CKPT\\CKP_ep_60__loss_171.51089_.h5\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 169.7360 - Si-sdr: 4.3035 - val_loss: 167.1271 - val_Si-sdr: 4.3874\n",
      "\n",
      "Epoch 00061: val_loss improved from 171.51089 to 167.12708, saving model to ./CKPT\\CKP_ep_61__loss_167.12708_.h5\n",
      "Epoch 62/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 642ms/step - loss: 166.6244 - Si-sdr: 4.4220 - val_loss: 164.0025 - val_Si-sdr: 4.4962\n",
      "\n",
      "Epoch 00062: val_loss improved from 167.12708 to 164.00252, saving model to ./CKPT\\CKP_ep_62__loss_164.00252_.h5\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 1s 796ms/step - loss: 164.3413 - Si-sdr: 4.4934 - val_loss: 160.7601 - val_Si-sdr: 4.6202\n",
      "\n",
      "Epoch 00063: val_loss improved from 164.00252 to 160.76009, saving model to ./CKPT\\CKP_ep_63__loss_160.76009_.h5\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 161.4836 - Si-sdr: 4.5870 - val_loss: 160.7765 - val_Si-sdr: 4.6097\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 160.76009\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 159.2918 - Si-sdr: 4.6650 - val_loss: 158.5458 - val_Si-sdr: 4.7085\n",
      "\n",
      "Epoch 00065: val_loss improved from 160.76009 to 158.54579, saving model to ./CKPT\\CKP_ep_65__loss_158.54579_.h5\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 158.0627 - Si-sdr: 4.6982 - val_loss: 156.1718 - val_Si-sdr: 4.7780\n",
      "\n",
      "Epoch 00066: val_loss improved from 158.54579 to 156.17181, saving model to ./CKPT\\CKP_ep_66__loss_156.17181_.h5\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 156.0251 - Si-sdr: 4.7620 - val_loss: 154.6184 - val_Si-sdr: 4.8253\n",
      "\n",
      "Epoch 00067: val_loss improved from 156.17181 to 154.61838, saving model to ./CKPT\\CKP_ep_67__loss_154.61838_.h5\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 154.0685 - Si-sdr: 4.8479 - val_loss: 156.2849 - val_Si-sdr: 4.7613\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 154.61838\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 154.8561 - Si-sdr: 4.7869 - val_loss: 149.1188 - val_Si-sdr: 5.0539\n",
      "\n",
      "Epoch 00069: val_loss improved from 154.61838 to 149.11877, saving model to ./CKPT\\CKP_ep_69__loss_149.11877_.h5\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 1s 765ms/step - loss: 147.9235 - Si-sdr: 5.0705 - val_loss: 147.3239 - val_Si-sdr: 5.1062\n",
      "\n",
      "Epoch 00070: val_loss improved from 149.11877 to 147.32394, saving model to ./CKPT\\CKP_ep_70__loss_147.32394_.h5\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 147.1887 - Si-sdr: 5.1144 - val_loss: 144.8703 - val_Si-sdr: 5.1822\n",
      "\n",
      "Epoch 00071: val_loss improved from 147.32394 to 144.87029, saving model to ./CKPT\\CKP_ep_71__loss_144.87029_.h5\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 145.3298 - Si-sdr: 5.1741 - val_loss: 142.8248 - val_Si-sdr: 5.2737\n",
      "\n",
      "Epoch 00072: val_loss improved from 144.87029 to 142.82483, saving model to ./CKPT\\CKP_ep_72__loss_142.82483_.h5\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 142.2028 - Si-sdr: 5.2857 - val_loss: 141.0386 - val_Si-sdr: 5.3279\n",
      "\n",
      "Epoch 00073: val_loss improved from 142.82483 to 141.03857, saving model to ./CKPT\\CKP_ep_73__loss_141.03857_.h5\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 139.9592 - Si-sdr: 5.3795 - val_loss: 138.0119 - val_Si-sdr: 5.4492\n",
      "\n",
      "Epoch 00074: val_loss improved from 141.03857 to 138.01190, saving model to ./CKPT\\CKP_ep_74__loss_138.01190_.h5\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 139.3168 - Si-sdr: 5.3919 - val_loss: 138.1234 - val_Si-sdr: 5.4547\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 138.01190\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 137.1970 - Si-sdr: 5.5161 - val_loss: 134.3186 - val_Si-sdr: 5.5935\n",
      "\n",
      "Epoch 00076: val_loss improved from 138.01190 to 134.31859, saving model to ./CKPT\\CKP_ep_76__loss_134.31859_.h5\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 1s 764ms/step - loss: 135.1676 - Si-sdr: 5.5723 - val_loss: 136.1316 - val_Si-sdr: 5.5262\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 134.31859\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 1s 758ms/step - loss: 134.9445 - Si-sdr: 5.5505 - val_loss: 134.9992 - val_Si-sdr: 5.5771\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 134.31859\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 135.0449 - Si-sdr: 5.5348 - val_loss: 131.9216 - val_Si-sdr: 5.6639\n",
      "\n",
      "Epoch 00079: val_loss improved from 134.31859 to 131.92155, saving model to ./CKPT\\CKP_ep_79__loss_131.92155_.h5\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 132.5125 - Si-sdr: 5.6415 - val_loss: 129.9933 - val_Si-sdr: 5.7558\n",
      "\n",
      "Epoch 00080: val_loss improved from 131.92155 to 129.99335, saving model to ./CKPT\\CKP_ep_80__loss_129.99335_.h5\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 129.5105 - Si-sdr: 5.7778 - val_loss: 131.2349 - val_Si-sdr: 5.7162\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 129.99335\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 129.2722 - Si-sdr: 5.7842 - val_loss: 127.3781 - val_Si-sdr: 5.8562\n",
      "\n",
      "Epoch 00082: val_loss improved from 129.99335 to 127.37806, saving model to ./CKPT\\CKP_ep_82__loss_127.37806_.h5\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 128.6730 - Si-sdr: 5.8285 - val_loss: 127.8246 - val_Si-sdr: 5.8464\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 127.37806\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 128.2582 - Si-sdr: 5.8226 - val_loss: 128.3413 - val_Si-sdr: 5.8013\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 127.37806\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 127.9035 - Si-sdr: 5.8213 - val_loss: 127.7374 - val_Si-sdr: 5.8387\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 127.37806\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 126.6802 - Si-sdr: 5.8705 - val_loss: 127.0013 - val_Si-sdr: 5.8529\n",
      "\n",
      "Epoch 00086: val_loss improved from 127.37806 to 127.00133, saving model to ./CKPT\\CKP_ep_86__loss_127.00133_.h5\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 127.1512 - Si-sdr: 5.8487 - val_loss: 127.0382 - val_Si-sdr: 5.8789\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 127.00133\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 127.6193 - Si-sdr: 5.8973 - val_loss: 125.2029 - val_Si-sdr: 5.9499\n",
      "\n",
      "Epoch 00088: val_loss improved from 127.00133 to 125.20291, saving model to ./CKPT\\CKP_ep_88__loss_125.20291_.h5\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 124.8256 - Si-sdr: 5.9618 - val_loss: 125.6836 - val_Si-sdr: 5.9422\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 125.20291\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 125.4329 - Si-sdr: 5.9489 - val_loss: 123.5665 - val_Si-sdr: 6.0179\n",
      "\n",
      "Epoch 00090: val_loss improved from 125.20291 to 123.56647, saving model to ./CKPT\\CKP_ep_90__loss_123.56647_.h5\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 124.2382 - Si-sdr: 5.9829 - val_loss: 121.9854 - val_Si-sdr: 6.0722\n",
      "\n",
      "Epoch 00091: val_loss improved from 123.56647 to 121.98544, saving model to ./CKPT\\CKP_ep_91__loss_121.98544_.h5\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 1s 656ms/step - loss: 121.8753 - Si-sdr: 6.0557 - val_loss: 121.9346 - val_Si-sdr: 6.0634\n",
      "\n",
      "Epoch 00092: val_loss improved from 121.98544 to 121.93462, saving model to ./CKPT\\CKP_ep_92__loss_121.93462_.h5\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 121.9493 - Si-sdr: 6.0766 - val_loss: 120.3237 - val_Si-sdr: 6.1390\n",
      "\n",
      "Epoch 00093: val_loss improved from 121.93462 to 120.32373, saving model to ./CKPT\\CKP_ep_93__loss_120.32373_.h5\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 120.9395 - Si-sdr: 6.1294 - val_loss: 119.1302 - val_Si-sdr: 6.1948\n",
      "\n",
      "Epoch 00094: val_loss improved from 120.32373 to 119.13025, saving model to ./CKPT\\CKP_ep_94__loss_119.13025_.h5\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 119.7317 - Si-sdr: 6.1559 - val_loss: 120.7478 - val_Si-sdr: 6.1299\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 119.13025\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 120.8356 - Si-sdr: 6.1176 - val_loss: 117.8344 - val_Si-sdr: 6.2458\n",
      "\n",
      "Epoch 00096: val_loss improved from 119.13025 to 117.83438, saving model to ./CKPT\\CKP_ep_96__loss_117.83438_.h5\n",
      "Epoch 97/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 678ms/step - loss: 119.1384 - Si-sdr: 6.2108 - val_loss: 120.0763 - val_Si-sdr: 6.1745\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 117.83438\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 118.4677 - Si-sdr: 6.2380 - val_loss: 117.4224 - val_Si-sdr: 6.2808\n",
      "\n",
      "Epoch 00098: val_loss improved from 117.83438 to 117.42240, saving model to ./CKPT\\CKP_ep_98__loss_117.42240_.h5\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 118.0248 - Si-sdr: 6.2593 - val_loss: 117.3988 - val_Si-sdr: 6.2730\n",
      "\n",
      "Epoch 00099: val_loss improved from 117.42240 to 117.39876, saving model to ./CKPT\\CKP_ep_99__loss_117.39876_.h5\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 118.2218 - Si-sdr: 6.2444 - val_loss: 117.0382 - val_Si-sdr: 6.3110\n",
      "\n",
      "Epoch 00100: val_loss improved from 117.39876 to 117.03818, saving model to ./CKPT\\CKP_ep_100__loss_117.03818_.h5\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 116.0714 - Si-sdr: 6.3369 - val_loss: 114.9549 - val_Si-sdr: 6.3897\n",
      "\n",
      "Epoch 00101: val_loss improved from 117.03818 to 114.95489, saving model to ./CKPT\\CKP_ep_101__loss_114.95489_.h5\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 116.8547 - Si-sdr: 6.3199 - val_loss: 116.9180 - val_Si-sdr: 6.3097\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 114.95489\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 1s 754ms/step - loss: 116.2396 - Si-sdr: 6.3206 - val_loss: 112.9479 - val_Si-sdr: 6.4710\n",
      "\n",
      "Epoch 00103: val_loss improved from 114.95489 to 112.94792, saving model to ./CKPT\\CKP_ep_103__loss_112.94792_.h5\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 113.7313 - Si-sdr: 6.4376 - val_loss: 114.0451 - val_Si-sdr: 6.4250\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 112.94792\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 114.0294 - Si-sdr: 6.4383 - val_loss: 112.8238 - val_Si-sdr: 6.4751\n",
      "\n",
      "Epoch 00105: val_loss improved from 112.94792 to 112.82384, saving model to ./CKPT\\CKP_ep_105__loss_112.82384_.h5\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 1s 723ms/step - loss: 112.1485 - Si-sdr: 6.4988 - val_loss: 114.2051 - val_Si-sdr: 6.3925\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 112.82384\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 113.7517 - Si-sdr: 6.4146 - val_loss: 112.9006 - val_Si-sdr: 6.4580\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 112.82384\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 111.4077 - Si-sdr: 6.5147 - val_loss: 110.9060 - val_Si-sdr: 6.5660\n",
      "\n",
      "Epoch 00108: val_loss improved from 112.82384 to 110.90601, saving model to ./CKPT\\CKP_ep_108__loss_110.90601_.h5\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 110.7369 - Si-sdr: 6.6109 - val_loss: 109.0618 - val_Si-sdr: 6.6486\n",
      "\n",
      "Epoch 00109: val_loss improved from 110.90601 to 109.06178, saving model to ./CKPT\\CKP_ep_109__loss_109.06178_.h5\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 1s 755ms/step - loss: 110.5389 - Si-sdr: 6.5958 - val_loss: 107.8281 - val_Si-sdr: 6.7256\n",
      "\n",
      "Epoch 00110: val_loss improved from 109.06178 to 107.82806, saving model to ./CKPT\\CKP_ep_110__loss_107.82806_.h5\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 1s 767ms/step - loss: 109.8959 - Si-sdr: 6.6172 - val_loss: 113.0127 - val_Si-sdr: 6.4488\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 107.82806\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 112.6863 - Si-sdr: 6.4946 - val_loss: 109.4869 - val_Si-sdr: 6.6529\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 107.82806\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 108.5724 - Si-sdr: 6.7132 - val_loss: 108.4801 - val_Si-sdr: 6.6644\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 107.82806\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 108.7752 - Si-sdr: 6.6759 - val_loss: 108.9713 - val_Si-sdr: 6.6945\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 107.82806\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 109.2590 - Si-sdr: 6.6333 - val_loss: 107.2000 - val_Si-sdr: 6.7250\n",
      "\n",
      "Epoch 00115: val_loss improved from 107.82806 to 107.19995, saving model to ./CKPT\\CKP_ep_115__loss_107.19995_.h5\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 108.3514 - Si-sdr: 6.6713 - val_loss: 105.7069 - val_Si-sdr: 6.8002\n",
      "\n",
      "Epoch 00116: val_loss improved from 107.19995 to 105.70694, saving model to ./CKPT\\CKP_ep_116__loss_105.70694_.h5\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 106.5245 - Si-sdr: 6.7568 - val_loss: 109.5795 - val_Si-sdr: 6.6198\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 105.70694\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 109.3154 - Si-sdr: 6.6655 - val_loss: 108.6013 - val_Si-sdr: 6.6785\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 105.70694\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 106.6473 - Si-sdr: 6.7673 - val_loss: 104.7024 - val_Si-sdr: 6.8612\n",
      "\n",
      "Epoch 00119: val_loss improved from 105.70694 to 104.70239, saving model to ./CKPT\\CKP_ep_119__loss_104.70239_.h5\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 106.2901 - Si-sdr: 6.7823 - val_loss: 104.7144 - val_Si-sdr: 6.8433\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 104.70239\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 103.7909 - Si-sdr: 6.9064 - val_loss: 103.5836 - val_Si-sdr: 6.9036\n",
      "\n",
      "Epoch 00121: val_loss improved from 104.70239 to 103.58357, saving model to ./CKPT\\CKP_ep_121__loss_103.58357_.h5\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 105.3326 - Si-sdr: 6.8252 - val_loss: 103.2053 - val_Si-sdr: 6.9333\n",
      "\n",
      "Epoch 00122: val_loss improved from 103.58357 to 103.20534, saving model to ./CKPT\\CKP_ep_122__loss_103.20534_.h5\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 103.8063 - Si-sdr: 6.8954 - val_loss: 101.6912 - val_Si-sdr: 6.9929\n",
      "\n",
      "Epoch 00123: val_loss improved from 103.20534 to 101.69122, saving model to ./CKPT\\CKP_ep_123__loss_101.69122_.h5\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 102.5371 - Si-sdr: 6.9534 - val_loss: 102.7239 - val_Si-sdr: 6.9235\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 101.69122\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 104.4850 - Si-sdr: 6.8361 - val_loss: 103.5271 - val_Si-sdr: 6.8969\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 101.69122\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 102.3157 - Si-sdr: 6.9635 - val_loss: 103.8848 - val_Si-sdr: 6.8980\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 101.69122\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 103.3759 - Si-sdr: 6.9107 - val_loss: 103.5861 - val_Si-sdr: 6.8963\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 101.69122\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 102.1260 - Si-sdr: 6.9684 - val_loss: 103.0720 - val_Si-sdr: 6.9362\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 101.69122\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 104.8677 - Si-sdr: 6.8344 - val_loss: 101.1505 - val_Si-sdr: 7.0214\n",
      "\n",
      "Epoch 00129: val_loss improved from 101.69122 to 101.15051, saving model to ./CKPT\\CKP_ep_129__loss_101.15051_.h5\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 1s 755ms/step - loss: 101.2590 - Si-sdr: 7.0177 - val_loss: 102.5226 - val_Si-sdr: 6.9610\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 101.15051\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 103.4148 - Si-sdr: 6.9059 - val_loss: 99.6055 - val_Si-sdr: 7.0900\n",
      "\n",
      "Epoch 00131: val_loss improved from 101.15051 to 99.60555, saving model to ./CKPT\\CKP_ep_131__loss_99.60555_.h5\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 100.8726 - Si-sdr: 7.0147 - val_loss: 99.1169 - val_Si-sdr: 7.1437\n",
      "\n",
      "Epoch 00132: val_loss improved from 99.60555 to 99.11692, saving model to ./CKPT\\CKP_ep_132__loss_99.11692_.h5\n",
      "Epoch 133/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 743ms/step - loss: 100.2040 - Si-sdr: 7.0830 - val_loss: 101.3407 - val_Si-sdr: 6.9951\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 99.11692\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 102.2211 - Si-sdr: 6.9868 - val_loss: 100.4242 - val_Si-sdr: 7.0739\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 99.11692\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 102.9584 - Si-sdr: 6.9218 - val_loss: 102.0878 - val_Si-sdr: 6.9714\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 99.11692\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 100.1880 - Si-sdr: 7.0627 - val_loss: 102.6768 - val_Si-sdr: 6.9483\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 99.11692\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 1s 730ms/step - loss: 100.6889 - Si-sdr: 7.0582 - val_loss: 100.5697 - val_Si-sdr: 7.0616\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 99.11692\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 98.9603 - Si-sdr: 7.1251 - val_loss: 98.7863 - val_Si-sdr: 7.1586\n",
      "\n",
      "Epoch 00138: val_loss improved from 99.11692 to 98.78632, saving model to ./CKPT\\CKP_ep_138__loss_98.78632_.h5\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 1s 759ms/step - loss: 100.0636 - Si-sdr: 7.0934 - val_loss: 98.8203 - val_Si-sdr: 7.1308\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 98.78632\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 1s 754ms/step - loss: 97.0693 - Si-sdr: 7.2571 - val_loss: 99.2915 - val_Si-sdr: 7.1640\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 98.78632\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 1s 730ms/step - loss: 99.8998 - Si-sdr: 7.1321 - val_loss: 97.4285 - val_Si-sdr: 7.2109\n",
      "\n",
      "Epoch 00141: val_loss improved from 98.78632 to 97.42847, saving model to ./CKPT\\CKP_ep_141__loss_97.42847_.h5\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 98.2629 - Si-sdr: 7.1730 - val_loss: 96.1408 - val_Si-sdr: 7.2897\n",
      "\n",
      "Epoch 00142: val_loss improved from 97.42847 to 96.14082, saving model to ./CKPT\\CKP_ep_142__loss_96.14082_.h5\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 97.3056 - Si-sdr: 7.2433 - val_loss: 96.8526 - val_Si-sdr: 7.2394\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 96.14082\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 97.4233 - Si-sdr: 7.1989 - val_loss: 97.6075 - val_Si-sdr: 7.2149\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 96.14082\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 1s 797ms/step - loss: 96.9790 - Si-sdr: 7.2430 - val_loss: 96.2526 - val_Si-sdr: 7.3024\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 96.14082\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 97.5899 - Si-sdr: 7.2151 - val_loss: 96.0641 - val_Si-sdr: 7.2766\n",
      "\n",
      "Epoch 00146: val_loss improved from 96.14082 to 96.06410, saving model to ./CKPT\\CKP_ep_146__loss_96.06410_.h5\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 96.0490 - Si-sdr: 7.2866 - val_loss: 97.3213 - val_Si-sdr: 7.2489\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 96.06410\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 1s 651ms/step - loss: 98.2043 - Si-sdr: 7.2036 - val_loss: 96.3550 - val_Si-sdr: 7.2669\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 96.06410\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 96.5575 - Si-sdr: 7.2911 - val_loss: 96.1798 - val_Si-sdr: 7.2934\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 96.06410\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 96.3412 - Si-sdr: 7.2409 - val_loss: 96.3715 - val_Si-sdr: 7.2730\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 96.06410\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 1s 869ms/step - loss: 97.6834 - Si-sdr: 7.2126 - val_loss: 95.0633 - val_Si-sdr: 7.3321\n",
      "\n",
      "Epoch 00151: val_loss improved from 96.06410 to 95.06329, saving model to ./CKPT\\CKP_ep_151__loss_95.06329_.h5\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 95.9548 - Si-sdr: 7.3244 - val_loss: 98.0111 - val_Si-sdr: 7.1749\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 95.06329\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 1s 755ms/step - loss: 98.8834 - Si-sdr: 7.1648 - val_loss: 96.6077 - val_Si-sdr: 7.2659\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 95.06329\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 97.5576 - Si-sdr: 7.2346 - val_loss: 94.4243 - val_Si-sdr: 7.3354\n",
      "\n",
      "Epoch 00154: val_loss improved from 95.06329 to 94.42429, saving model to ./CKPT\\CKP_ep_154__loss_94.42429_.h5\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 1s 754ms/step - loss: 94.7257 - Si-sdr: 7.3355 - val_loss: 95.7321 - val_Si-sdr: 7.2846\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 94.42429\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 1s 656ms/step - loss: 96.0396 - Si-sdr: 7.2778 - val_loss: 95.4250 - val_Si-sdr: 7.3079\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 94.42429\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 1s 761ms/step - loss: 95.4705 - Si-sdr: 7.3088 - val_loss: 94.5258 - val_Si-sdr: 7.3579\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 94.42429\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 94.2440 - Si-sdr: 7.3815 - val_loss: 94.0184 - val_Si-sdr: 7.3915\n",
      "\n",
      "Epoch 00158: val_loss improved from 94.42429 to 94.01839, saving model to ./CKPT\\CKP_ep_158__loss_94.01839_.h5\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 94.8977 - Si-sdr: 7.2968 - val_loss: 94.7458 - val_Si-sdr: 7.3531\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 94.01839\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 94.5399 - Si-sdr: 7.3496 - val_loss: 93.2502 - val_Si-sdr: 7.4344\n",
      "\n",
      "Epoch 00160: val_loss improved from 94.01839 to 93.25024, saving model to ./CKPT\\CKP_ep_160__loss_93.25024_.h5\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 94.5614 - Si-sdr: 7.3644 - val_loss: 92.4437 - val_Si-sdr: 7.4691\n",
      "\n",
      "Epoch 00161: val_loss improved from 93.25024 to 92.44373, saving model to ./CKPT\\CKP_ep_161__loss_92.44373_.h5\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 93.7253 - Si-sdr: 7.4263 - val_loss: 92.3399 - val_Si-sdr: 7.4894\n",
      "\n",
      "Epoch 00162: val_loss improved from 92.44373 to 92.33992, saving model to ./CKPT\\CKP_ep_162__loss_92.33992_.h5\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 93.5738 - Si-sdr: 7.4213 - val_loss: 92.7565 - val_Si-sdr: 7.4509\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 92.33992\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 92.1589 - Si-sdr: 7.4933 - val_loss: 92.4517 - val_Si-sdr: 7.4736\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 92.33992\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 92.7760 - Si-sdr: 7.4435 - val_loss: 92.6872 - val_Si-sdr: 7.4413\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 92.33992\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 92.3874 - Si-sdr: 7.4658 - val_loss: 90.3507 - val_Si-sdr: 7.5830\n",
      "\n",
      "Epoch 00166: val_loss improved from 92.33992 to 90.35068, saving model to ./CKPT\\CKP_ep_166__loss_90.35068_.h5\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 91.2762 - Si-sdr: 7.5300 - val_loss: 92.7556 - val_Si-sdr: 7.4335\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 90.35068\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 93.7493 - Si-sdr: 7.3860 - val_loss: 91.2209 - val_Si-sdr: 7.5317\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 90.35068\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 92.3884 - Si-sdr: 7.4896 - val_loss: 92.3933 - val_Si-sdr: 7.4904\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 90.35068\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 93.4034 - Si-sdr: 7.4115 - val_loss: 93.0828 - val_Si-sdr: 7.4358\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 90.35068\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 1s 664ms/step - loss: 91.6810 - Si-sdr: 7.5160 - val_loss: 91.1246 - val_Si-sdr: 7.5362\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 90.35068\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 745ms/step - loss: 92.7899 - Si-sdr: 7.4601 - val_loss: 94.2262 - val_Si-sdr: 7.3797\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 90.35068\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 94.8795 - Si-sdr: 7.3511 - val_loss: 92.1942 - val_Si-sdr: 7.4782\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 90.35068\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 92.6809 - Si-sdr: 7.4673 - val_loss: 91.9366 - val_Si-sdr: 7.4726\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 90.35068\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 94.0505 - Si-sdr: 7.3596 - val_loss: 93.7140 - val_Si-sdr: 7.4128\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 90.35068\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 92.4946 - Si-sdr: 7.4650 - val_loss: 93.0159 - val_Si-sdr: 7.4488\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 90.35068\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 93.3895 - Si-sdr: 7.4310 - val_loss: 91.5989 - val_Si-sdr: 7.5197\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 90.35068\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 92.9120 - Si-sdr: 7.4468 - val_loss: 93.4896 - val_Si-sdr: 7.4068\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 90.35068\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 93.6340 - Si-sdr: 7.4013 - val_loss: 92.5800 - val_Si-sdr: 7.4647\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 90.35068\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 91.7201 - Si-sdr: 7.5156 - val_loss: 90.8318 - val_Si-sdr: 7.5673\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 90.35068\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 91.3879 - Si-sdr: 7.5454 - val_loss: 89.5761 - val_Si-sdr: 7.6450\n",
      "\n",
      "Epoch 00181: val_loss improved from 90.35068 to 89.57615, saving model to ./CKPT\\CKP_ep_181__loss_89.57615_.h5\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 90.5944 - Si-sdr: 7.5862 - val_loss: 90.8831 - val_Si-sdr: 7.5634\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 89.57615\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 90.3151 - Si-sdr: 7.5947 - val_loss: 91.2589 - val_Si-sdr: 7.5445\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 89.57615\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 89.6719 - Si-sdr: 7.6141 - val_loss: 89.1556 - val_Si-sdr: 7.6514\n",
      "\n",
      "Epoch 00184: val_loss improved from 89.57615 to 89.15559, saving model to ./CKPT\\CKP_ep_184__loss_89.15559_.h5\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 89.1775 - Si-sdr: 7.6700 - val_loss: 89.7150 - val_Si-sdr: 7.6183\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 89.15559\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 89.9582 - Si-sdr: 7.6259 - val_loss: 89.0807 - val_Si-sdr: 7.6570\n",
      "\n",
      "Epoch 00186: val_loss improved from 89.15559 to 89.08073, saving model to ./CKPT\\CKP_ep_186__loss_89.08073_.h5\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 89.8981 - Si-sdr: 7.6101 - val_loss: 89.2962 - val_Si-sdr: 7.6548\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 89.08073\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 1s 648ms/step - loss: 90.1523 - Si-sdr: 7.5957 - val_loss: 88.6635 - val_Si-sdr: 7.6710\n",
      "\n",
      "Epoch 00188: val_loss improved from 89.08073 to 88.66351, saving model to ./CKPT\\CKP_ep_188__loss_88.66351_.h5\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 1s 763ms/step - loss: 90.5946 - Si-sdr: 7.5534 - val_loss: 90.8825 - val_Si-sdr: 7.5560\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 88.66351\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 89.7516 - Si-sdr: 7.5981 - val_loss: 90.3769 - val_Si-sdr: 7.5722\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 88.66351\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 91.1286 - Si-sdr: 7.5236 - val_loss: 88.9183 - val_Si-sdr: 7.6730\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 88.66351\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 89.4053 - Si-sdr: 7.6236 - val_loss: 88.9458 - val_Si-sdr: 7.6639\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 88.66351\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 89.7328 - Si-sdr: 7.6125 - val_loss: 87.9253 - val_Si-sdr: 7.7018\n",
      "\n",
      "Epoch 00193: val_loss improved from 88.66351 to 87.92531, saving model to ./CKPT\\CKP_ep_193__loss_87.92531_.h5\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 89.7737 - Si-sdr: 7.6140 - val_loss: 90.7280 - val_Si-sdr: 7.5606\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 87.92531\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 89.7414 - Si-sdr: 7.6190 - val_loss: 89.1628 - val_Si-sdr: 7.6352\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 87.92531\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 88.6581 - Si-sdr: 7.6741 - val_loss: 89.8576 - val_Si-sdr: 7.5983\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 87.92531\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 90.6435 - Si-sdr: 7.5468 - val_loss: 89.1464 - val_Si-sdr: 7.6489\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 87.92531\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 90.0319 - Si-sdr: 7.6147 - val_loss: 89.3504 - val_Si-sdr: 7.6216\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 87.92531\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 89.0120 - Si-sdr: 7.6655 - val_loss: 89.8568 - val_Si-sdr: 7.6148\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 87.92531\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 89.8038 - Si-sdr: 7.6259 - val_loss: 89.6594 - val_Si-sdr: 7.6224\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 87.92531\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 90.3250 - Si-sdr: 7.5881 - val_loss: 88.9120 - val_Si-sdr: 7.6551\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 87.92531\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 89.0926 - Si-sdr: 7.6541 - val_loss: 87.5312 - val_Si-sdr: 7.7433\n",
      "\n",
      "Epoch 00202: val_loss improved from 87.92531 to 87.53120, saving model to ./CKPT\\CKP_ep_202__loss_87.53120_.h5\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 88.3653 - Si-sdr: 7.6995 - val_loss: 88.9460 - val_Si-sdr: 7.6473\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 87.53120\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 88.9431 - Si-sdr: 7.6817 - val_loss: 88.3095 - val_Si-sdr: 7.6887\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 87.53120\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 89.0829 - Si-sdr: 7.6701 - val_loss: 88.4360 - val_Si-sdr: 7.6872\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 87.53120\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 88.7516 - Si-sdr: 7.6706 - val_loss: 88.0416 - val_Si-sdr: 7.7116\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 87.53120\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 88.7880 - Si-sdr: 7.6517 - val_loss: 87.1037 - val_Si-sdr: 7.7423\n",
      "\n",
      "Epoch 00207: val_loss improved from 87.53120 to 87.10373, saving model to ./CKPT\\CKP_ep_207__loss_87.10373_.h5\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 87.5882 - Si-sdr: 7.7309 - val_loss: 86.9380 - val_Si-sdr: 7.7731\n",
      "\n",
      "Epoch 00208: val_loss improved from 87.10373 to 86.93803, saving model to ./CKPT\\CKP_ep_208__loss_86.93803_.h5\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 86.9713 - Si-sdr: 7.7543 - val_loss: 87.3450 - val_Si-sdr: 7.7365\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 86.93803\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 87.0685 - Si-sdr: 7.7609 - val_loss: 87.1091 - val_Si-sdr: 7.7412\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 86.93803\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 88.7161 - Si-sdr: 7.6446 - val_loss: 87.8627 - val_Si-sdr: 7.7037\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 86.93803\n",
      "Epoch 212/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 745ms/step - loss: 88.6839 - Si-sdr: 7.6740 - val_loss: 87.3058 - val_Si-sdr: 7.7204\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 86.93803\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 89.0576 - Si-sdr: 7.6407 - val_loss: 87.3448 - val_Si-sdr: 7.7466\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 86.93803\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 88.7600 - Si-sdr: 7.6534 - val_loss: 88.6251 - val_Si-sdr: 7.6822\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 86.93803\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 90.2199 - Si-sdr: 7.6079 - val_loss: 88.5384 - val_Si-sdr: 7.6778\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 86.93803\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 86.9065 - Si-sdr: 7.7904 - val_loss: 86.2196 - val_Si-sdr: 7.8146\n",
      "\n",
      "Epoch 00216: val_loss improved from 86.93803 to 86.21959, saving model to ./CKPT\\CKP_ep_216__loss_86.21959_.h5\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 86.7348 - Si-sdr: 7.7765 - val_loss: 85.6945 - val_Si-sdr: 7.8476\n",
      "\n",
      "Epoch 00217: val_loss improved from 86.21959 to 85.69449, saving model to ./CKPT\\CKP_ep_217__loss_85.69449_.h5\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 86.4004 - Si-sdr: 7.8039 - val_loss: 85.9082 - val_Si-sdr: 7.8324\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 85.69449\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 86.1003 - Si-sdr: 7.8290 - val_loss: 84.7557 - val_Si-sdr: 7.8842\n",
      "\n",
      "Epoch 00219: val_loss improved from 85.69449 to 84.75566, saving model to ./CKPT\\CKP_ep_219__loss_84.75566_.h5\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 85.7154 - Si-sdr: 7.8258 - val_loss: 85.7829 - val_Si-sdr: 7.8263\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 84.75566\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 85.7957 - Si-sdr: 7.8463 - val_loss: 85.8532 - val_Si-sdr: 7.8143\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 84.75566\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 1s 727ms/step - loss: 86.9217 - Si-sdr: 7.7589 - val_loss: 86.0766 - val_Si-sdr: 7.8031\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 84.75566\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 86.8644 - Si-sdr: 7.7559 - val_loss: 86.3921 - val_Si-sdr: 7.8075\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 84.75566\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 86.4646 - Si-sdr: 7.8148 - val_loss: 86.8022 - val_Si-sdr: 7.7900\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 84.75566\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 86.2006 - Si-sdr: 7.8298 - val_loss: 85.8476 - val_Si-sdr: 7.8261\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 84.75566\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 85.0872 - Si-sdr: 7.8781 - val_loss: 84.9017 - val_Si-sdr: 7.8925\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 84.75566\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 84.7081 - Si-sdr: 7.8938 - val_loss: 85.1786 - val_Si-sdr: 7.8754\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 84.75566\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 86.0620 - Si-sdr: 7.8307 - val_loss: 85.2652 - val_Si-sdr: 7.8776\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 84.75566\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 85.1928 - Si-sdr: 7.8770 - val_loss: 84.1792 - val_Si-sdr: 7.9131\n",
      "\n",
      "Epoch 00229: val_loss improved from 84.75566 to 84.17919, saving model to ./CKPT\\CKP_ep_229__loss_84.17919_.h5\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 84.7557 - Si-sdr: 7.8763 - val_loss: 86.2113 - val_Si-sdr: 7.8048\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 84.17919\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 1s 771ms/step - loss: 86.7950 - Si-sdr: 7.7710 - val_loss: 86.7401 - val_Si-sdr: 7.7531\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 84.17919\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 86.0544 - Si-sdr: 7.7920 - val_loss: 85.1743 - val_Si-sdr: 7.8791\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 84.17919\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 87.9910 - Si-sdr: 7.7351 - val_loss: 85.2878 - val_Si-sdr: 7.8665\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 84.17919\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 1s 648ms/step - loss: 86.0392 - Si-sdr: 7.8275 - val_loss: 86.1007 - val_Si-sdr: 7.8535\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 84.17919\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 85.0071 - Si-sdr: 7.8930 - val_loss: 85.3079 - val_Si-sdr: 7.8617\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 84.17919\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 85.5786 - Si-sdr: 7.8693 - val_loss: 85.7512 - val_Si-sdr: 7.8298\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 84.17919\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 86.8324 - Si-sdr: 7.7386 - val_loss: 88.6004 - val_Si-sdr: 7.6811\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 84.17919\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 89.4596 - Si-sdr: 7.6169 - val_loss: 86.6162 - val_Si-sdr: 7.7760\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 84.17919\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 87.3542 - Si-sdr: 7.7748 - val_loss: 87.4471 - val_Si-sdr: 7.7662\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 84.17919\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 87.2869 - Si-sdr: 7.7466 - val_loss: 86.0914 - val_Si-sdr: 7.8004\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 84.17919\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 87.4412 - Si-sdr: 7.7407 - val_loss: 87.1957 - val_Si-sdr: 7.7859\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 84.17919\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 86.7553 - Si-sdr: 7.8102 - val_loss: 86.5877 - val_Si-sdr: 7.7794\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 84.17919\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 1s 739ms/step - loss: 87.5854 - Si-sdr: 7.7381 - val_loss: 86.7039 - val_Si-sdr: 7.7795\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 84.17919\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 88.8765 - Si-sdr: 7.6574 - val_loss: 86.1273 - val_Si-sdr: 7.8279\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 84.17919\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 86.2237 - Si-sdr: 7.8164 - val_loss: 85.1105 - val_Si-sdr: 7.8660\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 84.17919\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 86.1077 - Si-sdr: 7.8016 - val_loss: 86.0159 - val_Si-sdr: 7.8476\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 84.17919\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 87.1007 - Si-sdr: 7.7748 - val_loss: 84.9076 - val_Si-sdr: 7.8714\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 84.17919\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 86.3353 - Si-sdr: 7.8067 - val_loss: 84.7623 - val_Si-sdr: 7.8938\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 84.17919\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 85.0006 - Si-sdr: 7.8977 - val_loss: 83.9399 - val_Si-sdr: 7.9537\n",
      "\n",
      "Epoch 00249: val_loss improved from 84.17919 to 83.93993, saving model to ./CKPT\\CKP_ep_249__loss_83.93993_.h5\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 85.8388 - Si-sdr: 7.8214 - val_loss: 86.5722 - val_Si-sdr: 7.7886\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 83.93993\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 86.0798 - Si-sdr: 7.8228 - val_loss: 84.3695 - val_Si-sdr: 7.9280\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 83.93993\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 85.0278 - Si-sdr: 7.8781 - val_loss: 84.9137 - val_Si-sdr: 7.8927\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 83.93993\n",
      "Epoch 253/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 625ms/step - loss: 84.6828 - Si-sdr: 7.9043 - val_loss: 84.0106 - val_Si-sdr: 7.9372\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 83.93993\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 84.4676 - Si-sdr: 7.9095 - val_loss: 84.4423 - val_Si-sdr: 7.9149\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 83.93993\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 83.7947 - Si-sdr: 7.9508 - val_loss: 84.0053 - val_Si-sdr: 7.9387\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 83.93993\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 82.9870 - Si-sdr: 7.9975 - val_loss: 83.6136 - val_Si-sdr: 7.9546\n",
      "\n",
      "Epoch 00256: val_loss improved from 83.93993 to 83.61360, saving model to ./CKPT\\CKP_ep_256__loss_83.61360_.h5\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 83.7990 - Si-sdr: 7.9379 - val_loss: 83.5835 - val_Si-sdr: 7.9741\n",
      "\n",
      "Epoch 00257: val_loss improved from 83.61360 to 83.58347, saving model to ./CKPT\\CKP_ep_257__loss_83.58347_.h5\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 84.2465 - Si-sdr: 7.9221 - val_loss: 83.2632 - val_Si-sdr: 7.9690\n",
      "\n",
      "Epoch 00258: val_loss improved from 83.58347 to 83.26318, saving model to ./CKPT\\CKP_ep_258__loss_83.26318_.h5\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 83.7979 - Si-sdr: 7.9361 - val_loss: 83.6910 - val_Si-sdr: 7.9481\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 83.26318\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 83.1760 - Si-sdr: 7.9899 - val_loss: 83.3728 - val_Si-sdr: 7.9663\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 83.26318\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 1s 662ms/step - loss: 83.2869 - Si-sdr: 7.9758 - val_loss: 81.6311 - val_Si-sdr: 8.0725\n",
      "\n",
      "Epoch 00261: val_loss improved from 83.26318 to 81.63107, saving model to ./CKPT\\CKP_ep_261__loss_81.63107_.h5\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 81.7044 - Si-sdr: 8.0775 - val_loss: 82.6624 - val_Si-sdr: 8.0057\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 81.63107\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 83.5042 - Si-sdr: 7.9519 - val_loss: 82.8118 - val_Si-sdr: 8.0002\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 81.63107\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 82.8416 - Si-sdr: 8.0027 - val_loss: 82.6187 - val_Si-sdr: 8.0091\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 81.63107\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 83.4437 - Si-sdr: 7.9619 - val_loss: 82.3274 - val_Si-sdr: 8.0295\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 81.63107\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 83.1521 - Si-sdr: 7.9852 - val_loss: 83.0753 - val_Si-sdr: 7.9778\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 81.63107\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 83.5642 - Si-sdr: 7.9619 - val_loss: 81.0706 - val_Si-sdr: 8.1176\n",
      "\n",
      "Epoch 00267: val_loss improved from 81.63107 to 81.07059, saving model to ./CKPT\\CKP_ep_267__loss_81.07059_.h5\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 81.7309 - Si-sdr: 8.0712 - val_loss: 81.3647 - val_Si-sdr: 8.1076\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 81.07059\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 81.8057 - Si-sdr: 8.0659 - val_loss: 81.8247 - val_Si-sdr: 8.0636\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 81.07059\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 1s 727ms/step - loss: 82.5375 - Si-sdr: 8.0307 - val_loss: 82.1562 - val_Si-sdr: 8.0344\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 81.07059\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 81.4932 - Si-sdr: 8.0708 - val_loss: 82.2439 - val_Si-sdr: 8.0433\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 81.07059\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 81.2434 - Si-sdr: 8.0978 - val_loss: 81.5104 - val_Si-sdr: 8.0863\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 81.07059\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 81.6922 - Si-sdr: 8.0916 - val_loss: 81.5267 - val_Si-sdr: 8.0932\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 81.07059\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 82.3472 - Si-sdr: 8.0550 - val_loss: 81.9277 - val_Si-sdr: 8.0668\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 81.07059\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 81.5367 - Si-sdr: 8.0805 - val_loss: 82.1181 - val_Si-sdr: 8.0365\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 81.07059\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 81.7896 - Si-sdr: 8.0518 - val_loss: 82.4807 - val_Si-sdr: 8.0402\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 81.07059\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 1s 662ms/step - loss: 81.6949 - Si-sdr: 8.0958 - val_loss: 80.3638 - val_Si-sdr: 8.1503\n",
      "\n",
      "Epoch 00277: val_loss improved from 81.07059 to 80.36382, saving model to ./CKPT\\CKP_ep_277__loss_80.36382_.h5\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 80.6453 - Si-sdr: 8.1321 - val_loss: 81.7210 - val_Si-sdr: 8.0622\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 80.36382\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 81.6395 - Si-sdr: 8.0784 - val_loss: 81.4292 - val_Si-sdr: 8.0914\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 80.36382\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 81.4822 - Si-sdr: 8.0957 - val_loss: 81.2162 - val_Si-sdr: 8.0974\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 80.36382\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 81.4883 - Si-sdr: 8.0856 - val_loss: 82.5256 - val_Si-sdr: 8.0367\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 80.36382\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 82.5250 - Si-sdr: 8.0273 - val_loss: 80.8831 - val_Si-sdr: 8.1369\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 80.36382\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 1s 751ms/step - loss: 80.8960 - Si-sdr: 8.1504 - val_loss: 80.4265 - val_Si-sdr: 8.1447\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 80.36382\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 1s 753ms/step - loss: 81.3066 - Si-sdr: 8.1161 - val_loss: 80.0419 - val_Si-sdr: 8.1781\n",
      "\n",
      "Epoch 00284: val_loss improved from 80.36382 to 80.04192, saving model to ./CKPT\\CKP_ep_284__loss_80.04192_.h5\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 1s 679ms/step - loss: 81.1187 - Si-sdr: 8.0994 - val_loss: 80.7860 - val_Si-sdr: 8.1331\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 80.04192\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 1s 760ms/step - loss: 81.9978 - Si-sdr: 8.0561 - val_loss: 80.4619 - val_Si-sdr: 8.1524\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 80.04192\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 81.9250 - Si-sdr: 8.0726 - val_loss: 81.6100 - val_Si-sdr: 8.0872\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 80.04192\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 81.3452 - Si-sdr: 8.1244 - val_loss: 81.2521 - val_Si-sdr: 8.1182\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 80.04192\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 80.2213 - Si-sdr: 8.1820 - val_loss: 80.3620 - val_Si-sdr: 8.1489\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 80.04192\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 1s 613ms/step - loss: 80.6833 - Si-sdr: 8.1179 - val_loss: 80.9304 - val_Si-sdr: 8.1277\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 80.04192\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 80.0039 - Si-sdr: 8.1958 - val_loss: 80.1962 - val_Si-sdr: 8.1749\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 80.04192\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 80.1928 - Si-sdr: 8.1735 - val_loss: 80.5120 - val_Si-sdr: 8.1472\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 80.04192\n",
      "Epoch 293/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 636ms/step - loss: 81.1681 - Si-sdr: 8.1159 - val_loss: 80.6442 - val_Si-sdr: 8.1516\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 80.04192\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 80.6328 - Si-sdr: 8.1648 - val_loss: 80.0329 - val_Si-sdr: 8.1891\n",
      "\n",
      "Epoch 00294: val_loss improved from 80.04192 to 80.03293, saving model to ./CKPT\\CKP_ep_294__loss_80.03293_.h5\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 81.2953 - Si-sdr: 8.1300 - val_loss: 80.1810 - val_Si-sdr: 8.1770\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 80.03293\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 80.2740 - Si-sdr: 8.1797 - val_loss: 80.3847 - val_Si-sdr: 8.1620\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 80.03293\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 80.1794 - Si-sdr: 8.2024 - val_loss: 80.3840 - val_Si-sdr: 8.1725\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 80.03293\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 79.8623 - Si-sdr: 8.1975 - val_loss: 78.8994 - val_Si-sdr: 8.2412\n",
      "\n",
      "Epoch 00298: val_loss improved from 80.03293 to 78.89944, saving model to ./CKPT\\CKP_ep_298__loss_78.89944_.h5\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 79.9199 - Si-sdr: 8.2036 - val_loss: 79.7044 - val_Si-sdr: 8.2140\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 78.89944\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 81.2398 - Si-sdr: 8.1053 - val_loss: 79.1352 - val_Si-sdr: 8.2438\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 78.89944\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "annoying-architect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_25 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_29 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cheap-aircraft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.shape(output_array)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.62064394e-02  8.17203820e-02  3.24754268e-02 -1.02852143e-01]\n",
      "  [-1.49157234e-02 -1.95800625e-02 -3.46802175e-02  5.72501123e-03]\n",
      "  [-7.80363381e-02 -6.07307628e-02 -9.69931781e-02 -5.16347066e-02]\n",
      "  [-2.81637251e-01  9.96127948e-02 -3.17231864e-02  3.74453515e-03]\n",
      "  [-1.18284672e-01  8.46180767e-02 -1.95280939e-01  2.93599963e-02]\n",
      "  [-3.21779072e-01  1.22240782e-02 -1.09718263e-01  7.65489265e-02]\n",
      "  [-3.70774984e-01  2.58951426e-01 -1.33752108e-01 -1.99191421e-02]\n",
      "  [-3.68880987e-01 -1.90456361e-02 -1.05885044e-01 -5.90537339e-02]\n",
      "  [-3.16750824e-01 -3.97674739e-04 -1.01323225e-01 -1.01796679e-01]\n",
      "  [ 3.26998197e-02  1.05254844e-01  2.80397180e-02  1.57616232e-02]]\n",
      "\n",
      " [[-1.11196786e-01  1.94959119e-02 -1.16245545e-01 -3.68264988e-02]\n",
      "  [-5.72053492e-02 -1.78234074e-02 -5.17670363e-02 -2.48496141e-02]\n",
      "  [-1.82820857e-01  6.84121251e-02 -5.39908707e-02 -5.49857728e-02]\n",
      "  [-1.42236158e-01  1.38846397e-01 -5.72850741e-02 -4.44535911e-03]\n",
      "  [-2.06366882e-01  4.27138396e-02 -1.56833917e-01  6.27594069e-03]\n",
      "  [-4.88018572e-01  4.39002290e-02 -3.97626013e-02 -9.82996821e-03]\n",
      "  [-1.87694669e-01  1.65011704e-01 -1.40135974e-01 -6.42954111e-02]\n",
      "  [-1.10402979e-01 -1.56401396e-02 -5.35031892e-02  1.23977875e-02]\n",
      "  [-6.74375072e-02  3.96592766e-02 -3.17989774e-02  1.84933282e-02]\n",
      "  [-4.95021492e-02 -4.83743809e-02 -3.42924185e-02  9.29954275e-03]]]\n",
      "(2, 10, 4)\n",
      "(2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
