{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_mean(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax (Softmax)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - ETA: 0s - loss: 632.4242 - Si-sdr: -105.5548"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step - loss: 632.4242 - Si-sdr: -105.5548 - val_loss: 629.0930 - val_Si-sdr: -103.7000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 629.09296, saving model to ./CKPT\\CKP_ep_1__loss_629.09296_.h5\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 628.9652 - Si-sdr: -104.7301 - val_loss: 628.7811 - val_Si-sdr: -113.4062\n",
      "\n",
      "Epoch 00002: val_loss improved from 629.09296 to 628.78107, saving model to ./CKPT\\CKP_ep_2__loss_628.78107_.h5\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 628.2877 - Si-sdr: -97.2818 - val_loss: 627.7272 - val_Si-sdr: -97.2024\n",
      "\n",
      "Epoch 00003: val_loss improved from 628.78107 to 627.72723, saving model to ./CKPT\\CKP_ep_3__loss_627.72723_.h5\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 627.7675 - Si-sdr: -100.3658 - val_loss: 627.7426 - val_Si-sdr: -125.3593\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 627.72723\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 627.7139 - Si-sdr: -109.3509 - val_loss: 627.4990 - val_Si-sdr: -117.7947\n",
      "\n",
      "Epoch 00005: val_loss improved from 627.72723 to 627.49902, saving model to ./CKPT\\CKP_ep_5__loss_627.49902_.h5\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 627.4274 - Si-sdr: -118.2310 - val_loss: 627.2648 - val_Si-sdr: -126.6691\n",
      "\n",
      "Epoch 00006: val_loss improved from 627.49902 to 627.26477, saving model to ./CKPT\\CKP_ep_6__loss_627.26477_.h5\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 627.3580 - Si-sdr: -106.1414 - val_loss: 627.2838 - val_Si-sdr: -108.5199\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 627.26477\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 627.2562 - Si-sdr: -102.4171 - val_loss: 627.2842 - val_Si-sdr: -105.0907\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 627.26477\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 627.2979 - Si-sdr: -113.0857 - val_loss: 627.2578 - val_Si-sdr: -135.4706\n",
      "\n",
      "Epoch 00009: val_loss improved from 627.26477 to 627.25775, saving model to ./CKPT\\CKP_ep_9__loss_627.25775_.h5\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 627.2098 - Si-sdr: -100.8201 - val_loss: 627.2269 - val_Si-sdr: -97.5514\n",
      "\n",
      "Epoch 00010: val_loss improved from 627.25775 to 627.22687, saving model to ./CKPT\\CKP_ep_10__loss_627.22687_.h5\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 627.1748 - Si-sdr: -98.6447 - val_loss: 627.2039 - val_Si-sdr: -113.0885\n",
      "\n",
      "Epoch 00011: val_loss improved from 627.22687 to 627.20392, saving model to ./CKPT\\CKP_ep_11__loss_627.20392_.h5\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 627.1967 - Si-sdr: -104.9773 - val_loss: 627.2162 - val_Si-sdr: -109.0618\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 627.20392\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 627.1982 - Si-sdr: -101.5683 - val_loss: 627.2185 - val_Si-sdr: -110.5783\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 627.20392\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 627.2010 - Si-sdr: -100.4445 - val_loss: 627.2075 - val_Si-sdr: -102.1787\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 627.20392\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 627.2148 - Si-sdr: -108.2906 - val_loss: 627.1882 - val_Si-sdr: -98.5496\n",
      "\n",
      "Epoch 00015: val_loss improved from 627.20392 to 627.18823, saving model to ./CKPT\\CKP_ep_15__loss_627.18823_.h5\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 627.1761 - Si-sdr: -97.3461 - val_loss: 627.1858 - val_Si-sdr: -91.9188\n",
      "\n",
      "Epoch 00016: val_loss improved from 627.18823 to 627.18579, saving model to ./CKPT\\CKP_ep_16__loss_627.18579_.h5\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 627.2098 - Si-sdr: -119.8225 - val_loss: 627.1749 - val_Si-sdr: -93.4471\n",
      "\n",
      "Epoch 00017: val_loss improved from 627.18579 to 627.17493, saving model to ./CKPT\\CKP_ep_17__loss_627.17493_.h5\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 627.1696 - Si-sdr: -92.9153 - val_loss: 627.1289 - val_Si-sdr: -81.0123\n",
      "\n",
      "Epoch 00018: val_loss improved from 627.17493 to 627.12891, saving model to ./CKPT\\CKP_ep_18__loss_627.12891_.h5\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 1s 604ms/step - loss: 627.1392 - Si-sdr: -83.4884 - val_loss: 627.1076 - val_Si-sdr: -76.1194\n",
      "\n",
      "Epoch 00019: val_loss improved from 627.12891 to 627.10760, saving model to ./CKPT\\CKP_ep_19__loss_627.10760_.h5\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 627.0803 - Si-sdr: -76.1233 - val_loss: 626.9999 - val_Si-sdr: -65.8540\n",
      "\n",
      "Epoch 00020: val_loss improved from 627.10760 to 626.99988, saving model to ./CKPT\\CKP_ep_20__loss_626.99988_.h5\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 626.8961 - Si-sdr: -60.3769 - val_loss: 626.5875 - val_Si-sdr: -50.5784\n",
      "\n",
      "Epoch 00021: val_loss improved from 626.99988 to 626.58752, saving model to ./CKPT\\CKP_ep_21__loss_626.58752_.h5\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 626.4426 - Si-sdr: -48.7675 - val_loss: 625.3160 - val_Si-sdr: -40.2976\n",
      "\n",
      "Epoch 00022: val_loss improved from 626.58752 to 625.31598, saving model to ./CKPT\\CKP_ep_22__loss_625.31598_.h5\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 624.4373 - Si-sdr: -38.7502 - val_loss: 621.8973 - val_Si-sdr: -36.5519\n",
      "\n",
      "Epoch 00023: val_loss improved from 625.31598 to 621.89728, saving model to ./CKPT\\CKP_ep_23__loss_621.89728_.h5\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 621.2532 - Si-sdr: -36.8833 - val_loss: 616.4158 - val_Si-sdr: -32.9577\n",
      "\n",
      "Epoch 00024: val_loss improved from 621.89728 to 616.41577, saving model to ./CKPT\\CKP_ep_24__loss_616.41577_.h5\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 614.4136 - Si-sdr: -30.0999 - val_loss: 606.1949 - val_Si-sdr: -24.0666\n",
      "\n",
      "Epoch 00025: val_loss improved from 616.41577 to 606.19495, saving model to ./CKPT\\CKP_ep_25__loss_606.19495_.h5\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 603.0194 - Si-sdr: -23.0453 - val_loss: 590.8635 - val_Si-sdr: -21.6485\n",
      "\n",
      "Epoch 00026: val_loss improved from 606.19495 to 590.86353, saving model to ./CKPT\\CKP_ep_26__loss_590.86353_.h5\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 589.8835 - Si-sdr: -22.3192 - val_loss: 579.5807 - val_Si-sdr: -21.7689\n",
      "\n",
      "Epoch 00027: val_loss improved from 590.86353 to 579.58075, saving model to ./CKPT\\CKP_ep_27__loss_579.58075_.h5\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 575.4828 - Si-sdr: -21.2310 - val_loss: 564.1230 - val_Si-sdr: -19.0860\n",
      "\n",
      "Epoch 00028: val_loss improved from 579.58075 to 564.12305, saving model to ./CKPT\\CKP_ep_28__loss_564.12305_.h5\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 558.1097 - Si-sdr: -18.3987 - val_loss: 540.4881 - val_Si-sdr: -15.5890\n",
      "\n",
      "Epoch 00029: val_loss improved from 564.12305 to 540.48810, saving model to ./CKPT\\CKP_ep_29__loss_540.48810_.h5\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 532.5818 - Si-sdr: -14.2325 - val_loss: 514.1943 - val_Si-sdr: -12.4646\n",
      "\n",
      "Epoch 00030: val_loss improved from 540.48810 to 514.19427, saving model to ./CKPT\\CKP_ep_30__loss_514.19427_.h5\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 503.0030 - Si-sdr: -11.5703 - val_loss: 478.3438 - val_Si-sdr: -9.8347\n",
      "\n",
      "Epoch 00031: val_loss improved from 514.19427 to 478.34381, saving model to ./CKPT\\CKP_ep_31__loss_478.34381_.h5\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 471.6923 - Si-sdr: -9.4813 - val_loss: 452.9409 - val_Si-sdr: -8.1557\n",
      "\n",
      "Epoch 00032: val_loss improved from 478.34381 to 452.94092, saving model to ./CKPT\\CKP_ep_32__loss_452.94092_.h5\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 448.7635 - Si-sdr: -8.0119 - val_loss: 427.3105 - val_Si-sdr: -6.4851\n",
      "\n",
      "Epoch 00033: val_loss improved from 452.94092 to 427.31046, saving model to ./CKPT\\CKP_ep_33__loss_427.31046_.h5\n",
      "Epoch 34/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 595ms/step - loss: 422.8682 - Si-sdr: -6.0927 - val_loss: 403.6177 - val_Si-sdr: -5.0366\n",
      "\n",
      "Epoch 00034: val_loss improved from 427.31046 to 403.61774, saving model to ./CKPT\\CKP_ep_34__loss_403.61774_.h5\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 395.7642 - Si-sdr: -4.5781 - val_loss: 375.9176 - val_Si-sdr: -3.3972\n",
      "\n",
      "Epoch 00035: val_loss improved from 403.61774 to 375.91763, saving model to ./CKPT\\CKP_ep_35__loss_375.91763_.h5\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 368.6956 - Si-sdr: -3.1167 - val_loss: 351.5663 - val_Si-sdr: -2.0518\n",
      "\n",
      "Epoch 00036: val_loss improved from 375.91763 to 351.56631, saving model to ./CKPT\\CKP_ep_36__loss_351.56631_.h5\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 344.5897 - Si-sdr: -1.7126 - val_loss: 322.7979 - val_Si-sdr: -0.4553\n",
      "\n",
      "Epoch 00037: val_loss improved from 351.56631 to 322.79794, saving model to ./CKPT\\CKP_ep_37__loss_322.79794_.h5\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 313.6651 - Si-sdr: 0.1803 - val_loss: 293.3594 - val_Si-sdr: 1.3575\n",
      "\n",
      "Epoch 00038: val_loss improved from 322.79794 to 293.35941, saving model to ./CKPT\\CKP_ep_38__loss_293.35941_.h5\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 286.5402 - Si-sdr: 1.6960 - val_loss: 272.3697 - val_Si-sdr: 2.4063\n",
      "\n",
      "Epoch 00039: val_loss improved from 293.35941 to 272.36969, saving model to ./CKPT\\CKP_ep_39__loss_272.36969_.h5\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 270.9864 - Si-sdr: 2.6685 - val_loss: 254.9218 - val_Si-sdr: 3.4163\n",
      "\n",
      "Epoch 00040: val_loss improved from 272.36969 to 254.92178, saving model to ./CKPT\\CKP_ep_40__loss_254.92178_.h5\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 250.9836 - Si-sdr: 3.5905 - val_loss: 243.2168 - val_Si-sdr: 4.0227\n",
      "\n",
      "Epoch 00041: val_loss improved from 254.92178 to 243.21683, saving model to ./CKPT\\CKP_ep_41__loss_243.21683_.h5\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 1s 584ms/step - loss: 242.3289 - Si-sdr: 4.0649 - val_loss: 231.1244 - val_Si-sdr: 4.7271\n",
      "\n",
      "Epoch 00042: val_loss improved from 243.21683 to 231.12444, saving model to ./CKPT\\CKP_ep_42__loss_231.12444_.h5\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 229.8787 - Si-sdr: 4.7643 - val_loss: 223.7886 - val_Si-sdr: 5.1803\n",
      "\n",
      "Epoch 00043: val_loss improved from 231.12444 to 223.78860, saving model to ./CKPT\\CKP_ep_43__loss_223.78860_.h5\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 224.2353 - Si-sdr: 5.1225 - val_loss: 220.3037 - val_Si-sdr: 5.3825\n",
      "\n",
      "Epoch 00044: val_loss improved from 223.78860 to 220.30373, saving model to ./CKPT\\CKP_ep_44__loss_220.30373_.h5\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 1s 583ms/step - loss: 217.3861 - Si-sdr: 5.6240 - val_loss: 215.0858 - val_Si-sdr: 5.7579\n",
      "\n",
      "Epoch 00045: val_loss improved from 220.30373 to 215.08585, saving model to ./CKPT\\CKP_ep_45__loss_215.08585_.h5\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 214.5966 - Si-sdr: 5.6636 - val_loss: 208.2036 - val_Si-sdr: 6.1161\n",
      "\n",
      "Epoch 00046: val_loss improved from 215.08585 to 208.20355, saving model to ./CKPT\\CKP_ep_46__loss_208.20355_.h5\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 206.5332 - Si-sdr: 6.1779 - val_loss: 199.9531 - val_Si-sdr: 6.6565\n",
      "\n",
      "Epoch 00047: val_loss improved from 208.20355 to 199.95306, saving model to ./CKPT\\CKP_ep_47__loss_199.95306_.h5\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 199.1722 - Si-sdr: 6.6726 - val_loss: 197.4861 - val_Si-sdr: 6.7815\n",
      "\n",
      "Epoch 00048: val_loss improved from 199.95306 to 197.48607, saving model to ./CKPT\\CKP_ep_48__loss_197.48607_.h5\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 1s 599ms/step - loss: 193.9680 - Si-sdr: 7.0337 - val_loss: 190.6457 - val_Si-sdr: 7.2407\n",
      "\n",
      "Epoch 00049: val_loss improved from 197.48607 to 190.64566, saving model to ./CKPT\\CKP_ep_49__loss_190.64566_.h5\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 191.4672 - Si-sdr: 7.2151 - val_loss: 186.8628 - val_Si-sdr: 7.4828\n",
      "\n",
      "Epoch 00050: val_loss improved from 190.64566 to 186.86276, saving model to ./CKPT\\CKP_ep_50__loss_186.86276_.h5\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 187.1909 - Si-sdr: 7.4749 - val_loss: 185.5342 - val_Si-sdr: 7.5591\n",
      "\n",
      "Epoch 00051: val_loss improved from 186.86276 to 185.53419, saving model to ./CKPT\\CKP_ep_51__loss_185.53419_.h5\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 183.1700 - Si-sdr: 7.7004 - val_loss: 180.7458 - val_Si-sdr: 7.8776\n",
      "\n",
      "Epoch 00052: val_loss improved from 185.53419 to 180.74579, saving model to ./CKPT\\CKP_ep_52__loss_180.74579_.h5\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 180.7060 - Si-sdr: 7.8812 - val_loss: 176.6376 - val_Si-sdr: 8.1127\n",
      "\n",
      "Epoch 00053: val_loss improved from 180.74579 to 176.63759, saving model to ./CKPT\\CKP_ep_53__loss_176.63759_.h5\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 1s 593ms/step - loss: 176.3856 - Si-sdr: 8.1084 - val_loss: 175.0259 - val_Si-sdr: 8.2467\n",
      "\n",
      "Epoch 00054: val_loss improved from 176.63759 to 175.02591, saving model to ./CKPT\\CKP_ep_54__loss_175.02591_.h5\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 175.4401 - Si-sdr: 8.2566 - val_loss: 170.2900 - val_Si-sdr: 8.5598\n",
      "\n",
      "Epoch 00055: val_loss improved from 175.02591 to 170.29002, saving model to ./CKPT\\CKP_ep_55__loss_170.29002_.h5\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 172.4175 - Si-sdr: 8.4303 - val_loss: 167.2769 - val_Si-sdr: 8.7327\n",
      "\n",
      "Epoch 00056: val_loss improved from 170.29002 to 167.27693, saving model to ./CKPT\\CKP_ep_56__loss_167.27693_.h5\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 167.5611 - Si-sdr: 8.7830 - val_loss: 165.0426 - val_Si-sdr: 8.8876\n",
      "\n",
      "Epoch 00057: val_loss improved from 167.27693 to 165.04263, saving model to ./CKPT\\CKP_ep_57__loss_165.04263_.h5\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 163.9745 - Si-sdr: 9.0023 - val_loss: 163.6537 - val_Si-sdr: 8.9694\n",
      "\n",
      "Epoch 00058: val_loss improved from 165.04263 to 163.65375, saving model to ./CKPT\\CKP_ep_58__loss_163.65375_.h5\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 163.9027 - Si-sdr: 8.9640 - val_loss: 160.5363 - val_Si-sdr: 9.2039\n",
      "\n",
      "Epoch 00059: val_loss improved from 163.65375 to 160.53632, saving model to ./CKPT\\CKP_ep_59__loss_160.53632_.h5\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 159.2673 - Si-sdr: 9.2935 - val_loss: 157.7991 - val_Si-sdr: 9.3897\n",
      "\n",
      "Epoch 00060: val_loss improved from 160.53632 to 157.79910, saving model to ./CKPT\\CKP_ep_60__loss_157.79910_.h5\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 157.5565 - Si-sdr: 9.4318 - val_loss: 155.7871 - val_Si-sdr: 9.5231\n",
      "\n",
      "Epoch 00061: val_loss improved from 157.79910 to 155.78708, saving model to ./CKPT\\CKP_ep_61__loss_155.78708_.h5\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 1s 569ms/step - loss: 155.4587 - Si-sdr: 9.5616 - val_loss: 152.7373 - val_Si-sdr: 9.7432\n",
      "\n",
      "Epoch 00062: val_loss improved from 155.78708 to 152.73734, saving model to ./CKPT\\CKP_ep_62__loss_152.73734_.h5\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 152.9818 - Si-sdr: 9.7316 - val_loss: 150.6658 - val_Si-sdr: 9.8165\n",
      "\n",
      "Epoch 00063: val_loss improved from 152.73734 to 150.66577, saving model to ./CKPT\\CKP_ep_63__loss_150.66577_.h5\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 151.4718 - Si-sdr: 9.8355 - val_loss: 150.0329 - val_Si-sdr: 9.9103\n",
      "\n",
      "Epoch 00064: val_loss improved from 150.66577 to 150.03287, saving model to ./CKPT\\CKP_ep_64__loss_150.03287_.h5\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 149.9834 - Si-sdr: 9.9165 - val_loss: 149.5536 - val_Si-sdr: 9.9500\n",
      "\n",
      "Epoch 00065: val_loss improved from 150.03287 to 149.55362, saving model to ./CKPT\\CKP_ep_65__loss_149.55362_.h5\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 615ms/step - loss: 150.1463 - Si-sdr: 9.9170 - val_loss: 146.7481 - val_Si-sdr: 10.1684\n",
      "\n",
      "Epoch 00066: val_loss improved from 149.55362 to 146.74806, saving model to ./CKPT\\CKP_ep_66__loss_146.74806_.h5\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 145.6052 - Si-sdr: 10.2681 - val_loss: 145.4835 - val_Si-sdr: 10.2129\n",
      "\n",
      "Epoch 00067: val_loss improved from 146.74806 to 145.48346, saving model to ./CKPT\\CKP_ep_67__loss_145.48346_.h5\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 145.4962 - Si-sdr: 10.2116 - val_loss: 140.6793 - val_Si-sdr: 10.6240\n",
      "\n",
      "Epoch 00068: val_loss improved from 145.48346 to 140.67932, saving model to ./CKPT\\CKP_ep_68__loss_140.67932_.h5\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 141.4162 - Si-sdr: 10.5763 - val_loss: 142.9495 - val_Si-sdr: 10.4279\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 140.67932\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 143.1064 - Si-sdr: 10.4180 - val_loss: 140.0961 - val_Si-sdr: 10.6729\n",
      "\n",
      "Epoch 00070: val_loss improved from 140.67932 to 140.09607, saving model to ./CKPT\\CKP_ep_70__loss_140.09607_.h5\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 140.9782 - Si-sdr: 10.6468 - val_loss: 141.0556 - val_Si-sdr: 10.6198\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 140.09607\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 140.1978 - Si-sdr: 10.6959 - val_loss: 138.6139 - val_Si-sdr: 10.7917\n",
      "\n",
      "Epoch 00072: val_loss improved from 140.09607 to 138.61386, saving model to ./CKPT\\CKP_ep_72__loss_138.61386_.h5\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 139.5392 - Si-sdr: 10.7522 - val_loss: 136.8483 - val_Si-sdr: 10.9505\n",
      "\n",
      "Epoch 00073: val_loss improved from 138.61386 to 136.84827, saving model to ./CKPT\\CKP_ep_73__loss_136.84827_.h5\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 136.2751 - Si-sdr: 10.9966 - val_loss: 134.3541 - val_Si-sdr: 11.1167\n",
      "\n",
      "Epoch 00074: val_loss improved from 136.84827 to 134.35410, saving model to ./CKPT\\CKP_ep_74__loss_134.35410_.h5\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 135.5948 - Si-sdr: 11.0539 - val_loss: 133.2725 - val_Si-sdr: 11.2260\n",
      "\n",
      "Epoch 00075: val_loss improved from 134.35410 to 133.27246, saving model to ./CKPT\\CKP_ep_75__loss_133.27246_.h5\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 133.1259 - Si-sdr: 11.2904 - val_loss: 132.3268 - val_Si-sdr: 11.3028\n",
      "\n",
      "Epoch 00076: val_loss improved from 133.27246 to 132.32684, saving model to ./CKPT\\CKP_ep_76__loss_132.32684_.h5\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 1s 603ms/step - loss: 131.4156 - Si-sdr: 11.3497 - val_loss: 131.7901 - val_Si-sdr: 11.3574\n",
      "\n",
      "Epoch 00077: val_loss improved from 132.32684 to 131.79007, saving model to ./CKPT\\CKP_ep_77__loss_131.79007_.h5\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 131.7557 - Si-sdr: 11.4140 - val_loss: 130.8364 - val_Si-sdr: 11.3633\n",
      "\n",
      "Epoch 00078: val_loss improved from 131.79007 to 130.83640, saving model to ./CKPT\\CKP_ep_78__loss_130.83640_.h5\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 131.1384 - Si-sdr: 11.4282 - val_loss: 132.4099 - val_Si-sdr: 11.3199\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 130.83640\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 130.0761 - Si-sdr: 11.5439 - val_loss: 128.5333 - val_Si-sdr: 11.5700\n",
      "\n",
      "Epoch 00080: val_loss improved from 130.83640 to 128.53326, saving model to ./CKPT\\CKP_ep_80__loss_128.53326_.h5\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 128.0809 - Si-sdr: 11.6418 - val_loss: 126.4219 - val_Si-sdr: 11.7720\n",
      "\n",
      "Epoch 00081: val_loss improved from 128.53326 to 126.42191, saving model to ./CKPT\\CKP_ep_81__loss_126.42191_.h5\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 126.2280 - Si-sdr: 11.7801 - val_loss: 127.6705 - val_Si-sdr: 11.6624\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 126.42191\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 127.5551 - Si-sdr: 11.6885 - val_loss: 124.6488 - val_Si-sdr: 11.9128\n",
      "\n",
      "Epoch 00083: val_loss improved from 126.42191 to 124.64880, saving model to ./CKPT\\CKP_ep_83__loss_124.64880_.h5\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 124.8299 - Si-sdr: 11.8884 - val_loss: 127.1811 - val_Si-sdr: 11.7048\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 124.64880\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 125.0431 - Si-sdr: 11.8645 - val_loss: 123.9402 - val_Si-sdr: 11.9878\n",
      "\n",
      "Epoch 00085: val_loss improved from 124.64880 to 123.94022, saving model to ./CKPT\\CKP_ep_85__loss_123.94022_.h5\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 126.0013 - Si-sdr: 11.8066 - val_loss: 123.6594 - val_Si-sdr: 11.9645\n",
      "\n",
      "Epoch 00086: val_loss improved from 123.94022 to 123.65942, saving model to ./CKPT\\CKP_ep_86__loss_123.65942_.h5\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 123.4445 - Si-sdr: 11.9658 - val_loss: 122.8089 - val_Si-sdr: 12.0564\n",
      "\n",
      "Epoch 00087: val_loss improved from 123.65942 to 122.80888, saving model to ./CKPT\\CKP_ep_87__loss_122.80888_.h5\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 122.1069 - Si-sdr: 12.0809 - val_loss: 120.6463 - val_Si-sdr: 12.2457\n",
      "\n",
      "Epoch 00088: val_loss improved from 122.80888 to 120.64626, saving model to ./CKPT\\CKP_ep_88__loss_120.64626_.h5\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 1s 773ms/step - loss: 122.0331 - Si-sdr: 12.1414 - val_loss: 120.4703 - val_Si-sdr: 12.2527\n",
      "\n",
      "Epoch 00089: val_loss improved from 120.64626 to 120.47032, saving model to ./CKPT\\CKP_ep_89__loss_120.47032_.h5\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 120.2991 - Si-sdr: 12.2916 - val_loss: 120.7360 - val_Si-sdr: 12.2859\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 120.47032\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 119.7878 - Si-sdr: 12.3348 - val_loss: 118.7123 - val_Si-sdr: 12.3833\n",
      "\n",
      "Epoch 00091: val_loss improved from 120.47032 to 118.71234, saving model to ./CKPT\\CKP_ep_91__loss_118.71234_.h5\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 118.5804 - Si-sdr: 12.4546 - val_loss: 117.6934 - val_Si-sdr: 12.4569\n",
      "\n",
      "Epoch 00092: val_loss improved from 118.71234 to 117.69337, saving model to ./CKPT\\CKP_ep_92__loss_117.69337_.h5\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 119.4880 - Si-sdr: 12.3179 - val_loss: 118.5245 - val_Si-sdr: 12.4448\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 117.69337\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 119.5370 - Si-sdr: 12.3435 - val_loss: 118.8154 - val_Si-sdr: 12.4156\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 117.69337\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 119.9602 - Si-sdr: 12.3914 - val_loss: 118.7817 - val_Si-sdr: 12.4570\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 117.69337\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 119.6955 - Si-sdr: 12.3394 - val_loss: 121.8402 - val_Si-sdr: 12.2082\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 117.69337\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 124.9754 - Si-sdr: 11.9567 - val_loss: 122.1542 - val_Si-sdr: 12.1486\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 117.69337\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 1s 593ms/step - loss: 120.1099 - Si-sdr: 12.2659 - val_loss: 120.6823 - val_Si-sdr: 12.3286\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 117.69337\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 120.5313 - Si-sdr: 12.3406 - val_loss: 119.7883 - val_Si-sdr: 12.2965\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 117.69337\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 121.4498 - Si-sdr: 12.1952 - val_loss: 119.2005 - val_Si-sdr: 12.3882\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 117.69337\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 600ms/step - loss: 118.8343 - Si-sdr: 12.4125 - val_loss: 117.7218 - val_Si-sdr: 12.5529\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 117.69337\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 122.0416 - Si-sdr: 12.1758 - val_loss: 118.1114 - val_Si-sdr: 12.4787\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 117.69337\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 116.5713 - Si-sdr: 12.6002 - val_loss: 115.6869 - val_Si-sdr: 12.7075\n",
      "\n",
      "Epoch 00103: val_loss improved from 117.69337 to 115.68686, saving model to ./CKPT\\CKP_ep_103__loss_115.68686_.h5\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 115.0370 - Si-sdr: 12.7432 - val_loss: 113.9321 - val_Si-sdr: 12.8505\n",
      "\n",
      "Epoch 00104: val_loss improved from 115.68686 to 113.93214, saving model to ./CKPT\\CKP_ep_104__loss_113.93214_.h5\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 114.3345 - Si-sdr: 12.8274 - val_loss: 118.2314 - val_Si-sdr: 12.4993\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 113.93214\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 116.0734 - Si-sdr: 12.6619 - val_loss: 113.5775 - val_Si-sdr: 12.8735\n",
      "\n",
      "Epoch 00106: val_loss improved from 113.93214 to 113.57750, saving model to ./CKPT\\CKP_ep_106__loss_113.57750_.h5\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 115.1824 - Si-sdr: 12.7233 - val_loss: 111.6421 - val_Si-sdr: 13.0439\n",
      "\n",
      "Epoch 00107: val_loss improved from 113.57750 to 111.64209, saving model to ./CKPT\\CKP_ep_107__loss_111.64209_.h5\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 112.1706 - Si-sdr: 13.0713 - val_loss: 111.1408 - val_Si-sdr: 13.0849\n",
      "\n",
      "Epoch 00108: val_loss improved from 111.64209 to 111.14075, saving model to ./CKPT\\CKP_ep_108__loss_111.14075_.h5\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 111.2328 - Si-sdr: 13.0985 - val_loss: 112.2678 - val_Si-sdr: 12.9859\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 111.14075\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 1s 768ms/step - loss: 110.7394 - Si-sdr: 13.1104 - val_loss: 110.4326 - val_Si-sdr: 13.1968\n",
      "\n",
      "Epoch 00110: val_loss improved from 111.14075 to 110.43259, saving model to ./CKPT\\CKP_ep_110__loss_110.43259_.h5\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 110.6845 - Si-sdr: 13.1595 - val_loss: 109.3089 - val_Si-sdr: 13.2996\n",
      "\n",
      "Epoch 00111: val_loss improved from 110.43259 to 109.30887, saving model to ./CKPT\\CKP_ep_111__loss_109.30887_.h5\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 108.6924 - Si-sdr: 13.3354 - val_loss: 111.0004 - val_Si-sdr: 13.0653\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 109.30887\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 110.6382 - Si-sdr: 13.1464 - val_loss: 109.0741 - val_Si-sdr: 13.3193\n",
      "\n",
      "Epoch 00113: val_loss improved from 109.30887 to 109.07409, saving model to ./CKPT\\CKP_ep_113__loss_109.07409_.h5\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 108.8968 - Si-sdr: 13.3377 - val_loss: 108.7484 - val_Si-sdr: 13.2943\n",
      "\n",
      "Epoch 00114: val_loss improved from 109.07409 to 108.74836, saving model to ./CKPT\\CKP_ep_114__loss_108.74836_.h5\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 108.1111 - Si-sdr: 13.3695 - val_loss: 107.2996 - val_Si-sdr: 13.5190\n",
      "\n",
      "Epoch 00115: val_loss improved from 108.74836 to 107.29956, saving model to ./CKPT\\CKP_ep_115__loss_107.29956_.h5\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 108.5002 - Si-sdr: 13.3317 - val_loss: 107.6650 - val_Si-sdr: 13.4219\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 107.29956\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 106.9207 - Si-sdr: 13.4828 - val_loss: 107.5870 - val_Si-sdr: 13.4306\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 107.29956\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 1s 748ms/step - loss: 107.5930 - Si-sdr: 13.4355 - val_loss: 106.6824 - val_Si-sdr: 13.5180\n",
      "\n",
      "Epoch 00118: val_loss improved from 107.29956 to 106.68238, saving model to ./CKPT\\CKP_ep_118__loss_106.68238_.h5\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 1s 664ms/step - loss: 106.8776 - Si-sdr: 13.5149 - val_loss: 106.6580 - val_Si-sdr: 13.5588\n",
      "\n",
      "Epoch 00119: val_loss improved from 106.68238 to 106.65803, saving model to ./CKPT\\CKP_ep_119__loss_106.65803_.h5\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 107.0188 - Si-sdr: 13.5411 - val_loss: 106.4548 - val_Si-sdr: 13.5060\n",
      "\n",
      "Epoch 00120: val_loss improved from 106.65803 to 106.45485, saving model to ./CKPT\\CKP_ep_120__loss_106.45485_.h5\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 105.9142 - Si-sdr: 13.6120 - val_loss: 105.3104 - val_Si-sdr: 13.6478\n",
      "\n",
      "Epoch 00121: val_loss improved from 106.45485 to 105.31044, saving model to ./CKPT\\CKP_ep_121__loss_105.31044_.h5\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 1s 759ms/step - loss: 104.2619 - Si-sdr: 13.7429 - val_loss: 104.2164 - val_Si-sdr: 13.7566\n",
      "\n",
      "Epoch 00122: val_loss improved from 105.31044 to 104.21645, saving model to ./CKPT\\CKP_ep_122__loss_104.21645_.h5\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 105.5052 - Si-sdr: 13.6045 - val_loss: 104.5837 - val_Si-sdr: 13.6923\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 104.21645\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 104.7939 - Si-sdr: 13.6724 - val_loss: 105.1298 - val_Si-sdr: 13.6699\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 104.21645\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 104.0545 - Si-sdr: 13.7914 - val_loss: 104.2001 - val_Si-sdr: 13.7502\n",
      "\n",
      "Epoch 00125: val_loss improved from 104.21645 to 104.20007, saving model to ./CKPT\\CKP_ep_125__loss_104.20007_.h5\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 104.5380 - Si-sdr: 13.7400 - val_loss: 103.7368 - val_Si-sdr: 13.8243\n",
      "\n",
      "Epoch 00126: val_loss improved from 104.20007 to 103.73683, saving model to ./CKPT\\CKP_ep_126__loss_103.73683_.h5\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 104.9697 - Si-sdr: 13.7179 - val_loss: 103.2038 - val_Si-sdr: 13.8455\n",
      "\n",
      "Epoch 00127: val_loss improved from 103.73683 to 103.20380, saving model to ./CKPT\\CKP_ep_127__loss_103.20380_.h5\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 102.6222 - Si-sdr: 13.9244 - val_loss: 101.7120 - val_Si-sdr: 13.9961\n",
      "\n",
      "Epoch 00128: val_loss improved from 103.20380 to 101.71202, saving model to ./CKPT\\CKP_ep_128__loss_101.71202_.h5\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 103.2751 - Si-sdr: 13.8148 - val_loss: 103.0985 - val_Si-sdr: 13.8361\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 101.71202\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 1s 767ms/step - loss: 102.2710 - Si-sdr: 13.9359 - val_loss: 102.3581 - val_Si-sdr: 13.9291\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 101.71202\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 102.4714 - Si-sdr: 13.9296 - val_loss: 101.7773 - val_Si-sdr: 13.9540\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 101.71202\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 103.6361 - Si-sdr: 13.7868 - val_loss: 104.9792 - val_Si-sdr: 13.6425\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 101.71202\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 104.1373 - Si-sdr: 13.7205 - val_loss: 101.7195 - val_Si-sdr: 13.9562\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 101.71202\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 1s 649ms/step - loss: 102.1451 - Si-sdr: 13.8950 - val_loss: 100.1692 - val_Si-sdr: 14.1272\n",
      "\n",
      "Epoch 00134: val_loss improved from 101.71202 to 100.16920, saving model to ./CKPT\\CKP_ep_134__loss_100.16920_.h5\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 101.6339 - Si-sdr: 13.9653 - val_loss: 99.4184 - val_Si-sdr: 14.2319\n",
      "\n",
      "Epoch 00135: val_loss improved from 100.16920 to 99.41837, saving model to ./CKPT\\CKP_ep_135__loss_99.41837_.h5\n",
      "Epoch 136/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 740ms/step - loss: 100.6518 - Si-sdr: 14.0932 - val_loss: 100.2658 - val_Si-sdr: 14.1385\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 99.41837\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 100.2819 - Si-sdr: 14.1614 - val_loss: 98.9753 - val_Si-sdr: 14.2400\n",
      "\n",
      "Epoch 00137: val_loss improved from 99.41837 to 98.97527, saving model to ./CKPT\\CKP_ep_137__loss_98.97527_.h5\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 1s 798ms/step - loss: 99.7189 - Si-sdr: 14.1962 - val_loss: 99.4884 - val_Si-sdr: 14.2340\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 98.97527\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 101.2514 - Si-sdr: 14.0133 - val_loss: 100.1970 - val_Si-sdr: 14.1272\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 98.97527\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 101.0887 - Si-sdr: 14.0374 - val_loss: 99.5385 - val_Si-sdr: 14.2087\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 98.97527\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 100.6516 - Si-sdr: 14.0658 - val_loss: 99.7066 - val_Si-sdr: 14.1815\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 98.97527\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 99.5540 - Si-sdr: 14.1843 - val_loss: 99.6358 - val_Si-sdr: 14.1686\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 98.97527\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 99.5050 - Si-sdr: 14.1915 - val_loss: 98.6662 - val_Si-sdr: 14.3148\n",
      "\n",
      "Epoch 00143: val_loss improved from 98.97527 to 98.66624, saving model to ./CKPT\\CKP_ep_143__loss_98.66624_.h5\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 98.2347 - Si-sdr: 14.3490 - val_loss: 97.6676 - val_Si-sdr: 14.3550\n",
      "\n",
      "Epoch 00144: val_loss improved from 98.66624 to 97.66763, saving model to ./CKPT\\CKP_ep_144__loss_97.66763_.h5\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 98.4321 - Si-sdr: 14.3088 - val_loss: 98.7036 - val_Si-sdr: 14.2829\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 97.66763\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 99.3883 - Si-sdr: 14.2011 - val_loss: 98.5371 - val_Si-sdr: 14.2748\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 97.66763\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 97.1748 - Si-sdr: 14.4160 - val_loss: 97.7682 - val_Si-sdr: 14.3752\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 97.66763\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 99.3467 - Si-sdr: 14.1688 - val_loss: 99.0610 - val_Si-sdr: 14.2371\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 97.66763\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 100.6952 - Si-sdr: 14.0906 - val_loss: 96.9579 - val_Si-sdr: 14.4255\n",
      "\n",
      "Epoch 00149: val_loss improved from 97.66763 to 96.95789, saving model to ./CKPT\\CKP_ep_149__loss_96.95789_.h5\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 97.5891 - Si-sdr: 14.4988 - val_loss: 98.0517 - val_Si-sdr: 14.4189\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 96.95789\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 1s 648ms/step - loss: 97.5301 - Si-sdr: 14.4432 - val_loss: 97.8599 - val_Si-sdr: 14.4292\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 96.95789\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 96.9602 - Si-sdr: 14.4975 - val_loss: 96.5337 - val_Si-sdr: 14.5197\n",
      "\n",
      "Epoch 00152: val_loss improved from 96.95789 to 96.53374, saving model to ./CKPT\\CKP_ep_152__loss_96.53374_.h5\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 96.9101 - Si-sdr: 14.4666 - val_loss: 95.8284 - val_Si-sdr: 14.5615\n",
      "\n",
      "Epoch 00153: val_loss improved from 96.53374 to 95.82843, saving model to ./CKPT\\CKP_ep_153__loss_95.82843_.h5\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 96.8031 - Si-sdr: 14.4902 - val_loss: 96.0168 - val_Si-sdr: 14.5407\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 95.82843\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 95.8137 - Si-sdr: 14.5976 - val_loss: 95.2768 - val_Si-sdr: 14.6354\n",
      "\n",
      "Epoch 00155: val_loss improved from 95.82843 to 95.27676, saving model to ./CKPT\\CKP_ep_155__loss_95.27676_.h5\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 1s 754ms/step - loss: 96.4306 - Si-sdr: 14.5612 - val_loss: 95.5156 - val_Si-sdr: 14.5889\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 95.27676\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 95.7250 - Si-sdr: 14.5788 - val_loss: 96.2268 - val_Si-sdr: 14.5179\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 95.27676\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 95.7847 - Si-sdr: 14.5992 - val_loss: 95.1126 - val_Si-sdr: 14.6355\n",
      "\n",
      "Epoch 00158: val_loss improved from 95.27676 to 95.11263, saving model to ./CKPT\\CKP_ep_158__loss_95.11263_.h5\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 95.2126 - Si-sdr: 14.6280 - val_loss: 96.6838 - val_Si-sdr: 14.4656\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 95.11263\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 96.0681 - Si-sdr: 14.5848 - val_loss: 96.9566 - val_Si-sdr: 14.4149\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 95.11263\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 97.8262 - Si-sdr: 14.4194 - val_loss: 96.0022 - val_Si-sdr: 14.5697\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 95.11263\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 96.9490 - Si-sdr: 14.4078 - val_loss: 95.6735 - val_Si-sdr: 14.6254\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 95.11263\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 1s 784ms/step - loss: 97.3783 - Si-sdr: 14.4426 - val_loss: 96.0357 - val_Si-sdr: 14.5711\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 95.11263\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 96.0994 - Si-sdr: 14.5527 - val_loss: 97.3194 - val_Si-sdr: 14.4107\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 95.11263\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 1s 799ms/step - loss: 96.4090 - Si-sdr: 14.5204 - val_loss: 96.3151 - val_Si-sdr: 14.5324\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 95.11263\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 95.9351 - Si-sdr: 14.5535 - val_loss: 94.2393 - val_Si-sdr: 14.7612\n",
      "\n",
      "Epoch 00166: val_loss improved from 95.11263 to 94.23929, saving model to ./CKPT\\CKP_ep_166__loss_94.23929_.h5\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 1s 757ms/step - loss: 95.6186 - Si-sdr: 14.6472 - val_loss: 93.6217 - val_Si-sdr: 14.8045\n",
      "\n",
      "Epoch 00167: val_loss improved from 94.23929 to 93.62166, saving model to ./CKPT\\CKP_ep_167__loss_93.62166_.h5\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 95.2897 - Si-sdr: 14.6370 - val_loss: 95.2690 - val_Si-sdr: 14.6038\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 93.62166\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 95.6201 - Si-sdr: 14.5524 - val_loss: 94.6059 - val_Si-sdr: 14.6756\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 93.62166\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 96.8196 - Si-sdr: 14.4346 - val_loss: 97.6138 - val_Si-sdr: 14.3656\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 93.62166\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 95.8164 - Si-sdr: 14.6076 - val_loss: 95.9423 - val_Si-sdr: 14.5629\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 93.62166\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 95.6629 - Si-sdr: 14.5945 - val_loss: 95.8906 - val_Si-sdr: 14.5543\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 93.62166\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 94.6457 - Si-sdr: 14.7161 - val_loss: 94.1557 - val_Si-sdr: 14.7289\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 93.62166\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 1s 726ms/step - loss: 93.3523 - Si-sdr: 14.8410 - val_loss: 94.9458 - val_Si-sdr: 14.6674\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 93.62166\n",
      "Epoch 175/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 723ms/step - loss: 92.9093 - Si-sdr: 14.8797 - val_loss: 93.8041 - val_Si-sdr: 14.7851\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 93.62166\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 94.6859 - Si-sdr: 14.7421 - val_loss: 96.3977 - val_Si-sdr: 14.5039\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 93.62166\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 94.9724 - Si-sdr: 14.6589 - val_loss: 93.3468 - val_Si-sdr: 14.8345\n",
      "\n",
      "Epoch 00177: val_loss improved from 93.62166 to 93.34680, saving model to ./CKPT\\CKP_ep_177__loss_93.34680_.h5\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 93.6809 - Si-sdr: 14.8126 - val_loss: 92.2497 - val_Si-sdr: 14.9626\n",
      "\n",
      "Epoch 00178: val_loss improved from 93.34680 to 92.24974, saving model to ./CKPT\\CKP_ep_178__loss_92.24974_.h5\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 93.5774 - Si-sdr: 14.8132 - val_loss: 91.6540 - val_Si-sdr: 14.9991\n",
      "\n",
      "Epoch 00179: val_loss improved from 92.24974 to 91.65399, saving model to ./CKPT\\CKP_ep_179__loss_91.65399_.h5\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 92.4973 - Si-sdr: 14.9659 - val_loss: 93.9332 - val_Si-sdr: 14.7824\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 91.65399\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 93.9592 - Si-sdr: 14.7609 - val_loss: 93.7792 - val_Si-sdr: 14.7889\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 91.65399\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 94.9950 - Si-sdr: 14.6814 - val_loss: 95.8983 - val_Si-sdr: 14.5674\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 91.65399\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 97.2041 - Si-sdr: 14.4454 - val_loss: 93.0161 - val_Si-sdr: 14.9089\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 91.65399\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 93.9354 - Si-sdr: 14.8281 - val_loss: 93.8097 - val_Si-sdr: 14.7896\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 91.65399\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 94.1286 - Si-sdr: 14.7819 - val_loss: 98.2742 - val_Si-sdr: 14.3352\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 91.65399\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 96.0942 - Si-sdr: 14.5700 - val_loss: 94.3018 - val_Si-sdr: 14.6999\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 91.65399\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 94.9772 - Si-sdr: 14.6360 - val_loss: 93.4842 - val_Si-sdr: 14.8287\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 91.65399\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 94.3658 - Si-sdr: 14.7282 - val_loss: 93.4582 - val_Si-sdr: 14.8388\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 91.65399\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 93.7157 - Si-sdr: 14.8046 - val_loss: 92.5354 - val_Si-sdr: 14.9132\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 91.65399\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 91.9624 - Si-sdr: 14.9791 - val_loss: 91.7661 - val_Si-sdr: 15.0159\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 91.65399\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 92.2905 - Si-sdr: 14.9663 - val_loss: 91.8997 - val_Si-sdr: 14.9880\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 91.65399\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 93.1628 - Si-sdr: 14.8646 - val_loss: 92.0291 - val_Si-sdr: 14.9914\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 91.65399\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 91.2565 - Si-sdr: 15.0827 - val_loss: 90.8494 - val_Si-sdr: 15.1007\n",
      "\n",
      "Epoch 00193: val_loss improved from 91.65399 to 90.84940, saving model to ./CKPT\\CKP_ep_193__loss_90.84940_.h5\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 92.0460 - Si-sdr: 15.0515 - val_loss: 90.5515 - val_Si-sdr: 15.2184\n",
      "\n",
      "Epoch 00194: val_loss improved from 90.84940 to 90.55154, saving model to ./CKPT\\CKP_ep_194__loss_90.55154_.h5\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 91.9982 - Si-sdr: 15.0133 - val_loss: 92.7035 - val_Si-sdr: 14.9048\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 90.55154\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 91.1794 - Si-sdr: 15.0974 - val_loss: 90.5635 - val_Si-sdr: 15.1598\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 90.55154\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 90.7328 - Si-sdr: 15.1734 - val_loss: 90.7493 - val_Si-sdr: 15.1205\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 90.55154\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 90.4102 - Si-sdr: 15.1876 - val_loss: 90.0961 - val_Si-sdr: 15.2486\n",
      "\n",
      "Epoch 00198: val_loss improved from 90.55154 to 90.09608, saving model to ./CKPT\\CKP_ep_198__loss_90.09608_.h5\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 90.7112 - Si-sdr: 15.1815 - val_loss: 89.9962 - val_Si-sdr: 15.2034\n",
      "\n",
      "Epoch 00199: val_loss improved from 90.09608 to 89.99623, saving model to ./CKPT\\CKP_ep_199__loss_89.99623_.h5\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 90.2736 - Si-sdr: 15.1263 - val_loss: 91.0639 - val_Si-sdr: 15.1293\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 89.99623\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 90.6335 - Si-sdr: 15.1200 - val_loss: 90.5041 - val_Si-sdr: 15.1424\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 89.99623\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 90.4386 - Si-sdr: 15.1556 - val_loss: 89.2086 - val_Si-sdr: 15.2897\n",
      "\n",
      "Epoch 00202: val_loss improved from 89.99623 to 89.20857, saving model to ./CKPT\\CKP_ep_202__loss_89.20857_.h5\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 89.4590 - Si-sdr: 15.2627 - val_loss: 91.8361 - val_Si-sdr: 14.9980\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 89.20857\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 90.5705 - Si-sdr: 15.1115 - val_loss: 90.3520 - val_Si-sdr: 15.1664\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 89.20857\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 91.2384 - Si-sdr: 15.0691 - val_loss: 90.1154 - val_Si-sdr: 15.1733\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 89.20857\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 1s 606ms/step - loss: 91.8824 - Si-sdr: 15.0115 - val_loss: 90.2685 - val_Si-sdr: 15.2086\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 89.20857\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 90.5002 - Si-sdr: 15.1870 - val_loss: 89.6620 - val_Si-sdr: 15.2619\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 89.20857\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 89.5244 - Si-sdr: 15.2485 - val_loss: 89.0660 - val_Si-sdr: 15.3219\n",
      "\n",
      "Epoch 00208: val_loss improved from 89.20857 to 89.06599, saving model to ./CKPT\\CKP_ep_208__loss_89.06599_.h5\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 89.5689 - Si-sdr: 15.2597 - val_loss: 89.6903 - val_Si-sdr: 15.2375\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 89.06599\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 90.2419 - Si-sdr: 15.1843 - val_loss: 88.6197 - val_Si-sdr: 15.3625\n",
      "\n",
      "Epoch 00210: val_loss improved from 89.06599 to 88.61967, saving model to ./CKPT\\CKP_ep_210__loss_88.61967_.h5\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 88.6113 - Si-sdr: 15.3707 - val_loss: 90.0601 - val_Si-sdr: 15.2146\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 88.61967\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 89.5943 - Si-sdr: 15.2480 - val_loss: 89.3432 - val_Si-sdr: 15.2730\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 88.61967\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 89.7322 - Si-sdr: 15.2130 - val_loss: 89.4810 - val_Si-sdr: 15.2731\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 88.61967\n",
      "Epoch 214/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 694ms/step - loss: 89.2443 - Si-sdr: 15.3345 - val_loss: 89.3214 - val_Si-sdr: 15.2755\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 88.61967\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 90.1268 - Si-sdr: 15.1927 - val_loss: 88.3359 - val_Si-sdr: 15.3746\n",
      "\n",
      "Epoch 00215: val_loss improved from 88.61967 to 88.33589, saving model to ./CKPT\\CKP_ep_215__loss_88.33589_.h5\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 88.7523 - Si-sdr: 15.3371 - val_loss: 89.7462 - val_Si-sdr: 15.2409\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 88.33589\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 89.4042 - Si-sdr: 15.2529 - val_loss: 88.4034 - val_Si-sdr: 15.3436\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 88.33589\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 89.7007 - Si-sdr: 15.2058 - val_loss: 91.1471 - val_Si-sdr: 15.0509\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 88.33589\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 92.6408 - Si-sdr: 14.9013 - val_loss: 90.5773 - val_Si-sdr: 15.1318\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 88.33589\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 1s 585ms/step - loss: 89.7387 - Si-sdr: 15.2407 - val_loss: 89.4868 - val_Si-sdr: 15.2525\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 88.33589\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 90.1335 - Si-sdr: 15.2959 - val_loss: 88.3848 - val_Si-sdr: 15.3975\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 88.33589\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 90.2450 - Si-sdr: 15.1681 - val_loss: 89.1686 - val_Si-sdr: 15.2793\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 88.33589\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 88.7509 - Si-sdr: 15.3671 - val_loss: 87.8099 - val_Si-sdr: 15.4582\n",
      "\n",
      "Epoch 00223: val_loss improved from 88.33589 to 87.80991, saving model to ./CKPT\\CKP_ep_223__loss_87.80991_.h5\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 88.3385 - Si-sdr: 15.4538 - val_loss: 87.5941 - val_Si-sdr: 15.4841\n",
      "\n",
      "Epoch 00224: val_loss improved from 87.80991 to 87.59406, saving model to ./CKPT\\CKP_ep_224__loss_87.59406_.h5\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 87.6262 - Si-sdr: 15.4806 - val_loss: 88.6367 - val_Si-sdr: 15.3270\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 87.59406\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 87.9553 - Si-sdr: 15.4250 - val_loss: 88.9687 - val_Si-sdr: 15.3338\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 87.59406\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 88.9348 - Si-sdr: 15.3629 - val_loss: 88.1719 - val_Si-sdr: 15.4013\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 87.59406\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 88.0344 - Si-sdr: 15.3977 - val_loss: 89.2292 - val_Si-sdr: 15.3086\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 87.59406\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 88.6285 - Si-sdr: 15.3446 - val_loss: 88.9354 - val_Si-sdr: 15.3100\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 87.59406\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 88.6360 - Si-sdr: 15.3555 - val_loss: 88.7521 - val_Si-sdr: 15.3402\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 87.59406\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 89.5041 - Si-sdr: 15.2978 - val_loss: 89.9903 - val_Si-sdr: 15.1659\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 87.59406\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 89.0183 - Si-sdr: 15.2883 - val_loss: 88.2818 - val_Si-sdr: 15.3768\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 87.59406\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 88.4079 - Si-sdr: 15.3841 - val_loss: 89.5364 - val_Si-sdr: 15.2515\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 87.59406\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 89.0668 - Si-sdr: 15.3296 - val_loss: 88.2700 - val_Si-sdr: 15.3779\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 87.59406\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 87.3547 - Si-sdr: 15.5238 - val_loss: 87.8228 - val_Si-sdr: 15.4689\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 87.59406\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 87.7717 - Si-sdr: 15.4476 - val_loss: 87.0118 - val_Si-sdr: 15.5432\n",
      "\n",
      "Epoch 00236: val_loss improved from 87.59406 to 87.01184, saving model to ./CKPT\\CKP_ep_236__loss_87.01184_.h5\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 88.2958 - Si-sdr: 15.3942 - val_loss: 87.3037 - val_Si-sdr: 15.4829\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 87.01184\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 87.9682 - Si-sdr: 15.4057 - val_loss: 87.7270 - val_Si-sdr: 15.4266\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 87.01184\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 1s 764ms/step - loss: 88.4142 - Si-sdr: 15.3467 - val_loss: 88.1317 - val_Si-sdr: 15.4381\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 87.01184\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 88.8268 - Si-sdr: 15.3642 - val_loss: 87.6996 - val_Si-sdr: 15.4784\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 87.01184\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 89.3393 - Si-sdr: 15.2792 - val_loss: 87.3007 - val_Si-sdr: 15.5099\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 87.01184\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 1s 764ms/step - loss: 89.2157 - Si-sdr: 15.3574 - val_loss: 85.9047 - val_Si-sdr: 15.7137\n",
      "\n",
      "Epoch 00242: val_loss improved from 87.01184 to 85.90469, saving model to ./CKPT\\CKP_ep_242__loss_85.90469_.h5\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 87.7510 - Si-sdr: 15.4552 - val_loss: 86.5312 - val_Si-sdr: 15.5962\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 85.90469\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 88.2092 - Si-sdr: 15.4368 - val_loss: 88.0207 - val_Si-sdr: 15.4412\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 85.90469\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 87.2931 - Si-sdr: 15.4911 - val_loss: 88.8805 - val_Si-sdr: 15.3040\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 85.90469\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 87.7852 - Si-sdr: 15.4714 - val_loss: 87.1938 - val_Si-sdr: 15.5190\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 85.90469\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 1s 603ms/step - loss: 87.0235 - Si-sdr: 15.5581 - val_loss: 87.5257 - val_Si-sdr: 15.5071\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 85.90469\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 87.2456 - Si-sdr: 15.5415 - val_loss: 87.2091 - val_Si-sdr: 15.5018\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 85.90469\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 87.0596 - Si-sdr: 15.5025 - val_loss: 85.8274 - val_Si-sdr: 15.6906\n",
      "\n",
      "Epoch 00249: val_loss improved from 85.90469 to 85.82737, saving model to ./CKPT\\CKP_ep_249__loss_85.82737_.h5\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 1s 859ms/step - loss: 87.1286 - Si-sdr: 15.5507 - val_loss: 86.9419 - val_Si-sdr: 15.5569\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 85.82737\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 1s 730ms/step - loss: 87.1928 - Si-sdr: 15.5243 - val_loss: 86.4525 - val_Si-sdr: 15.5858\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 85.82737\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 87.0189 - Si-sdr: 15.5201 - val_loss: 86.7996 - val_Si-sdr: 15.5348\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 85.82737\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 1s 786ms/step - loss: 87.4628 - Si-sdr: 15.4510 - val_loss: 87.7462 - val_Si-sdr: 15.4981\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 85.82737\n",
      "Epoch 254/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 598ms/step - loss: 86.7151 - Si-sdr: 15.6158 - val_loss: 86.0542 - val_Si-sdr: 15.6415\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 85.82737\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 1s 732ms/step - loss: 87.4815 - Si-sdr: 15.5862 - val_loss: 87.3691 - val_Si-sdr: 15.5201\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 85.82737\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 86.6987 - Si-sdr: 15.5691 - val_loss: 88.5646 - val_Si-sdr: 15.3458\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 85.82737\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 1s 726ms/step - loss: 86.3655 - Si-sdr: 15.5808 - val_loss: 85.8621 - val_Si-sdr: 15.6584\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 85.82737\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 84.8347 - Si-sdr: 15.7902 - val_loss: 86.0334 - val_Si-sdr: 15.6533\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 85.82737\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 86.6252 - Si-sdr: 15.5610 - val_loss: 85.3607 - val_Si-sdr: 15.7388\n",
      "\n",
      "Epoch 00259: val_loss improved from 85.82737 to 85.36066, saving model to ./CKPT\\CKP_ep_259__loss_85.36066_.h5\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 1s 798ms/step - loss: 85.9790 - Si-sdr: 15.6506 - val_loss: 85.1269 - val_Si-sdr: 15.7513\n",
      "\n",
      "Epoch 00260: val_loss improved from 85.36066 to 85.12687, saving model to ./CKPT\\CKP_ep_260__loss_85.12687_.h5\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 85.3723 - Si-sdr: 15.7113 - val_loss: 85.5531 - val_Si-sdr: 15.7041\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 85.12687\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 85.7176 - Si-sdr: 15.6852 - val_loss: 87.0574 - val_Si-sdr: 15.4967\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 85.12687\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 86.1221 - Si-sdr: 15.6312 - val_loss: 86.4626 - val_Si-sdr: 15.5806\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 85.12687\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 85.9578 - Si-sdr: 15.6623 - val_loss: 85.5087 - val_Si-sdr: 15.6952\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 85.12687\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 85.5873 - Si-sdr: 15.6651 - val_loss: 85.1411 - val_Si-sdr: 15.7292\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 85.12687\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 86.4632 - Si-sdr: 15.5910 - val_loss: 85.8665 - val_Si-sdr: 15.6202\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 85.12687\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 85.2538 - Si-sdr: 15.7260 - val_loss: 84.9525 - val_Si-sdr: 15.7738\n",
      "\n",
      "Epoch 00267: val_loss improved from 85.12687 to 84.95254, saving model to ./CKPT\\CKP_ep_267__loss_84.95254_.h5\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 85.5831 - Si-sdr: 15.7181 - val_loss: 85.1361 - val_Si-sdr: 15.7707\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 84.95254\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 85.7902 - Si-sdr: 15.6897 - val_loss: 85.1284 - val_Si-sdr: 15.7782\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 84.95254\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 85.5592 - Si-sdr: 15.7049 - val_loss: 83.0684 - val_Si-sdr: 15.9831\n",
      "\n",
      "Epoch 00270: val_loss improved from 84.95254 to 83.06841, saving model to ./CKPT\\CKP_ep_270__loss_83.06841_.h5\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 84.9055 - Si-sdr: 15.7444 - val_loss: 86.1703 - val_Si-sdr: 15.6322\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 83.06841\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 84.4267 - Si-sdr: 15.8494 - val_loss: 84.3241 - val_Si-sdr: 15.8134\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 83.06841\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 85.6782 - Si-sdr: 15.6577 - val_loss: 83.7672 - val_Si-sdr: 15.8998\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 83.06841\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 84.9399 - Si-sdr: 15.7620 - val_loss: 84.3952 - val_Si-sdr: 15.8532\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 83.06841\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 83.3277 - Si-sdr: 15.9569 - val_loss: 84.9877 - val_Si-sdr: 15.7967\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 83.06841\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 84.2713 - Si-sdr: 15.8395 - val_loss: 83.6949 - val_Si-sdr: 15.9164\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 83.06841\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 84.0542 - Si-sdr: 15.9483 - val_loss: 84.4285 - val_Si-sdr: 15.8283\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 83.06841\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 84.0404 - Si-sdr: 15.8586 - val_loss: 83.9305 - val_Si-sdr: 15.8770\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 83.06841\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 83.5600 - Si-sdr: 15.9341 - val_loss: 84.3849 - val_Si-sdr: 15.8002\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 83.06841\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 83.7654 - Si-sdr: 15.9255 - val_loss: 84.4820 - val_Si-sdr: 15.8260\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 83.06841\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 83.6905 - Si-sdr: 15.8910 - val_loss: 84.8327 - val_Si-sdr: 15.7611\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 83.06841\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 86.2922 - Si-sdr: 15.6072 - val_loss: 85.9319 - val_Si-sdr: 15.6287\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 83.06841\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 85.3166 - Si-sdr: 15.7660 - val_loss: 85.6028 - val_Si-sdr: 15.6986\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 83.06841\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 86.2123 - Si-sdr: 15.6748 - val_loss: 84.2399 - val_Si-sdr: 15.8900\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 83.06841\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 83.9421 - Si-sdr: 15.9364 - val_loss: 85.4838 - val_Si-sdr: 15.7193\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 83.06841\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 84.6415 - Si-sdr: 15.7457 - val_loss: 83.9195 - val_Si-sdr: 15.9277\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 83.06841\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 85.0548 - Si-sdr: 15.8386 - val_loss: 84.2070 - val_Si-sdr: 15.8504\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 83.06841\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 83.8134 - Si-sdr: 15.8769 - val_loss: 85.4588 - val_Si-sdr: 15.7181\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 83.06841\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 1s 770ms/step - loss: 85.4494 - Si-sdr: 15.7086 - val_loss: 85.1732 - val_Si-sdr: 15.7395\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 83.06841\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 86.2672 - Si-sdr: 15.5945 - val_loss: 84.7757 - val_Si-sdr: 15.7476\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 83.06841\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 85.4836 - Si-sdr: 15.6631 - val_loss: 85.7399 - val_Si-sdr: 15.6566\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 83.06841\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 86.5545 - Si-sdr: 15.5455 - val_loss: 83.9272 - val_Si-sdr: 15.8538\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 83.06841\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 85.2115 - Si-sdr: 15.7297 - val_loss: 84.2696 - val_Si-sdr: 15.8083\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 83.06841\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 84.8335 - Si-sdr: 15.7774 - val_loss: 84.2665 - val_Si-sdr: 15.7899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00294: val_loss did not improve from 83.06841\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 83.5573 - Si-sdr: 15.9266 - val_loss: 83.0207 - val_Si-sdr: 15.9672\n",
      "\n",
      "Epoch 00295: val_loss improved from 83.06841 to 83.02071, saving model to ./CKPT\\CKP_ep_295__loss_83.02071_.h5\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 84.0468 - Si-sdr: 15.8102 - val_loss: 83.7409 - val_Si-sdr: 15.8825\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 83.02071\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 1s 753ms/step - loss: 84.5387 - Si-sdr: 15.8039 - val_loss: 82.7307 - val_Si-sdr: 16.0066\n",
      "\n",
      "Epoch 00297: val_loss improved from 83.02071 to 82.73073, saving model to ./CKPT\\CKP_ep_297__loss_82.73073_.h5\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 83.9489 - Si-sdr: 15.8648 - val_loss: 83.1736 - val_Si-sdr: 15.9465\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 82.73073\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 83.3315 - Si-sdr: 15.9331 - val_loss: 83.6128 - val_Si-sdr: 15.8937\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 82.73073\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 83.0276 - Si-sdr: 16.0075 - val_loss: 83.9312 - val_Si-sdr: 15.8830\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 82.73073\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-nurse",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "protective-chuck",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_25 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_29 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.14930329 -0.05488385 -0.03165275  0.02511186]\n",
      "  [-0.04538564 -0.02340828 -0.13736942  0.03185076]\n",
      "  [-0.18286747 -0.01420305  0.02583414  0.20739384]\n",
      "  [-0.15128048 -0.15295134  0.00826486 -0.03651741]\n",
      "  [-0.19406578 -0.14820886 -0.24136184  0.01954714]\n",
      "  [-0.11608941  0.05977409 -0.05805521  0.08262399]\n",
      "  [-0.10437049  0.00281959  0.00744648  0.06939161]\n",
      "  [-0.32889026 -0.11109954 -0.06513101  0.02347144]\n",
      "  [-0.10914627  0.04005037 -0.11194003  0.34588984]\n",
      "  [-0.55321944 -0.24241613 -0.17234027  0.2255739 ]]\n",
      "\n",
      " [[-0.12665638 -0.02560038 -0.07357763 -0.03386865]\n",
      "  [-0.00296582  0.05382628 -0.02431939 -0.00124017]\n",
      "  [-0.152773    0.01729362 -0.02049777  0.00555693]\n",
      "  [-0.01630396 -0.0508009   0.00338947 -0.00536075]\n",
      "  [-0.11710438 -0.0411608   0.01888775  0.10048383]\n",
      "  [-0.1000828   0.06160894 -0.04584875  0.14583868]\n",
      "  [-0.29426795 -0.04820506 -0.1607864   0.03541403]\n",
      "  [-0.14156486  0.07812893 -0.13114211  0.12421213]\n",
      "  [-0.20461929  0.08133916 -0.08211927  0.03100684]\n",
      "  [-0.00948744 -0.11452891  0.03942408 -0.1281477 ]]]\n",
      "(2, 10, 4)\n",
      "(2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
