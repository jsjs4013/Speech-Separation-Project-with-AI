{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "447o0302_2.1067_422o030k_-2.1067\n",
      "447o0302_0.62948_441c0212_-0.62948\n",
      "447o0303_0.14144_441c0212_-0.14144\n",
      "447o0302_1.3388_22ho010i_-1.3388\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)\n",
    "\n",
    "for batch in test_dataset:\n",
    "    a, b, c = batch\n",
    "    print(c[0][:-5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.reduce_max(y, 2, keep_dims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, use_enc=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.use_enc = use_enc\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax()\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        \n",
    "        if self.use_enc:\n",
    "            one_hot = tf.cast(tf.equal(encode, tf.reduce_max(encode, 2, keep_dims=True)), encode.dtype)\n",
    "            \n",
    "            return one_hot\n",
    "        \n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax (Softmax)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - ETA: 0s - loss: 629.4828 - Si-sdr: -231.1930"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step - loss: 629.4828 - Si-sdr: -231.1930 - val_loss: 627.6610 - val_Si-sdr: -243.0287\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 627.66101, saving model to ./CKPT\\CKP_ep_1__loss_627.66101_.h5\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 627.7321 - Si-sdr: -211.6282 - val_loss: 628.0947 - val_Si-sdr: -242.4515\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 627.66101\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 627.9047 - Si-sdr: -226.9610 - val_loss: 627.4020 - val_Si-sdr: -251.9299\n",
      "\n",
      "Epoch 00003: val_loss improved from 627.66101 to 627.40198, saving model to ./CKPT\\CKP_ep_3__loss_627.40198_.h5\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 627.4252 - Si-sdr: -215.8552 - val_loss: 627.1998 - val_Si-sdr: -228.6046\n",
      "\n",
      "Epoch 00004: val_loss improved from 627.40198 to 627.19983, saving model to ./CKPT\\CKP_ep_4__loss_627.19983_.h5\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 627.1805 - Si-sdr: -189.0958 - val_loss: 627.2406 - val_Si-sdr: -225.0666\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 627.19983\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 627.2516 - Si-sdr: -210.7064 - val_loss: 627.2192 - val_Si-sdr: -203.8730\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 627.19983\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 627.2526 - Si-sdr: -214.9535 - val_loss: 627.2247 - val_Si-sdr: -204.5883\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 627.19983\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 627.2162 - Si-sdr: -261.8494 - val_loss: 627.1867 - val_Si-sdr: -232.3371\n",
      "\n",
      "Epoch 00008: val_loss improved from 627.19983 to 627.18671, saving model to ./CKPT\\CKP_ep_8__loss_627.18671_.h5\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 627.2211 - Si-sdr: -220.2569 - val_loss: 627.2152 - val_Si-sdr: -243.1984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 627.18671\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 627.1990 - Si-sdr: -236.2406 - val_loss: 627.2087 - val_Si-sdr: -238.7491\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 627.18671\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 627.2168 - Si-sdr: -240.6471 - val_loss: 627.2184 - val_Si-sdr: -266.0711\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 627.18671\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 627.2048 - Si-sdr: -206.7737 - val_loss: 627.1865 - val_Si-sdr: -185.4471\n",
      "\n",
      "Epoch 00012: val_loss improved from 627.18671 to 627.18646, saving model to ./CKPT\\CKP_ep_12__loss_627.18646_.h5\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 547ms/step - loss: 627.1688 - Si-sdr: -189.6425 - val_loss: 627.1665 - val_Si-sdr: -191.5283\n",
      "\n",
      "Epoch 00013: val_loss improved from 627.18646 to 627.16650, saving model to ./CKPT\\CKP_ep_13__loss_627.16650_.h5\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 627.1644 - Si-sdr: -187.1943 - val_loss: 627.1395 - val_Si-sdr: -164.1791\n",
      "\n",
      "Epoch 00014: val_loss improved from 627.16650 to 627.13953, saving model to ./CKPT\\CKP_ep_14__loss_627.13953_.h5\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 627.1617 - Si-sdr: -182.0373 - val_loss: 627.1367 - val_Si-sdr: -163.4585\n",
      "\n",
      "Epoch 00015: val_loss improved from 627.13953 to 627.13672, saving model to ./CKPT\\CKP_ep_15__loss_627.13672_.h5\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 574ms/step - loss: 627.1158 - Si-sdr: -158.9588 - val_loss: 627.0746 - val_Si-sdr: -142.5621\n",
      "\n",
      "Epoch 00016: val_loss improved from 627.13672 to 627.07458, saving model to ./CKPT\\CKP_ep_16__loss_627.07458_.h5\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 627.0128 - Si-sdr: -131.1836 - val_loss: 626.9204 - val_Si-sdr: -121.7332\n",
      "\n",
      "Epoch 00017: val_loss improved from 627.07458 to 626.92041, saving model to ./CKPT\\CKP_ep_17__loss_626.92041_.h5\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 626.7927 - Si-sdr: -114.1446 - val_loss: 626.4144 - val_Si-sdr: -96.2830\n",
      "\n",
      "Epoch 00018: val_loss improved from 626.92041 to 626.41443, saving model to ./CKPT\\CKP_ep_18__loss_626.41443_.h5\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 626.1158 - Si-sdr: -89.5219 - val_loss: 625.1323 - val_Si-sdr: -74.0647\n",
      "\n",
      "Epoch 00019: val_loss improved from 626.41443 to 625.13232, saving model to ./CKPT\\CKP_ep_19__loss_625.13232_.h5\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 624.3043 - Si-sdr: -69.4171 - val_loss: 621.0930 - val_Si-sdr: -51.9355\n",
      "\n",
      "Epoch 00020: val_loss improved from 625.13232 to 621.09296, saving model to ./CKPT\\CKP_ep_20__loss_621.09296_.h5\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 1s 553ms/step - loss: 618.6438 - Si-sdr: -48.9429 - val_loss: 610.5940 - val_Si-sdr: -40.2366\n",
      "\n",
      "Epoch 00021: val_loss improved from 621.09296 to 610.59399, saving model to ./CKPT\\CKP_ep_21__loss_610.59399_.h5\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 488ms/step - loss: 605.5365 - Si-sdr: -36.8629 - val_loss: 590.0044 - val_Si-sdr: -32.4264\n",
      "\n",
      "Epoch 00022: val_loss improved from 610.59399 to 590.00439, saving model to ./CKPT\\CKP_ep_22__loss_590.00439_.h5\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 581.1281 - Si-sdr: -31.1705 - val_loss: 557.4304 - val_Si-sdr: -27.9853\n",
      "\n",
      "Epoch 00023: val_loss improved from 590.00439 to 557.43042, saving model to ./CKPT\\CKP_ep_23__loss_557.43042_.h5\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 512ms/step - loss: 550.8621 - Si-sdr: -27.4147 - val_loss: 513.8843 - val_Si-sdr: -22.1487\n",
      "\n",
      "Epoch 00024: val_loss improved from 557.43042 to 513.88428, saving model to ./CKPT\\CKP_ep_24__loss_513.88428_.h5\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 492ms/step - loss: 502.7077 - Si-sdr: -21.8312 - val_loss: 467.3505 - val_Si-sdr: -17.5067\n",
      "\n",
      "Epoch 00025: val_loss improved from 513.88428 to 467.35052, saving model to ./CKPT\\CKP_ep_25__loss_467.35052_.h5\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 452.6318 - Si-sdr: -16.3363 - val_loss: 409.0439 - val_Si-sdr: -10.1452\n",
      "\n",
      "Epoch 00026: val_loss improved from 467.35052 to 409.04388, saving model to ./CKPT\\CKP_ep_26__loss_409.04388_.h5\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 391.0134 - Si-sdr: -8.2682 - val_loss: 351.4103 - val_Si-sdr: -3.5547\n",
      "\n",
      "Epoch 00027: val_loss improved from 409.04388 to 351.41034, saving model to ./CKPT\\CKP_ep_27__loss_351.41034_.h5\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 341.5949 - Si-sdr: -2.7585 - val_loss: 306.9601 - val_Si-sdr: 1.4726\n",
      "\n",
      "Epoch 00028: val_loss improved from 351.41034 to 306.96008, saving model to ./CKPT\\CKP_ep_28__loss_306.96008_.h5\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 294.1072 - Si-sdr: 2.2428 - val_loss: 283.8172 - val_Si-sdr: 3.6835\n",
      "\n",
      "Epoch 00029: val_loss improved from 306.96008 to 283.81720, saving model to ./CKPT\\CKP_ep_29__loss_283.81720_.h5\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 280.5278 - Si-sdr: 4.0813 - val_loss: 251.2334 - val_Si-sdr: 7.3373\n",
      "\n",
      "Epoch 00030: val_loss improved from 283.81720 to 251.23335, saving model to ./CKPT\\CKP_ep_30__loss_251.23335_.h5\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 254.8528 - Si-sdr: 7.0242 - val_loss: 232.1426 - val_Si-sdr: 9.7559\n",
      "\n",
      "Epoch 00031: val_loss improved from 251.23335 to 232.14264, saving model to ./CKPT\\CKP_ep_31__loss_232.14264_.h5\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 233.6984 - Si-sdr: 9.5776 - val_loss: 221.9040 - val_Si-sdr: 10.8209\n",
      "\n",
      "Epoch 00032: val_loss improved from 232.14264 to 221.90396, saving model to ./CKPT\\CKP_ep_32__loss_221.90396_.h5\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 493ms/step - loss: 222.3597 - Si-sdr: 10.6212 - val_loss: 209.3515 - val_Si-sdr: 12.3185\n",
      "\n",
      "Epoch 00033: val_loss improved from 221.90396 to 209.35150, saving model to ./CKPT\\CKP_ep_33__loss_209.35150_.h5\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 442ms/step - loss: 208.4931 - Si-sdr: 12.3296 - val_loss: 202.1084 - val_Si-sdr: 13.0687\n",
      "\n",
      "Epoch 00034: val_loss improved from 209.35150 to 202.10835, saving model to ./CKPT\\CKP_ep_34__loss_202.10835_.h5\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 202.2928 - Si-sdr: 12.8967 - val_loss: 191.8822 - val_Si-sdr: 14.2489\n",
      "\n",
      "Epoch 00035: val_loss improved from 202.10835 to 191.88225, saving model to ./CKPT\\CKP_ep_35__loss_191.88225_.h5\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 191.1416 - Si-sdr: 14.3868 - val_loss: 185.6155 - val_Si-sdr: 15.1415\n",
      "\n",
      "Epoch 00036: val_loss improved from 191.88225 to 185.61548, saving model to ./CKPT\\CKP_ep_36__loss_185.61548_.h5\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 183.9900 - Si-sdr: 15.3842 - val_loss: 180.2583 - val_Si-sdr: 15.7195\n",
      "\n",
      "Epoch 00037: val_loss improved from 185.61548 to 180.25835, saving model to ./CKPT\\CKP_ep_37__loss_180.25835_.h5\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 178.1971 - Si-sdr: 16.1786 - val_loss: 173.8012 - val_Si-sdr: 16.5218\n",
      "\n",
      "Epoch 00038: val_loss improved from 180.25835 to 173.80124, saving model to ./CKPT\\CKP_ep_38__loss_173.80124_.h5\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 1s 481ms/step - loss: 172.9895 - Si-sdr: 16.8362 - val_loss: 170.9485 - val_Si-sdr: 17.0743\n",
      "\n",
      "Epoch 00039: val_loss improved from 173.80124 to 170.94853, saving model to ./CKPT\\CKP_ep_39__loss_170.94853_.h5\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 166.6720 - Si-sdr: 17.6678 - val_loss: 166.5325 - val_Si-sdr: 17.7098\n",
      "\n",
      "Epoch 00040: val_loss improved from 170.94853 to 166.53253, saving model to ./CKPT\\CKP_ep_40__loss_166.53253_.h5\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 1s 550ms/step - loss: 163.0740 - Si-sdr: 18.0989 - val_loss: 158.8145 - val_Si-sdr: 18.8409\n",
      "\n",
      "Epoch 00041: val_loss improved from 166.53253 to 158.81451, saving model to ./CKPT\\CKP_ep_41__loss_158.81451_.h5\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 1s 507ms/step - loss: 159.1466 - Si-sdr: 18.7535 - val_loss: 157.6475 - val_Si-sdr: 18.8428\n",
      "\n",
      "Epoch 00042: val_loss improved from 158.81451 to 157.64751, saving model to ./CKPT\\CKP_ep_42__loss_157.64751_.h5\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 1s 484ms/step - loss: 155.5901 - Si-sdr: 19.2005 - val_loss: 153.7622 - val_Si-sdr: 19.5358\n",
      "\n",
      "Epoch 00043: val_loss improved from 157.64751 to 153.76218, saving model to ./CKPT\\CKP_ep_43__loss_153.76218_.h5\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 150.5252 - Si-sdr: 19.8833 - val_loss: 150.2751 - val_Si-sdr: 19.9653\n",
      "\n",
      "Epoch 00044: val_loss improved from 153.76218 to 150.27515, saving model to ./CKPT\\CKP_ep_44__loss_150.27515_.h5\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 150.2388 - Si-sdr: 20.0782 - val_loss: 144.7374 - val_Si-sdr: 20.7501\n",
      "\n",
      "Epoch 00045: val_loss improved from 150.27515 to 144.73743, saving model to ./CKPT\\CKP_ep_45__loss_144.73743_.h5\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 145.0534 - Si-sdr: 20.7567 - val_loss: 143.8127 - val_Si-sdr: 20.9964\n",
      "\n",
      "Epoch 00046: val_loss improved from 144.73743 to 143.81268, saving model to ./CKPT\\CKP_ep_46__loss_143.81268_.h5\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 1s 535ms/step - loss: 141.2614 - Si-sdr: 21.3517 - val_loss: 137.7090 - val_Si-sdr: 21.9071\n",
      "\n",
      "Epoch 00047: val_loss improved from 143.81268 to 137.70901, saving model to ./CKPT\\CKP_ep_47__loss_137.70901_.h5\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 508ms/step - loss: 138.1284 - Si-sdr: 21.8694 - val_loss: 137.1812 - val_Si-sdr: 21.7144\n",
      "\n",
      "Epoch 00048: val_loss improved from 137.70901 to 137.18124, saving model to ./CKPT\\CKP_ep_48__loss_137.18124_.h5\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 136.4371 - Si-sdr: 21.8570 - val_loss: 131.9269 - val_Si-sdr: 22.7589\n",
      "\n",
      "Epoch 00049: val_loss improved from 137.18124 to 131.92686, saving model to ./CKPT\\CKP_ep_49__loss_131.92686_.h5\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 133.4718 - Si-sdr: 22.5118 - val_loss: 132.1868 - val_Si-sdr: 22.6956\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 131.92686\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 1s 507ms/step - loss: 130.9039 - Si-sdr: 22.8802 - val_loss: 130.2116 - val_Si-sdr: 23.0226\n",
      "\n",
      "Epoch 00051: val_loss improved from 131.92686 to 130.21156, saving model to ./CKPT\\CKP_ep_51__loss_130.21156_.h5\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 127.9263 - Si-sdr: 23.3381 - val_loss: 126.3600 - val_Si-sdr: 23.5938\n",
      "\n",
      "Epoch 00052: val_loss improved from 130.21156 to 126.36002, saving model to ./CKPT\\CKP_ep_52__loss_126.36002_.h5\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 125.1528 - Si-sdr: 23.8640 - val_loss: 126.0637 - val_Si-sdr: 23.6679\n",
      "\n",
      "Epoch 00053: val_loss improved from 126.36002 to 126.06367, saving model to ./CKPT\\CKP_ep_53__loss_126.06367_.h5\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 123.0399 - Si-sdr: 24.0734 - val_loss: 122.7159 - val_Si-sdr: 24.2577\n",
      "\n",
      "Epoch 00054: val_loss improved from 126.06367 to 122.71594, saving model to ./CKPT\\CKP_ep_54__loss_122.71594_.h5\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 120.6028 - Si-sdr: 24.6636 - val_loss: 118.2989 - val_Si-sdr: 25.0136\n",
      "\n",
      "Epoch 00055: val_loss improved from 122.71594 to 118.29889, saving model to ./CKPT\\CKP_ep_55__loss_118.29889_.h5\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 1s 390ms/step - loss: 118.4375 - Si-sdr: 25.0933 - val_loss: 119.1113 - val_Si-sdr: 24.8159\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 118.29889\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 119.1799 - Si-sdr: 24.7542 - val_loss: 114.0619 - val_Si-sdr: 25.8115\n",
      "\n",
      "Epoch 00057: val_loss improved from 118.29889 to 114.06193, saving model to ./CKPT\\CKP_ep_57__loss_114.06193_.h5\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 1s 559ms/step - loss: 116.6304 - Si-sdr: 25.2774 - val_loss: 113.2906 - val_Si-sdr: 25.9096\n",
      "\n",
      "Epoch 00058: val_loss improved from 114.06193 to 113.29063, saving model to ./CKPT\\CKP_ep_58__loss_113.29063_.h5\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 114.4172 - Si-sdr: 25.7121 - val_loss: 112.7804 - val_Si-sdr: 26.0296\n",
      "\n",
      "Epoch 00059: val_loss improved from 113.29063 to 112.78036, saving model to ./CKPT\\CKP_ep_59__loss_112.78036_.h5\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 113.1766 - Si-sdr: 25.8779 - val_loss: 113.0401 - val_Si-sdr: 25.9960\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 112.78036\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 111.4499 - Si-sdr: 26.4082 - val_loss: 109.5402 - val_Si-sdr: 26.5827\n",
      "\n",
      "Epoch 00061: val_loss improved from 112.78036 to 109.54016, saving model to ./CKPT\\CKP_ep_61__loss_109.54016_.h5\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 110.8940 - Si-sdr: 26.2949 - val_loss: 110.4493 - val_Si-sdr: 26.4010\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 109.54016\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 1s 584ms/step - loss: 110.6265 - Si-sdr: 26.2527 - val_loss: 109.2076 - val_Si-sdr: 26.6174\n",
      "\n",
      "Epoch 00063: val_loss improved from 109.54016 to 109.20760, saving model to ./CKPT\\CKP_ep_63__loss_109.20760_.h5\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 108.5699 - Si-sdr: 26.6550 - val_loss: 105.3243 - val_Si-sdr: 27.3349\n",
      "\n",
      "Epoch 00064: val_loss improved from 109.20760 to 105.32426, saving model to ./CKPT\\CKP_ep_64__loss_105.32426_.h5\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 1s 556ms/step - loss: 105.6158 - Si-sdr: 27.3580 - val_loss: 105.4357 - val_Si-sdr: 27.3707\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 105.32426\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 106.5425 - Si-sdr: 27.0660 - val_loss: 105.3135 - val_Si-sdr: 27.3626\n",
      "\n",
      "Epoch 00066: val_loss improved from 105.32426 to 105.31348, saving model to ./CKPT\\CKP_ep_66__loss_105.31348_.h5\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 429ms/step - loss: 104.9222 - Si-sdr: 27.4807 - val_loss: 103.0128 - val_Si-sdr: 27.8055\n",
      "\n",
      "Epoch 00067: val_loss improved from 105.31348 to 103.01282, saving model to ./CKPT\\CKP_ep_67__loss_103.01282_.h5\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 103.3815 - Si-sdr: 27.7338 - val_loss: 101.5780 - val_Si-sdr: 28.1464\n",
      "\n",
      "Epoch 00068: val_loss improved from 103.01282 to 101.57796, saving model to ./CKPT\\CKP_ep_68__loss_101.57796_.h5\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 102.3575 - Si-sdr: 28.0017 - val_loss: 101.6640 - val_Si-sdr: 28.1404\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 101.57796\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 102.4473 - Si-sdr: 27.9454 - val_loss: 102.5608 - val_Si-sdr: 27.9337\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 101.57796\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 1s 384ms/step - loss: 101.9013 - Si-sdr: 28.0220 - val_loss: 100.2076 - val_Si-sdr: 28.3372\n",
      "\n",
      "Epoch 00071: val_loss improved from 101.57796 to 100.20757, saving model to ./CKPT\\CKP_ep_71__loss_100.20757_.h5\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 100.8538 - Si-sdr: 28.2408 - val_loss: 99.6509 - val_Si-sdr: 28.5113\n",
      "\n",
      "Epoch 00072: val_loss improved from 100.20757 to 99.65089, saving model to ./CKPT\\CKP_ep_72__loss_99.65089_.h5\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 1s 520ms/step - loss: 100.1929 - Si-sdr: 28.3951 - val_loss: 99.5050 - val_Si-sdr: 28.5406\n",
      "\n",
      "Epoch 00073: val_loss improved from 99.65089 to 99.50497, saving model to ./CKPT\\CKP_ep_73__loss_99.50497_.h5\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 100.2631 - Si-sdr: 28.4225 - val_loss: 97.9995 - val_Si-sdr: 28.8599\n",
      "\n",
      "Epoch 00074: val_loss improved from 99.50497 to 97.99945, saving model to ./CKPT\\CKP_ep_74__loss_97.99945_.h5\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 97.2578 - Si-sdr: 28.9810 - val_loss: 99.2851 - val_Si-sdr: 28.5622\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 97.99945\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 98.5944 - Si-sdr: 28.6905 - val_loss: 97.6086 - val_Si-sdr: 28.8115\n",
      "\n",
      "Epoch 00076: val_loss improved from 97.99945 to 97.60864, saving model to ./CKPT\\CKP_ep_76__loss_97.60864_.h5\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 97.6793 - Si-sdr: 28.9492 - val_loss: 95.9325 - val_Si-sdr: 29.2416\n",
      "\n",
      "Epoch 00077: val_loss improved from 97.60864 to 95.93248, saving model to ./CKPT\\CKP_ep_77__loss_95.93248_.h5\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 95.1513 - Si-sdr: 29.4689 - val_loss: 97.5444 - val_Si-sdr: 28.9206\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 95.93248\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 1s 482ms/step - loss: 98.3904 - Si-sdr: 28.8713 - val_loss: 95.2599 - val_Si-sdr: 29.3908\n",
      "\n",
      "Epoch 00079: val_loss improved from 95.93248 to 95.25989, saving model to ./CKPT\\CKP_ep_79__loss_95.25989_.h5\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 1s 561ms/step - loss: 94.6831 - Si-sdr: 29.4558 - val_loss: 94.1404 - val_Si-sdr: 29.6141\n",
      "\n",
      "Epoch 00080: val_loss improved from 95.25989 to 94.14043, saving model to ./CKPT\\CKP_ep_80__loss_94.14043_.h5\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 96.3559 - Si-sdr: 29.2263 - val_loss: 93.8218 - val_Si-sdr: 29.6916\n",
      "\n",
      "Epoch 00081: val_loss improved from 94.14043 to 93.82178, saving model to ./CKPT\\CKP_ep_81__loss_93.82178_.h5\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 94.3442 - Si-sdr: 29.5752 - val_loss: 92.4843 - val_Si-sdr: 29.9221\n",
      "\n",
      "Epoch 00082: val_loss improved from 93.82178 to 92.48431, saving model to ./CKPT\\CKP_ep_82__loss_92.48431_.h5\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 93.8148 - Si-sdr: 29.7809 - val_loss: 92.8255 - val_Si-sdr: 29.9028\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 92.48431\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 1s 492ms/step - loss: 93.6723 - Si-sdr: 29.6867 - val_loss: 94.5772 - val_Si-sdr: 29.5082\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 92.48431\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 92.9283 - Si-sdr: 29.7662 - val_loss: 92.4078 - val_Si-sdr: 29.9663\n",
      "\n",
      "Epoch 00085: val_loss improved from 92.48431 to 92.40775, saving model to ./CKPT\\CKP_ep_85__loss_92.40775_.h5\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 1s 374ms/step - loss: 91.4681 - Si-sdr: 30.2616 - val_loss: 91.1849 - val_Si-sdr: 30.1668\n",
      "\n",
      "Epoch 00086: val_loss improved from 92.40775 to 91.18488, saving model to ./CKPT\\CKP_ep_86__loss_91.18488_.h5\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 91.7540 - Si-sdr: 30.1558 - val_loss: 89.8960 - val_Si-sdr: 30.5681\n",
      "\n",
      "Epoch 00087: val_loss improved from 91.18488 to 89.89601, saving model to ./CKPT\\CKP_ep_87__loss_89.89601_.h5\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 91.1154 - Si-sdr: 30.2520 - val_loss: 88.5354 - val_Si-sdr: 30.8460\n",
      "\n",
      "Epoch 00088: val_loss improved from 89.89601 to 88.53540, saving model to ./CKPT\\CKP_ep_88__loss_88.53540_.h5\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 89.6624 - Si-sdr: 30.6292 - val_loss: 89.2280 - val_Si-sdr: 30.7053\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 88.53540\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 90.7042 - Si-sdr: 30.3526 - val_loss: 88.4637 - val_Si-sdr: 30.8435\n",
      "\n",
      "Epoch 00090: val_loss improved from 88.53540 to 88.46367, saving model to ./CKPT\\CKP_ep_90__loss_88.46367_.h5\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 1s 533ms/step - loss: 88.9685 - Si-sdr: 30.7150 - val_loss: 90.1794 - val_Si-sdr: 30.3643\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 88.46367\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 1s 492ms/step - loss: 89.0691 - Si-sdr: 30.6328 - val_loss: 86.3152 - val_Si-sdr: 31.2911\n",
      "\n",
      "Epoch 00092: val_loss improved from 88.46367 to 86.31522, saving model to ./CKPT\\CKP_ep_92__loss_86.31522_.h5\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 87.7536 - Si-sdr: 31.0085 - val_loss: 86.8057 - val_Si-sdr: 31.2143\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 86.31522\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 85.9903 - Si-sdr: 31.3869 - val_loss: 86.8876 - val_Si-sdr: 31.1523\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 86.31522\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 86.6664 - Si-sdr: 31.1917 - val_loss: 85.2483 - val_Si-sdr: 31.5085\n",
      "\n",
      "Epoch 00095: val_loss improved from 86.31522 to 85.24827, saving model to ./CKPT\\CKP_ep_95__loss_85.24827_.h5\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 84.9478 - Si-sdr: 31.7196 - val_loss: 86.0459 - val_Si-sdr: 31.2834\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 85.24827\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 1s 480ms/step - loss: 85.4543 - Si-sdr: 31.4718 - val_loss: 84.8248 - val_Si-sdr: 31.5419\n",
      "\n",
      "Epoch 00097: val_loss improved from 85.24827 to 84.82484, saving model to ./CKPT\\CKP_ep_97__loss_84.82484_.h5\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 1s 497ms/step - loss: 85.4900 - Si-sdr: 31.5395 - val_loss: 84.8189 - val_Si-sdr: 31.6479\n",
      "\n",
      "Epoch 00098: val_loss improved from 84.82484 to 84.81886, saving model to ./CKPT\\CKP_ep_98__loss_84.81886_.h5\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 83.9902 - Si-sdr: 31.9034 - val_loss: 84.1878 - val_Si-sdr: 31.7520\n",
      "\n",
      "Epoch 00099: val_loss improved from 84.81886 to 84.18775, saving model to ./CKPT\\CKP_ep_99__loss_84.18775_.h5\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 83.5904 - Si-sdr: 31.9837 - val_loss: 82.2610 - val_Si-sdr: 32.2263\n",
      "\n",
      "Epoch 00100: val_loss improved from 84.18775 to 82.26105, saving model to ./CKPT\\CKP_ep_100__loss_82.26105_.h5\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 100\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60cbdb",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e935f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "409e2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47b79c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4241781e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_10 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_100__loss_82.26105_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "        tf.executing_eagerly() # requires r1.7\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)\n",
    "#         result = vq_vae.predict(input_batch)\n",
    "#         print(result)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=1, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.28700954]\n",
      "  [-0.4718614 ]\n",
      "  [-0.3124033 ]\n",
      "  [-0.2531465 ]\n",
      "  [-0.17277378]\n",
      "  [-0.13993308]\n",
      "  [-0.23689504]\n",
      "  [-0.15480173]\n",
      "  [-0.15366751]\n",
      "  [-0.21193263]]\n",
      "\n",
      " [[ 0.00840923]\n",
      "  [ 0.05616647]\n",
      "  [-0.17278165]\n",
      "  [-0.39486957]\n",
      "  [-0.24275026]\n",
      "  [-0.14337276]\n",
      "  [-0.0620722 ]\n",
      "  [-0.33342713]\n",
      "  [-0.12811692]\n",
      "  [-0.41662824]]]\n",
      "(2, 10, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23794276, 0.3758409 , 0.20595631, 0.18026009],\n",
       "        [0.16875331, 0.20575501, 0.26814145, 0.35735017],\n",
       "        [0.08178392, 0.23333283, 0.32606637, 0.35881683],\n",
       "        [0.13777426, 0.4296873 , 0.2569556 , 0.17558284],\n",
       "        [0.2141833 , 0.3373918 , 0.2515016 , 0.19692335]],\n",
       "\n",
       "       [[0.2109271 , 0.3640199 , 0.20789267, 0.21716031],\n",
       "        [0.25047418, 0.33341768, 0.23042291, 0.18568526],\n",
       "        [0.22128512, 0.30281523, 0.21816052, 0.25773916],\n",
       "        [0.17393257, 0.30159497, 0.2893706 , 0.23510183],\n",
       "        [0.29052198, 0.27541766, 0.23140128, 0.2026591 ]]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
