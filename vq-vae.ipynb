{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax (Softmax)            (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - ETA: 0s - loss: 634.7297 - Si-sdr: -260.8775"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step - loss: 634.7297 - Si-sdr: -260.8775 - val_loss: 632.0584 - val_Si-sdr: -222.9522\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 632.05841, saving model to ./CKPT\\CKP_ep_1__loss_632.05841_.h5\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 1s 804ms/step - loss: 631.5106 - Si-sdr: -244.2579 - val_loss: 629.4757 - val_Si-sdr: -243.2752\n",
      "\n",
      "Epoch 00002: val_loss improved from 632.05841 to 629.47571, saving model to ./CKPT\\CKP_ep_2__loss_629.47571_.h5\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 1s 763ms/step - loss: 628.7528 - Si-sdr: -219.0233 - val_loss: 628.1381 - val_Si-sdr: -202.9614\n",
      "\n",
      "Epoch 00003: val_loss improved from 629.47571 to 628.13806, saving model to ./CKPT\\CKP_ep_3__loss_628.13806_.h5\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 1s 664ms/step - loss: 628.3107 - Si-sdr: -222.8348 - val_loss: 628.1638 - val_Si-sdr: -250.7644\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 628.13806\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 1s 773ms/step - loss: 628.2162 - Si-sdr: -224.7879 - val_loss: 627.9109 - val_Si-sdr: -224.0312\n",
      "\n",
      "Epoch 00005: val_loss improved from 628.13806 to 627.91089, saving model to ./CKPT\\CKP_ep_5__loss_627.91089_.h5\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 627.8625 - Si-sdr: -218.6673 - val_loss: 627.6707 - val_Si-sdr: -271.5607\n",
      "\n",
      "Epoch 00006: val_loss improved from 627.91089 to 627.67065, saving model to ./CKPT\\CKP_ep_6__loss_627.67065_.h5\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 627.4799 - Si-sdr: -211.7897 - val_loss: 627.3275 - val_Si-sdr: -183.2043\n",
      "\n",
      "Epoch 00007: val_loss improved from 627.67065 to 627.32751, saving model to ./CKPT\\CKP_ep_7__loss_627.32751_.h5\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 627.3585 - Si-sdr: -196.7797 - val_loss: 627.4468 - val_Si-sdr: -218.3549\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 627.32751\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 627.3252 - Si-sdr: -190.5535 - val_loss: 627.2898 - val_Si-sdr: -178.8140\n",
      "\n",
      "Epoch 00009: val_loss improved from 627.32751 to 627.28979, saving model to ./CKPT\\CKP_ep_9__loss_627.28979_.h5\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 627.4448 - Si-sdr: -213.7399 - val_loss: 627.2455 - val_Si-sdr: -175.2617\n",
      "\n",
      "Epoch 00010: val_loss improved from 627.28979 to 627.24548, saving model to ./CKPT\\CKP_ep_10__loss_627.24548_.h5\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 627.1717 - Si-sdr: -161.6800 - val_loss: 627.0968 - val_Si-sdr: -150.5590\n",
      "\n",
      "Epoch 00011: val_loss improved from 627.24548 to 627.09680, saving model to ./CKPT\\CKP_ep_11__loss_627.09680_.h5\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 1s 776ms/step - loss: 627.1782 - Si-sdr: -164.7848 - val_loss: 626.9733 - val_Si-sdr: -139.9633\n",
      "\n",
      "Epoch 00012: val_loss improved from 627.09680 to 626.97327, saving model to ./CKPT\\CKP_ep_12__loss_626.97327_.h5\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 1s 767ms/step - loss: 626.9864 - Si-sdr: -139.5190 - val_loss: 626.6672 - val_Si-sdr: -118.2613\n",
      "\n",
      "Epoch 00013: val_loss improved from 626.97327 to 626.66718, saving model to ./CKPT\\CKP_ep_13__loss_626.66718_.h5\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 1s 807ms/step - loss: 626.6511 - Si-sdr: -118.9432 - val_loss: 626.2653 - val_Si-sdr: -102.9503\n",
      "\n",
      "Epoch 00014: val_loss improved from 626.66718 to 626.26532, saving model to ./CKPT\\CKP_ep_14__loss_626.26532_.h5\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 1s 773ms/step - loss: 625.9988 - Si-sdr: -96.5209 - val_loss: 625.1919 - val_Si-sdr: -84.5993\n",
      "\n",
      "Epoch 00015: val_loss improved from 626.26532 to 625.19189, saving model to ./CKPT\\CKP_ep_15__loss_625.19189_.h5\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 624.6075 - Si-sdr: -80.0901 - val_loss: 622.7712 - val_Si-sdr: -70.0196\n",
      "\n",
      "Epoch 00016: val_loss improved from 625.19189 to 622.77124, saving model to ./CKPT\\CKP_ep_16__loss_622.77124_.h5\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 621.5571 - Si-sdr: -65.7882 - val_loss: 617.1426 - val_Si-sdr: -59.7606\n",
      "\n",
      "Epoch 00017: val_loss improved from 622.77124 to 617.14258, saving model to ./CKPT\\CKP_ep_17__loss_617.14258_.h5\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 1s 770ms/step - loss: 615.7056 - Si-sdr: -59.6347 - val_loss: 609.1151 - val_Si-sdr: -57.0855\n",
      "\n",
      "Epoch 00018: val_loss improved from 617.14258 to 609.11511, saving model to ./CKPT\\CKP_ep_18__loss_609.11511_.h5\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 606.7264 - Si-sdr: -55.0845 - val_loss: 600.9713 - val_Si-sdr: -53.6835\n",
      "\n",
      "Epoch 00019: val_loss improved from 609.11511 to 600.97131, saving model to ./CKPT\\CKP_ep_19__loss_600.97131_.h5\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 1s 806ms/step - loss: 600.8801 - Si-sdr: -53.1484 - val_loss: 594.0092 - val_Si-sdr: -50.0513\n",
      "\n",
      "Epoch 00020: val_loss improved from 600.97131 to 594.00916, saving model to ./CKPT\\CKP_ep_20__loss_594.00916_.h5\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 593.1321 - Si-sdr: -49.5648 - val_loss: 581.4051 - val_Si-sdr: -43.7262\n",
      "\n",
      "Epoch 00021: val_loss improved from 594.00916 to 581.40509, saving model to ./CKPT\\CKP_ep_21__loss_581.40509_.h5\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 575.3713 - Si-sdr: -40.5747 - val_loss: 557.4686 - val_Si-sdr: -32.2079\n",
      "\n",
      "Epoch 00022: val_loss improved from 581.40509 to 557.46863, saving model to ./CKPT\\CKP_ep_22__loss_557.46863_.h5\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 552.1112 - Si-sdr: -30.5330 - val_loss: 534.9413 - val_Si-sdr: -28.8314\n",
      "\n",
      "Epoch 00023: val_loss improved from 557.46863 to 534.94135, saving model to ./CKPT\\CKP_ep_23__loss_534.94135_.h5\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 528.3061 - Si-sdr: -27.2989 - val_loss: 504.0037 - val_Si-sdr: -23.1950\n",
      "\n",
      "Epoch 00024: val_loss improved from 534.94135 to 504.00366, saving model to ./CKPT\\CKP_ep_24__loss_504.00366_.h5\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 500.5528 - Si-sdr: -22.9364 - val_loss: 472.8032 - val_Si-sdr: -18.6610\n",
      "\n",
      "Epoch 00025: val_loss improved from 504.00366 to 472.80322, saving model to ./CKPT\\CKP_ep_25__loss_472.80322_.h5\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 467.2778 - Si-sdr: -17.9812 - val_loss: 442.3439 - val_Si-sdr: -14.6264\n",
      "\n",
      "Epoch 00026: val_loss improved from 472.80322 to 442.34387, saving model to ./CKPT\\CKP_ep_26__loss_442.34387_.h5\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 435.4781 - Si-sdr: -13.5249 - val_loss: 411.9971 - val_Si-sdr: -10.7079\n",
      "\n",
      "Epoch 00027: val_loss improved from 442.34387 to 411.99707, saving model to ./CKPT\\CKP_ep_27__loss_411.99707_.h5\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 406.8828 - Si-sdr: -10.0777 - val_loss: 389.7974 - val_Si-sdr: -8.0702\n",
      "\n",
      "Epoch 00028: val_loss improved from 411.99707 to 389.79739, saving model to ./CKPT\\CKP_ep_28__loss_389.79739_.h5\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 383.2697 - Si-sdr: -7.1600 - val_loss: 366.3461 - val_Si-sdr: -5.4266\n",
      "\n",
      "Epoch 00029: val_loss improved from 389.79739 to 366.34607, saving model to ./CKPT\\CKP_ep_29__loss_366.34607_.h5\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 360.2718 - Si-sdr: -4.6476 - val_loss: 349.5864 - val_Si-sdr: -3.3410\n",
      "\n",
      "Epoch 00030: val_loss improved from 366.34607 to 349.58636, saving model to ./CKPT\\CKP_ep_30__loss_349.58636_.h5\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 345.6994 - Si-sdr: -3.0390 - val_loss: 345.2974 - val_Si-sdr: -2.9439\n",
      "\n",
      "Epoch 00031: val_loss improved from 349.58636 to 345.29736, saving model to ./CKPT\\CKP_ep_31__loss_345.29736_.h5\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 342.9817 - Si-sdr: -2.8436 - val_loss: 331.5402 - val_Si-sdr: -1.5933\n",
      "\n",
      "Epoch 00032: val_loss improved from 345.29736 to 331.54022, saving model to ./CKPT\\CKP_ep_32__loss_331.54022_.h5\n",
      "Epoch 33/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 650ms/step - loss: 328.7020 - Si-sdr: -1.3753 - val_loss: 323.0475 - val_Si-sdr: -0.7638\n",
      "\n",
      "Epoch 00033: val_loss improved from 331.54022 to 323.04755, saving model to ./CKPT\\CKP_ep_33__loss_323.04755_.h5\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 323.2036 - Si-sdr: -0.6322 - val_loss: 309.1516 - val_Si-sdr: 0.7496\n",
      "\n",
      "Epoch 00034: val_loss improved from 323.04755 to 309.15155, saving model to ./CKPT\\CKP_ep_34__loss_309.15155_.h5\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 308.9323 - Si-sdr: 0.7294 - val_loss: 294.5701 - val_Si-sdr: 2.2450\n",
      "\n",
      "Epoch 00035: val_loss improved from 309.15155 to 294.57007, saving model to ./CKPT\\CKP_ep_35__loss_294.57007_.h5\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 295.4250 - Si-sdr: 2.2456 - val_loss: 288.5581 - val_Si-sdr: 2.9431\n",
      "\n",
      "Epoch 00036: val_loss improved from 294.57007 to 288.55814, saving model to ./CKPT\\CKP_ep_36__loss_288.55814_.h5\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 287.2991 - Si-sdr: 3.0425 - val_loss: 281.4278 - val_Si-sdr: 3.7986\n",
      "\n",
      "Epoch 00037: val_loss improved from 288.55814 to 281.42783, saving model to ./CKPT\\CKP_ep_37__loss_281.42783_.h5\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 275.4922 - Si-sdr: 4.4113 - val_loss: 273.9705 - val_Si-sdr: 4.4727\n",
      "\n",
      "Epoch 00038: val_loss improved from 281.42783 to 273.97046, saving model to ./CKPT\\CKP_ep_38__loss_273.97046_.h5\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 1s 652ms/step - loss: 269.4604 - Si-sdr: 5.0636 - val_loss: 272.2134 - val_Si-sdr: 4.6377\n",
      "\n",
      "Epoch 00039: val_loss improved from 273.97046 to 272.21338, saving model to ./CKPT\\CKP_ep_39__loss_272.21338_.h5\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 266.7454 - Si-sdr: 5.2751 - val_loss: 262.9570 - val_Si-sdr: 5.7722\n",
      "\n",
      "Epoch 00040: val_loss improved from 272.21338 to 262.95703, saving model to ./CKPT\\CKP_ep_40__loss_262.95703_.h5\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 258.1911 - Si-sdr: 6.0604 - val_loss: 257.8026 - val_Si-sdr: 6.1555\n",
      "\n",
      "Epoch 00041: val_loss improved from 262.95703 to 257.80261, saving model to ./CKPT\\CKP_ep_41__loss_257.80261_.h5\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 255.3871 - Si-sdr: 6.4957 - val_loss: 246.0687 - val_Si-sdr: 7.5808\n",
      "\n",
      "Epoch 00042: val_loss improved from 257.80261 to 246.06874, saving model to ./CKPT\\CKP_ep_42__loss_246.06874_.h5\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 244.9894 - Si-sdr: 7.7169 - val_loss: 239.3094 - val_Si-sdr: 8.2695\n",
      "\n",
      "Epoch 00043: val_loss improved from 246.06874 to 239.30936, saving model to ./CKPT\\CKP_ep_43__loss_239.30936_.h5\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 240.8787 - Si-sdr: 8.2031 - val_loss: 239.0128 - val_Si-sdr: 8.2899\n",
      "\n",
      "Epoch 00044: val_loss improved from 239.30936 to 239.01285, saving model to ./CKPT\\CKP_ep_44__loss_239.01285_.h5\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 242.8024 - Si-sdr: 7.7496 - val_loss: 233.3009 - val_Si-sdr: 8.9006\n",
      "\n",
      "Epoch 00045: val_loss improved from 239.01285 to 233.30087, saving model to ./CKPT\\CKP_ep_45__loss_233.30087_.h5\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 233.2799 - Si-sdr: 8.9070 - val_loss: 231.9185 - val_Si-sdr: 9.0207\n",
      "\n",
      "Epoch 00046: val_loss improved from 233.30087 to 231.91847, saving model to ./CKPT\\CKP_ep_46__loss_231.91847_.h5\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 234.3488 - Si-sdr: 8.8509 - val_loss: 228.9723 - val_Si-sdr: 9.4069\n",
      "\n",
      "Epoch 00047: val_loss improved from 231.91847 to 228.97227, saving model to ./CKPT\\CKP_ep_47__loss_228.97227_.h5\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 229.1934 - Si-sdr: 9.3821 - val_loss: 226.6897 - val_Si-sdr: 9.7902\n",
      "\n",
      "Epoch 00048: val_loss improved from 228.97227 to 226.68973, saving model to ./CKPT\\CKP_ep_48__loss_226.68973_.h5\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 223.9830 - Si-sdr: 10.1417 - val_loss: 220.7860 - val_Si-sdr: 10.3718\n",
      "\n",
      "Epoch 00049: val_loss improved from 226.68973 to 220.78601, saving model to ./CKPT\\CKP_ep_49__loss_220.78601_.h5\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 222.0419 - Si-sdr: 10.2029 - val_loss: 220.7501 - val_Si-sdr: 10.4602\n",
      "\n",
      "Epoch 00050: val_loss improved from 220.78601 to 220.75008, saving model to ./CKPT\\CKP_ep_50__loss_220.75008_.h5\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 220.3382 - Si-sdr: 10.3654 - val_loss: 217.8964 - val_Si-sdr: 10.7251\n",
      "\n",
      "Epoch 00051: val_loss improved from 220.75008 to 217.89638, saving model to ./CKPT\\CKP_ep_51__loss_217.89638_.h5\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 214.9940 - Si-sdr: 11.0782 - val_loss: 213.0522 - val_Si-sdr: 11.4541\n",
      "\n",
      "Epoch 00052: val_loss improved from 217.89638 to 213.05215, saving model to ./CKPT\\CKP_ep_52__loss_213.05215_.h5\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 215.2932 - Si-sdr: 11.0266 - val_loss: 213.9056 - val_Si-sdr: 11.3519\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 213.05215\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 1s 583ms/step - loss: 212.2579 - Si-sdr: 11.4068 - val_loss: 207.4267 - val_Si-sdr: 12.0067\n",
      "\n",
      "Epoch 00054: val_loss improved from 213.05215 to 207.42665, saving model to ./CKPT\\CKP_ep_54__loss_207.42665_.h5\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 209.7680 - Si-sdr: 11.7112 - val_loss: 207.2209 - val_Si-sdr: 12.1395\n",
      "\n",
      "Epoch 00055: val_loss improved from 207.42665 to 207.22092, saving model to ./CKPT\\CKP_ep_55__loss_207.22092_.h5\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 208.9810 - Si-sdr: 11.9258 - val_loss: 206.7747 - val_Si-sdr: 12.1978\n",
      "\n",
      "Epoch 00056: val_loss improved from 207.22092 to 206.77473, saving model to ./CKPT\\CKP_ep_56__loss_206.77473_.h5\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 210.1152 - Si-sdr: 11.7254 - val_loss: 213.1668 - val_Si-sdr: 11.2945\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 206.77473\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 209.3815 - Si-sdr: 11.8453 - val_loss: 205.8833 - val_Si-sdr: 12.1916\n",
      "\n",
      "Epoch 00058: val_loss improved from 206.77473 to 205.88333, saving model to ./CKPT\\CKP_ep_58__loss_205.88333_.h5\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 204.5058 - Si-sdr: 12.2969 - val_loss: 203.1329 - val_Si-sdr: 12.6037\n",
      "\n",
      "Epoch 00059: val_loss improved from 205.88333 to 203.13289, saving model to ./CKPT\\CKP_ep_59__loss_203.13289_.h5\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 199.7182 - Si-sdr: 12.9610 - val_loss: 199.8124 - val_Si-sdr: 13.0273\n",
      "\n",
      "Epoch 00060: val_loss improved from 203.13289 to 199.81239, saving model to ./CKPT\\CKP_ep_60__loss_199.81239_.h5\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 200.3088 - Si-sdr: 12.9864 - val_loss: 199.1244 - val_Si-sdr: 13.0584\n",
      "\n",
      "Epoch 00061: val_loss improved from 199.81239 to 199.12436, saving model to ./CKPT\\CKP_ep_61__loss_199.12436_.h5\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 1s 727ms/step - loss: 200.5132 - Si-sdr: 12.9893 - val_loss: 201.0505 - val_Si-sdr: 12.9503\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 199.12436\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 199.4391 - Si-sdr: 13.1156 - val_loss: 197.9000 - val_Si-sdr: 13.1789\n",
      "\n",
      "Epoch 00063: val_loss improved from 199.12436 to 197.90004, saving model to ./CKPT\\CKP_ep_63__loss_197.90004_.h5\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 195.7626 - Si-sdr: 13.6244 - val_loss: 196.4539 - val_Si-sdr: 13.3412\n",
      "\n",
      "Epoch 00064: val_loss improved from 197.90004 to 196.45392, saving model to ./CKPT\\CKP_ep_64__loss_196.45392_.h5\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 197.2273 - Si-sdr: 13.2913 - val_loss: 202.3504 - val_Si-sdr: 12.5840\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 196.45392\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 613ms/step - loss: 199.7366 - Si-sdr: 13.0513 - val_loss: 197.9937 - val_Si-sdr: 13.1789\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 196.45392\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 195.0018 - Si-sdr: 13.4929 - val_loss: 197.5035 - val_Si-sdr: 13.3415\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 196.45392\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 195.1840 - Si-sdr: 13.4990 - val_loss: 189.1712 - val_Si-sdr: 14.2989\n",
      "\n",
      "Epoch 00068: val_loss improved from 196.45392 to 189.17120, saving model to ./CKPT\\CKP_ep_68__loss_189.17120_.h5\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 189.9528 - Si-sdr: 14.2065 - val_loss: 190.4040 - val_Si-sdr: 14.0972\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 189.17120\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 191.4688 - Si-sdr: 13.9516 - val_loss: 191.7602 - val_Si-sdr: 13.7799\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 189.17120\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 192.0192 - Si-sdr: 13.9260 - val_loss: 190.1541 - val_Si-sdr: 14.1768\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 189.17120\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 192.6754 - Si-sdr: 13.7502 - val_loss: 188.3323 - val_Si-sdr: 14.3302\n",
      "\n",
      "Epoch 00072: val_loss improved from 189.17120 to 188.33231, saving model to ./CKPT\\CKP_ep_72__loss_188.33231_.h5\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 188.6396 - Si-sdr: 14.3131 - val_loss: 188.7296 - val_Si-sdr: 14.2686\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 188.33231\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 190.4258 - Si-sdr: 14.2833 - val_loss: 188.3926 - val_Si-sdr: 14.4702\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 188.33231\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 188.9666 - Si-sdr: 14.3486 - val_loss: 191.8117 - val_Si-sdr: 14.0433\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 188.33231\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 1s 726ms/step - loss: 195.9245 - Si-sdr: 13.5066 - val_loss: 187.6434 - val_Si-sdr: 14.5499\n",
      "\n",
      "Epoch 00076: val_loss improved from 188.33231 to 187.64343, saving model to ./CKPT\\CKP_ep_76__loss_187.64343_.h5\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 184.4078 - Si-sdr: 14.9784 - val_loss: 183.2631 - val_Si-sdr: 15.0629\n",
      "\n",
      "Epoch 00077: val_loss improved from 187.64343 to 183.26312, saving model to ./CKPT\\CKP_ep_77__loss_183.26312_.h5\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 188.0865 - Si-sdr: 14.4948 - val_loss: 182.5715 - val_Si-sdr: 15.0947\n",
      "\n",
      "Epoch 00078: val_loss improved from 183.26312 to 182.57153, saving model to ./CKPT\\CKP_ep_78__loss_182.57153_.h5\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 186.7821 - Si-sdr: 14.4885 - val_loss: 182.0788 - val_Si-sdr: 15.1620\n",
      "\n",
      "Epoch 00079: val_loss improved from 182.57153 to 182.07877, saving model to ./CKPT\\CKP_ep_79__loss_182.07877_.h5\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 182.3146 - Si-sdr: 15.1242 - val_loss: 182.0820 - val_Si-sdr: 15.0877\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 182.07877\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 183.6418 - Si-sdr: 15.0092 - val_loss: 183.3308 - val_Si-sdr: 15.0714\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 182.07877\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 1s 818ms/step - loss: 183.5110 - Si-sdr: 14.8842 - val_loss: 186.2878 - val_Si-sdr: 14.6214\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 182.07877\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 182.8485 - Si-sdr: 15.0438 - val_loss: 177.3218 - val_Si-sdr: 15.9221\n",
      "\n",
      "Epoch 00083: val_loss improved from 182.07877 to 177.32184, saving model to ./CKPT\\CKP_ep_83__loss_177.32184_.h5\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 177.4381 - Si-sdr: 15.7976 - val_loss: 183.0944 - val_Si-sdr: 15.0747\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 177.32184\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 184.0656 - Si-sdr: 14.8926 - val_loss: 179.3886 - val_Si-sdr: 15.6543\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 177.32184\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 178.3037 - Si-sdr: 15.7312 - val_loss: 175.4960 - val_Si-sdr: 16.0616\n",
      "\n",
      "Epoch 00086: val_loss improved from 177.32184 to 175.49602, saving model to ./CKPT\\CKP_ep_86__loss_175.49602_.h5\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 175.2042 - Si-sdr: 16.1594 - val_loss: 181.3637 - val_Si-sdr: 15.1734\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 175.49602\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 180.8550 - Si-sdr: 15.2132 - val_loss: 179.6807 - val_Si-sdr: 15.3232\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 175.49602\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 180.7772 - Si-sdr: 15.2395 - val_loss: 178.4970 - val_Si-sdr: 15.5521\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 175.49602\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 181.1356 - Si-sdr: 15.3197 - val_loss: 176.4983 - val_Si-sdr: 15.8565\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 175.49602\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 179.7855 - Si-sdr: 15.4608 - val_loss: 177.5192 - val_Si-sdr: 15.7605\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 175.49602\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 175.5323 - Si-sdr: 16.0580 - val_loss: 175.0534 - val_Si-sdr: 16.0792\n",
      "\n",
      "Epoch 00092: val_loss improved from 175.49602 to 175.05341, saving model to ./CKPT\\CKP_ep_92__loss_175.05341_.h5\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 177.1644 - Si-sdr: 15.7253 - val_loss: 174.2004 - val_Si-sdr: 16.1758\n",
      "\n",
      "Epoch 00093: val_loss improved from 175.05341 to 174.20044, saving model to ./CKPT\\CKP_ep_93__loss_174.20044_.h5\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 174.1975 - Si-sdr: 16.1775 - val_loss: 175.2245 - val_Si-sdr: 16.0916\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 174.20044\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 174.5343 - Si-sdr: 16.1314 - val_loss: 175.1630 - val_Si-sdr: 15.9803\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 174.20044\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 177.7696 - Si-sdr: 15.6733 - val_loss: 174.3787 - val_Si-sdr: 16.1753\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 174.20044\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 173.2044 - Si-sdr: 16.3977 - val_loss: 174.1996 - val_Si-sdr: 16.1579\n",
      "\n",
      "Epoch 00097: val_loss improved from 174.20044 to 174.19958, saving model to ./CKPT\\CKP_ep_97__loss_174.19958_.h5\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 1s 585ms/step - loss: 174.2710 - Si-sdr: 16.1606 - val_loss: 172.9720 - val_Si-sdr: 16.4306\n",
      "\n",
      "Epoch 00098: val_loss improved from 174.19958 to 172.97205, saving model to ./CKPT\\CKP_ep_98__loss_172.97205_.h5\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 171.5350 - Si-sdr: 16.5477 - val_loss: 175.1028 - val_Si-sdr: 16.0584\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 172.97205\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 176.0402 - Si-sdr: 16.0034 - val_loss: 172.9284 - val_Si-sdr: 16.3345\n",
      "\n",
      "Epoch 00100: val_loss improved from 172.97205 to 172.92836, saving model to ./CKPT\\CKP_ep_100__loss_172.92836_.h5\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 171.0581 - Si-sdr: 16.5002 - val_loss: 173.6408 - val_Si-sdr: 16.1733\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 172.92836\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 1s 603ms/step - loss: 171.1511 - Si-sdr: 16.6192 - val_loss: 176.6381 - val_Si-sdr: 15.8159\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 172.92836\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 170.7774 - Si-sdr: 16.6802 - val_loss: 171.3975 - val_Si-sdr: 16.5800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00103: val_loss improved from 172.92836 to 171.39751, saving model to ./CKPT\\CKP_ep_103__loss_171.39751_.h5\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 172.4542 - Si-sdr: 16.3649 - val_loss: 170.8503 - val_Si-sdr: 16.7015\n",
      "\n",
      "Epoch 00104: val_loss improved from 171.39751 to 170.85028, saving model to ./CKPT\\CKP_ep_104__loss_170.85028_.h5\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 172.1273 - Si-sdr: 16.5210 - val_loss: 174.5478 - val_Si-sdr: 16.1844\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 170.85028\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 174.5616 - Si-sdr: 16.0929 - val_loss: 175.1502 - val_Si-sdr: 15.9056\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 170.85028\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 172.8551 - Si-sdr: 16.4023 - val_loss: 176.1766 - val_Si-sdr: 15.8164\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 170.85028\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 175.0439 - Si-sdr: 16.0410 - val_loss: 174.4723 - val_Si-sdr: 15.9149\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 170.85028\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 1s 604ms/step - loss: 174.9745 - Si-sdr: 16.0019 - val_loss: 173.5390 - val_Si-sdr: 16.2313\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 170.85028\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 175.6489 - Si-sdr: 15.9317 - val_loss: 169.6691 - val_Si-sdr: 16.8792\n",
      "\n",
      "Epoch 00110: val_loss improved from 170.85028 to 169.66913, saving model to ./CKPT\\CKP_ep_110__loss_169.66913_.h5\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 170.3615 - Si-sdr: 16.6836 - val_loss: 170.2517 - val_Si-sdr: 16.7772\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 169.66913\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 171.5143 - Si-sdr: 16.4904 - val_loss: 173.9704 - val_Si-sdr: 16.0917\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 169.66913\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 170.5686 - Si-sdr: 16.6191 - val_loss: 175.6949 - val_Si-sdr: 15.9704\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 169.66913\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 171.6145 - Si-sdr: 16.4184 - val_loss: 163.9978 - val_Si-sdr: 17.5622\n",
      "\n",
      "Epoch 00114: val_loss improved from 169.66913 to 163.99777, saving model to ./CKPT\\CKP_ep_114__loss_163.99777_.h5\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 166.0964 - Si-sdr: 17.4616 - val_loss: 165.8099 - val_Si-sdr: 17.3124\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 163.99777\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 168.9495 - Si-sdr: 17.0417 - val_loss: 169.4224 - val_Si-sdr: 16.9170\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 163.99777\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 171.2761 - Si-sdr: 16.6796 - val_loss: 167.2202 - val_Si-sdr: 17.0837\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 163.99777\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 168.2538 - Si-sdr: 17.1176 - val_loss: 168.1362 - val_Si-sdr: 17.0604\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 163.99777\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 164.7359 - Si-sdr: 17.6111 - val_loss: 167.3843 - val_Si-sdr: 17.0082\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 163.99777\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 170.3796 - Si-sdr: 16.6941 - val_loss: 169.2466 - val_Si-sdr: 17.0876\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 163.99777\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 166.0081 - Si-sdr: 17.4060 - val_loss: 165.6702 - val_Si-sdr: 17.4623\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 163.99777\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 164.6564 - Si-sdr: 17.5349 - val_loss: 167.2766 - val_Si-sdr: 17.0634\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 163.99777\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 164.2902 - Si-sdr: 17.4963 - val_loss: 165.3789 - val_Si-sdr: 17.3283\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 163.99777\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 165.6775 - Si-sdr: 17.3728 - val_loss: 163.7496 - val_Si-sdr: 17.7210\n",
      "\n",
      "Epoch 00124: val_loss improved from 163.99777 to 163.74960, saving model to ./CKPT\\CKP_ep_124__loss_163.74960_.h5\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 169.1572 - Si-sdr: 16.9593 - val_loss: 163.8388 - val_Si-sdr: 17.5982\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 163.74960\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 165.3373 - Si-sdr: 17.3587 - val_loss: 163.7289 - val_Si-sdr: 17.6212\n",
      "\n",
      "Epoch 00126: val_loss improved from 163.74960 to 163.72894, saving model to ./CKPT\\CKP_ep_126__loss_163.72894_.h5\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 163.5087 - Si-sdr: 17.7048 - val_loss: 163.4795 - val_Si-sdr: 17.6995\n",
      "\n",
      "Epoch 00127: val_loss improved from 163.72894 to 163.47954, saving model to ./CKPT\\CKP_ep_127__loss_163.47954_.h5\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 165.7777 - Si-sdr: 17.3110 - val_loss: 163.6700 - val_Si-sdr: 17.6832\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 163.47954\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 166.4131 - Si-sdr: 17.2551 - val_loss: 164.1015 - val_Si-sdr: 17.5265\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 163.47954\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 163.5865 - Si-sdr: 17.5137 - val_loss: 163.2729 - val_Si-sdr: 17.6363\n",
      "\n",
      "Epoch 00130: val_loss improved from 163.47954 to 163.27286, saving model to ./CKPT\\CKP_ep_130__loss_163.27286_.h5\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 162.5276 - Si-sdr: 17.6937 - val_loss: 161.9133 - val_Si-sdr: 17.8702\n",
      "\n",
      "Epoch 00131: val_loss improved from 163.27286 to 161.91327, saving model to ./CKPT\\CKP_ep_131__loss_161.91327_.h5\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 159.9788 - Si-sdr: 18.2294 - val_loss: 163.3332 - val_Si-sdr: 17.6537\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 161.91327\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 163.0169 - Si-sdr: 17.7265 - val_loss: 162.3003 - val_Si-sdr: 17.8103\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 161.91327\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 161.7186 - Si-sdr: 17.9455 - val_loss: 164.4876 - val_Si-sdr: 17.5401\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 161.91327\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 162.4313 - Si-sdr: 17.8840 - val_loss: 165.9505 - val_Si-sdr: 17.3508\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 161.91327\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 164.6172 - Si-sdr: 17.4385 - val_loss: 163.9151 - val_Si-sdr: 17.7491\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 161.91327\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 165.4178 - Si-sdr: 17.5337 - val_loss: 161.4899 - val_Si-sdr: 17.9826\n",
      "\n",
      "Epoch 00137: val_loss improved from 161.91327 to 161.48990, saving model to ./CKPT\\CKP_ep_137__loss_161.48990_.h5\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 159.6463 - Si-sdr: 18.1932 - val_loss: 159.0366 - val_Si-sdr: 18.3310\n",
      "\n",
      "Epoch 00138: val_loss improved from 161.48990 to 159.03656, saving model to ./CKPT\\CKP_ep_138__loss_159.03656_.h5\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 159.2538 - Si-sdr: 18.1315 - val_loss: 165.7135 - val_Si-sdr: 17.2806\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 159.03656\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 159.7635 - Si-sdr: 18.3978 - val_loss: 163.6735 - val_Si-sdr: 17.7302\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 159.03656\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 161.3720 - Si-sdr: 18.0417 - val_loss: 159.2741 - val_Si-sdr: 18.4719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00141: val_loss did not improve from 159.03656\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 160.0375 - Si-sdr: 18.5107 - val_loss: 162.2784 - val_Si-sdr: 17.9225\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 159.03656\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 160.9550 - Si-sdr: 17.9441 - val_loss: 165.4547 - val_Si-sdr: 17.5024\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 159.03656\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 165.3210 - Si-sdr: 17.6751 - val_loss: 161.5985 - val_Si-sdr: 18.0631\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 159.03656\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 162.5649 - Si-sdr: 17.9792 - val_loss: 158.6469 - val_Si-sdr: 18.4516\n",
      "\n",
      "Epoch 00145: val_loss improved from 159.03656 to 158.64691, saving model to ./CKPT\\CKP_ep_145__loss_158.64691_.h5\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 157.5287 - Si-sdr: 18.6590 - val_loss: 158.1015 - val_Si-sdr: 18.4577\n",
      "\n",
      "Epoch 00146: val_loss improved from 158.64691 to 158.10152, saving model to ./CKPT\\CKP_ep_146__loss_158.10152_.h5\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 158.5275 - Si-sdr: 18.4505 - val_loss: 157.1494 - val_Si-sdr: 18.5302\n",
      "\n",
      "Epoch 00147: val_loss improved from 158.10152 to 157.14941, saving model to ./CKPT\\CKP_ep_147__loss_157.14941_.h5\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 157.4345 - Si-sdr: 18.5463 - val_loss: 158.2803 - val_Si-sdr: 18.4100\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 157.14941\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 159.7332 - Si-sdr: 18.2277 - val_loss: 156.9829 - val_Si-sdr: 18.6007\n",
      "\n",
      "Epoch 00149: val_loss improved from 157.14941 to 156.98294, saving model to ./CKPT\\CKP_ep_149__loss_156.98294_.h5\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 155.4767 - Si-sdr: 18.8311 - val_loss: 155.4952 - val_Si-sdr: 18.7857\n",
      "\n",
      "Epoch 00150: val_loss improved from 156.98294 to 155.49516, saving model to ./CKPT\\CKP_ep_150__loss_155.49516_.h5\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 157.8588 - Si-sdr: 18.4039 - val_loss: 160.6342 - val_Si-sdr: 18.1312\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 155.49516\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 158.1814 - Si-sdr: 18.4204 - val_loss: 155.9878 - val_Si-sdr: 18.5923\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 155.49516\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 157.6945 - Si-sdr: 18.5480 - val_loss: 164.7359 - val_Si-sdr: 17.5161\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 155.49516\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 162.9962 - Si-sdr: 17.7022 - val_loss: 161.6706 - val_Si-sdr: 17.8736\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 155.49516\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 156.4267 - Si-sdr: 18.6895 - val_loss: 156.5827 - val_Si-sdr: 18.6713\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 155.49516\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 154.6360 - Si-sdr: 18.9756 - val_loss: 156.9329 - val_Si-sdr: 18.6940\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 155.49516\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 155.8954 - Si-sdr: 18.8629 - val_loss: 156.5347 - val_Si-sdr: 18.5393\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 155.49516\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 156.3118 - Si-sdr: 18.6194 - val_loss: 157.6295 - val_Si-sdr: 18.5154\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 155.49516\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 154.7983 - Si-sdr: 19.0096 - val_loss: 156.2713 - val_Si-sdr: 18.5769\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 155.49516\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 159.0450 - Si-sdr: 18.4429 - val_loss: 157.6979 - val_Si-sdr: 18.5597\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 155.49516\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 154.2340 - Si-sdr: 19.0524 - val_loss: 155.2863 - val_Si-sdr: 18.8625\n",
      "\n",
      "Epoch 00161: val_loss improved from 155.49516 to 155.28625, saving model to ./CKPT\\CKP_ep_161__loss_155.28625_.h5\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 157.3838 - Si-sdr: 18.4728 - val_loss: 162.7439 - val_Si-sdr: 17.7090\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 155.28625\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 160.2797 - Si-sdr: 18.1952 - val_loss: 156.7141 - val_Si-sdr: 18.6855\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 155.28625\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 155.7953 - Si-sdr: 18.9285 - val_loss: 156.4327 - val_Si-sdr: 18.7632\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 155.28625\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 1s 588ms/step - loss: 154.8535 - Si-sdr: 18.8596 - val_loss: 155.9644 - val_Si-sdr: 18.9835\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 155.28625\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 161.7948 - Si-sdr: 18.0120 - val_loss: 156.6985 - val_Si-sdr: 18.6059\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 155.28625\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 157.2787 - Si-sdr: 18.7383 - val_loss: 159.3074 - val_Si-sdr: 18.3474\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 155.28625\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 157.3334 - Si-sdr: 18.7405 - val_loss: 158.1591 - val_Si-sdr: 18.6761\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 155.28625\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 154.2229 - Si-sdr: 19.0213 - val_loss: 151.8788 - val_Si-sdr: 19.2915\n",
      "\n",
      "Epoch 00169: val_loss improved from 155.28625 to 151.87880, saving model to ./CKPT\\CKP_ep_169__loss_151.87880_.h5\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 152.8020 - Si-sdr: 19.1637 - val_loss: 154.0778 - val_Si-sdr: 19.0719\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 151.87880\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 153.7028 - Si-sdr: 19.1203 - val_loss: 155.4484 - val_Si-sdr: 18.7648\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 151.87880\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 152.5764 - Si-sdr: 19.2392 - val_loss: 151.4720 - val_Si-sdr: 19.3880\n",
      "\n",
      "Epoch 00172: val_loss improved from 151.87880 to 151.47202, saving model to ./CKPT\\CKP_ep_172__loss_151.47202_.h5\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 156.0213 - Si-sdr: 18.6945 - val_loss: 155.5786 - val_Si-sdr: 18.8565\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 151.47202\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 152.2084 - Si-sdr: 19.3351 - val_loss: 155.9534 - val_Si-sdr: 18.8830\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 151.47202\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 154.0413 - Si-sdr: 19.1017 - val_loss: 156.3087 - val_Si-sdr: 18.7613\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 151.47202\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 155.8212 - Si-sdr: 18.6972 - val_loss: 154.3326 - val_Si-sdr: 18.9574\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 151.47202\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 157.9183 - Si-sdr: 18.5603 - val_loss: 155.5146 - val_Si-sdr: 18.8268\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 151.47202\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 154.1761 - Si-sdr: 19.1017 - val_loss: 153.4243 - val_Si-sdr: 19.1664\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 151.47202\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 150.9031 - Si-sdr: 19.4910 - val_loss: 157.4737 - val_Si-sdr: 18.7437\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 151.47202\n",
      "Epoch 180/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 729ms/step - loss: 155.2471 - Si-sdr: 18.9507 - val_loss: 159.3701 - val_Si-sdr: 18.2663\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 151.47202\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 157.4455 - Si-sdr: 18.5917 - val_loss: 156.1230 - val_Si-sdr: 18.8511\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 151.47202\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 155.1693 - Si-sdr: 18.9950 - val_loss: 155.3969 - val_Si-sdr: 18.8599\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 151.47202\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 1s 596ms/step - loss: 153.4191 - Si-sdr: 19.1476 - val_loss: 149.8477 - val_Si-sdr: 19.6674\n",
      "\n",
      "Epoch 00183: val_loss improved from 151.47202 to 149.84772, saving model to ./CKPT\\CKP_ep_183__loss_149.84772_.h5\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 152.2370 - Si-sdr: 19.3061 - val_loss: 154.8158 - val_Si-sdr: 18.6971\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 149.84772\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 153.5115 - Si-sdr: 19.1408 - val_loss: 155.1000 - val_Si-sdr: 18.6739\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 149.84772\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 154.1151 - Si-sdr: 18.8889 - val_loss: 151.3551 - val_Si-sdr: 19.4263\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 149.84772\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 153.1131 - Si-sdr: 19.1728 - val_loss: 152.8495 - val_Si-sdr: 19.1803\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 149.84772\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 153.7153 - Si-sdr: 19.0989 - val_loss: 151.5444 - val_Si-sdr: 19.4770\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 149.84772\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 152.7824 - Si-sdr: 19.2517 - val_loss: 150.3700 - val_Si-sdr: 19.4985\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 149.84772\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 151.8499 - Si-sdr: 19.3448 - val_loss: 154.0590 - val_Si-sdr: 19.0179\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 149.84772\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 153.9463 - Si-sdr: 18.9762 - val_loss: 149.9895 - val_Si-sdr: 19.6354\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 149.84772\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 150.2407 - Si-sdr: 19.6417 - val_loss: 151.6412 - val_Si-sdr: 19.2532\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 149.84772\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 150.8357 - Si-sdr: 19.5710 - val_loss: 154.8118 - val_Si-sdr: 18.8249\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 149.84772\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 152.2160 - Si-sdr: 19.2717 - val_loss: 150.5706 - val_Si-sdr: 19.5194\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 149.84772\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 151.7256 - Si-sdr: 19.2728 - val_loss: 156.5472 - val_Si-sdr: 18.6097\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 149.84772\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 154.6449 - Si-sdr: 18.8988 - val_loss: 151.7992 - val_Si-sdr: 19.3178\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 149.84772\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 151.2549 - Si-sdr: 19.4425 - val_loss: 151.9572 - val_Si-sdr: 19.3624\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 149.84772\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 151.2289 - Si-sdr: 19.6130 - val_loss: 150.8418 - val_Si-sdr: 19.4905\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 149.84772\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 148.9645 - Si-sdr: 19.5879 - val_loss: 149.4372 - val_Si-sdr: 19.7346\n",
      "\n",
      "Epoch 00199: val_loss improved from 149.84772 to 149.43721, saving model to ./CKPT\\CKP_ep_199__loss_149.43721_.h5\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 150.2851 - Si-sdr: 19.6885 - val_loss: 154.5612 - val_Si-sdr: 18.9071\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 149.43721\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 153.5231 - Si-sdr: 19.2054 - val_loss: 151.0417 - val_Si-sdr: 19.4145\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 149.43721\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 155.4185 - Si-sdr: 18.7753 - val_loss: 157.7336 - val_Si-sdr: 18.3348\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 149.43721\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 153.8836 - Si-sdr: 18.9572 - val_loss: 155.7379 - val_Si-sdr: 18.7290\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 149.43721\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 157.9329 - Si-sdr: 18.3752 - val_loss: 152.6814 - val_Si-sdr: 19.3031\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 149.43721\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 155.2366 - Si-sdr: 18.7674 - val_loss: 149.9504 - val_Si-sdr: 19.5520\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 149.43721\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 146.2688 - Si-sdr: 20.0827 - val_loss: 152.3965 - val_Si-sdr: 19.2981\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 149.43721\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 155.3459 - Si-sdr: 18.7136 - val_loss: 153.7372 - val_Si-sdr: 19.1048\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 149.43721\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 154.0007 - Si-sdr: 18.8566 - val_loss: 152.3448 - val_Si-sdr: 19.2031\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 149.43721\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 147.5731 - Si-sdr: 19.8880 - val_loss: 150.0150 - val_Si-sdr: 19.6357\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 149.43721\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 147.7789 - Si-sdr: 19.9338 - val_loss: 148.6873 - val_Si-sdr: 19.8154\n",
      "\n",
      "Epoch 00210: val_loss improved from 149.43721 to 148.68726, saving model to ./CKPT\\CKP_ep_210__loss_148.68726_.h5\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 1s 764ms/step - loss: 146.5560 - Si-sdr: 20.1614 - val_loss: 149.1976 - val_Si-sdr: 19.6816\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 148.68726\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 152.5346 - Si-sdr: 19.1454 - val_loss: 149.1364 - val_Si-sdr: 19.6312\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 148.68726\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 149.4973 - Si-sdr: 19.5906 - val_loss: 150.8698 - val_Si-sdr: 19.2924\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 148.68726\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 148.9274 - Si-sdr: 19.7603 - val_loss: 147.3154 - val_Si-sdr: 19.9056\n",
      "\n",
      "Epoch 00214: val_loss improved from 148.68726 to 147.31540, saving model to ./CKPT\\CKP_ep_214__loss_147.31540_.h5\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 146.3699 - Si-sdr: 20.0115 - val_loss: 145.7569 - val_Si-sdr: 20.1598\n",
      "\n",
      "Epoch 00215: val_loss improved from 147.31540 to 145.75688, saving model to ./CKPT\\CKP_ep_215__loss_145.75688_.h5\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 147.7586 - Si-sdr: 19.8757 - val_loss: 148.7512 - val_Si-sdr: 19.7977\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 145.75688\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 147.7803 - Si-sdr: 19.8056 - val_loss: 146.6060 - val_Si-sdr: 20.0748\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 145.75688\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 149.2622 - Si-sdr: 19.7551 - val_loss: 146.5326 - val_Si-sdr: 20.1019\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 145.75688\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 147.4528 - Si-sdr: 19.8121 - val_loss: 147.8085 - val_Si-sdr: 19.9454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00219: val_loss did not improve from 145.75688\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 150.4275 - Si-sdr: 19.5352 - val_loss: 144.9190 - val_Si-sdr: 20.3836\n",
      "\n",
      "Epoch 00220: val_loss improved from 145.75688 to 144.91896, saving model to ./CKPT\\CKP_ep_220__loss_144.91896_.h5\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 144.5539 - Si-sdr: 20.4609 - val_loss: 143.6686 - val_Si-sdr: 20.5164\n",
      "\n",
      "Epoch 00221: val_loss improved from 144.91896 to 143.66861, saving model to ./CKPT\\CKP_ep_221__loss_143.66861_.h5\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 144.4215 - Si-sdr: 20.2803 - val_loss: 151.3765 - val_Si-sdr: 19.2733\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 143.66861\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 154.0011 - Si-sdr: 18.8763 - val_loss: 149.1919 - val_Si-sdr: 19.5616\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 143.66861\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 1s 723ms/step - loss: 148.4862 - Si-sdr: 19.8256 - val_loss: 148.4599 - val_Si-sdr: 19.6436\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 143.66861\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 148.1053 - Si-sdr: 19.9059 - val_loss: 150.1525 - val_Si-sdr: 19.5705\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 143.66861\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 151.8546 - Si-sdr: 19.3009 - val_loss: 151.8838 - val_Si-sdr: 19.2791\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 143.66861\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 150.5624 - Si-sdr: 19.4541 - val_loss: 148.3102 - val_Si-sdr: 19.7177\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 143.66861\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 151.1422 - Si-sdr: 19.2589 - val_loss: 150.2200 - val_Si-sdr: 19.2733\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 143.66861\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 149.3755 - Si-sdr: 19.4047 - val_loss: 148.9801 - val_Si-sdr: 19.6114\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 143.66861\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 148.6238 - Si-sdr: 19.6169 - val_loss: 148.2468 - val_Si-sdr: 19.7351\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 143.66861\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 147.5362 - Si-sdr: 19.8548 - val_loss: 149.7648 - val_Si-sdr: 19.4963\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 143.66861\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 145.8832 - Si-sdr: 20.0536 - val_loss: 151.1385 - val_Si-sdr: 19.4842\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 143.66861\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 147.0215 - Si-sdr: 19.9698 - val_loss: 144.9541 - val_Si-sdr: 20.3705\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 143.66861\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 146.3119 - Si-sdr: 20.2434 - val_loss: 147.4395 - val_Si-sdr: 20.0390\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 143.66861\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 146.8196 - Si-sdr: 20.1676 - val_loss: 149.4608 - val_Si-sdr: 19.5553\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 143.66861\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 148.5005 - Si-sdr: 19.6820 - val_loss: 150.7961 - val_Si-sdr: 19.5011\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 143.66861\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 150.0614 - Si-sdr: 19.5290 - val_loss: 148.2125 - val_Si-sdr: 19.6265\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 143.66861\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 142.2941 - Si-sdr: 20.6328 - val_loss: 146.2313 - val_Si-sdr: 20.0389\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 143.66861\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 147.3598 - Si-sdr: 19.8620 - val_loss: 148.8386 - val_Si-sdr: 19.6844\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 143.66861\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 1s 657ms/step - loss: 150.1940 - Si-sdr: 19.5172 - val_loss: 146.6664 - val_Si-sdr: 20.0630\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 143.66861\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 150.7229 - Si-sdr: 19.6015 - val_loss: 153.4465 - val_Si-sdr: 18.8256\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 143.66861\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 154.8416 - Si-sdr: 18.6450 - val_loss: 150.7781 - val_Si-sdr: 19.2366\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 143.66861\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 152.7428 - Si-sdr: 18.9565 - val_loss: 148.8907 - val_Si-sdr: 19.6617\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 143.66861\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 145.5510 - Si-sdr: 20.2042 - val_loss: 146.5015 - val_Si-sdr: 20.0895\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 143.66861\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 145.5314 - Si-sdr: 20.1510 - val_loss: 147.8964 - val_Si-sdr: 19.8093\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 143.66861\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 149.6350 - Si-sdr: 19.5514 - val_loss: 148.2938 - val_Si-sdr: 19.8288\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 143.66861\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 148.2659 - Si-sdr: 19.6662 - val_loss: 144.6972 - val_Si-sdr: 20.1422\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 143.66861\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 146.8532 - Si-sdr: 20.0285 - val_loss: 146.9675 - val_Si-sdr: 19.9007\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 143.66861\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 149.4497 - Si-sdr: 19.3845 - val_loss: 148.9859 - val_Si-sdr: 19.5459\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 143.66861\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 150.3282 - Si-sdr: 19.4083 - val_loss: 150.3875 - val_Si-sdr: 19.4033\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 143.66861\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 1s 613ms/step - loss: 151.4720 - Si-sdr: 19.3586 - val_loss: 153.6331 - val_Si-sdr: 18.9428\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 143.66861\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 151.6967 - Si-sdr: 19.1872 - val_loss: 150.1575 - val_Si-sdr: 19.4034\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 143.66861\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 147.9052 - Si-sdr: 19.7703 - val_loss: 154.0520 - val_Si-sdr: 18.7737\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 143.66861\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 150.6779 - Si-sdr: 19.3688 - val_loss: 148.3591 - val_Si-sdr: 19.7381\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 143.66861\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 145.3815 - Si-sdr: 20.1303 - val_loss: 146.0203 - val_Si-sdr: 20.0696\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 143.66861\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 147.0277 - Si-sdr: 20.1251 - val_loss: 150.0764 - val_Si-sdr: 19.4172\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 143.66861\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 147.0121 - Si-sdr: 19.8629 - val_loss: 146.9964 - val_Si-sdr: 19.8914\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 143.66861\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 148.0248 - Si-sdr: 19.7989 - val_loss: 146.3008 - val_Si-sdr: 19.9997\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 143.66861\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 148.5238 - Si-sdr: 19.6258 - val_loss: 149.0420 - val_Si-sdr: 19.5099\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 143.66861\n",
      "Epoch 260/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 617ms/step - loss: 147.2238 - Si-sdr: 19.9538 - val_loss: 146.4517 - val_Si-sdr: 19.8904\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 143.66861\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 147.0620 - Si-sdr: 19.8635 - val_loss: 144.2795 - val_Si-sdr: 20.4416\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 143.66861\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 144.5047 - Si-sdr: 20.2767 - val_loss: 142.7053 - val_Si-sdr: 20.5460\n",
      "\n",
      "Epoch 00262: val_loss improved from 143.66861 to 142.70529, saving model to ./CKPT\\CKP_ep_262__loss_142.70529_.h5\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 1s 606ms/step - loss: 145.1328 - Si-sdr: 20.2379 - val_loss: 147.5621 - val_Si-sdr: 19.8593\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 142.70529\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 146.2952 - Si-sdr: 20.1618 - val_loss: 146.3513 - val_Si-sdr: 19.9954\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 142.70529\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 145.8908 - Si-sdr: 20.2463 - val_loss: 145.2702 - val_Si-sdr: 20.1397\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 142.70529\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 144.4470 - Si-sdr: 20.2778 - val_loss: 143.6780 - val_Si-sdr: 20.4201\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 142.70529\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 143.7396 - Si-sdr: 20.4314 - val_loss: 151.5223 - val_Si-sdr: 19.0823\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 142.70529\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 149.8766 - Si-sdr: 19.2839 - val_loss: 151.9580 - val_Si-sdr: 18.9695\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 142.70529\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 149.6256 - Si-sdr: 19.4558 - val_loss: 149.8652 - val_Si-sdr: 19.3928\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 142.70529\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 148.0653 - Si-sdr: 19.7237 - val_loss: 150.0133 - val_Si-sdr: 19.4964\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 142.70529\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 147.4475 - Si-sdr: 19.8071 - val_loss: 148.6267 - val_Si-sdr: 19.6359\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 142.70529\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 149.0689 - Si-sdr: 19.6936 - val_loss: 148.2833 - val_Si-sdr: 19.7407\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 142.70529\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 147.5685 - Si-sdr: 19.8531 - val_loss: 151.2575 - val_Si-sdr: 19.2537\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 142.70529\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 156.2034 - Si-sdr: 18.5657 - val_loss: 150.3380 - val_Si-sdr: 19.4883\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 142.70529\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 150.1082 - Si-sdr: 19.5522 - val_loss: 148.1054 - val_Si-sdr: 19.7570\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 142.70529\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 148.7690 - Si-sdr: 19.6175 - val_loss: 146.1024 - val_Si-sdr: 20.0725\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 142.70529\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 145.3339 - Si-sdr: 20.1550 - val_loss: 154.7399 - val_Si-sdr: 18.5561\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 142.70529\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 153.1665 - Si-sdr: 18.8541 - val_loss: 157.7091 - val_Si-sdr: 18.2340\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 142.70529\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 153.4224 - Si-sdr: 18.8712 - val_loss: 144.9104 - val_Si-sdr: 20.1479\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 142.70529\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 147.3001 - Si-sdr: 19.8336 - val_loss: 147.9829 - val_Si-sdr: 19.7702\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 142.70529\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 149.3348 - Si-sdr: 19.4199 - val_loss: 153.6665 - val_Si-sdr: 18.7901\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 142.70529\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 151.5237 - Si-sdr: 19.2278 - val_loss: 148.1419 - val_Si-sdr: 19.5913\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 142.70529\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 149.7747 - Si-sdr: 19.4180 - val_loss: 149.4080 - val_Si-sdr: 19.7174\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 142.70529\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 149.9474 - Si-sdr: 19.7269 - val_loss: 149.0102 - val_Si-sdr: 19.5318\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 142.70529\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 149.1194 - Si-sdr: 19.7142 - val_loss: 146.9117 - val_Si-sdr: 19.9014\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 142.70529\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 147.5288 - Si-sdr: 19.8768 - val_loss: 146.4102 - val_Si-sdr: 20.1713\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 142.70529\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 145.7158 - Si-sdr: 20.2024 - val_loss: 146.6838 - val_Si-sdr: 19.9756\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 142.70529\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 148.5207 - Si-sdr: 19.7120 - val_loss: 145.8140 - val_Si-sdr: 19.9890\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 142.70529\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 1s 658ms/step - loss: 148.8314 - Si-sdr: 19.6185 - val_loss: 148.7067 - val_Si-sdr: 19.5872\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 142.70529\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 145.9358 - Si-sdr: 20.0604 - val_loss: 147.3467 - val_Si-sdr: 19.8880\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 142.70529\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 147.9932 - Si-sdr: 19.6647 - val_loss: 145.7383 - val_Si-sdr: 20.2466\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 142.70529\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 145.4637 - Si-sdr: 20.2229 - val_loss: 146.0560 - val_Si-sdr: 20.1212\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 142.70529\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 146.5511 - Si-sdr: 19.8863 - val_loss: 146.3029 - val_Si-sdr: 19.9615\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 142.70529\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 145.0094 - Si-sdr: 20.0376 - val_loss: 147.2218 - val_Si-sdr: 19.7534\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 142.70529\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 148.4973 - Si-sdr: 19.6520 - val_loss: 149.6497 - val_Si-sdr: 19.3647\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 142.70529\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 148.5349 - Si-sdr: 19.6427 - val_loss: 145.7368 - val_Si-sdr: 20.0994\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 142.70529\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 147.4378 - Si-sdr: 19.7585 - val_loss: 146.6905 - val_Si-sdr: 19.9932\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 142.70529\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 144.5197 - Si-sdr: 20.2322 - val_loss: 145.7693 - val_Si-sdr: 20.1112\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 142.70529\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 147.7059 - Si-sdr: 19.8613 - val_loss: 147.2968 - val_Si-sdr: 19.8804\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 142.70529\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 1s 808ms/step - loss: 148.9585 - Si-sdr: 19.6269 - val_loss: 150.3796 - val_Si-sdr: 19.4817\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 142.70529\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-gregory",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clear-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_2 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_262__loss_142.70529_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        gumbelmax = vq_vae.gumbel(encode).numpy()\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        gumbelmax = vq_vae.gumbel(encode).numpy()\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(gumbelmax).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_3 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_262__loss_142.70529_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.14930329 -0.05488385 -0.03165275  0.02511186]\n",
      "  [-0.04538564 -0.02340828 -0.13736942  0.03185076]\n",
      "  [-0.18286747 -0.01420305  0.02583414  0.20739384]\n",
      "  [-0.15128048 -0.15295134  0.00826486 -0.03651741]\n",
      "  [-0.19406578 -0.14820886 -0.24136184  0.01954714]\n",
      "  [-0.11608941  0.05977409 -0.05805521  0.08262399]\n",
      "  [-0.10437049  0.00281959  0.00744648  0.06939161]\n",
      "  [-0.32889026 -0.11109954 -0.06513101  0.02347144]\n",
      "  [-0.10914627  0.04005037 -0.11194003  0.34588984]\n",
      "  [-0.55321944 -0.24241613 -0.17234027  0.2255739 ]]\n",
      "\n",
      " [[-0.12665638 -0.02560038 -0.07357763 -0.03386865]\n",
      "  [-0.00296582  0.05382628 -0.02431939 -0.00124017]\n",
      "  [-0.152773    0.01729362 -0.02049777  0.00555693]\n",
      "  [-0.01630396 -0.0508009   0.00338947 -0.00536075]\n",
      "  [-0.11710438 -0.0411608   0.01888775  0.10048383]\n",
      "  [-0.1000828   0.06160894 -0.04584875  0.14583868]\n",
      "  [-0.29426795 -0.04820506 -0.1607864   0.03541403]\n",
      "  [-0.14156486  0.07812893 -0.13114211  0.12421213]\n",
      "  [-0.20461929  0.08133916 -0.08211927  0.03100684]\n",
      "  [-0.00948744 -0.11452891  0.03942408 -0.1281477 ]]]\n",
      "(2, 10, 4)\n",
      "(2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
