{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_1 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 5s 2s/step - loss: 633.6434 - Si-sdr: -220.5407 - val_loss: 631.5074 - val_Si-sdr: -271.8297\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 195.17917\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 630.0446 - Si-sdr: -233.7352 - val_loss: 627.9093 - val_Si-sdr: -277.1785\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 195.17917\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 628.0853 - Si-sdr: -226.5460 - val_loss: 628.2919 - val_Si-sdr: -224.8035\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 195.17917\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 628.1738 - Si-sdr: -243.3682 - val_loss: 627.8430 - val_Si-sdr: -218.8198\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 195.17917\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 627.7383 - Si-sdr: -238.2677 - val_loss: 627.4503 - val_Si-sdr: -243.9063\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 195.17917\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 627.4158 - Si-sdr: -232.7974 - val_loss: 627.2935 - val_Si-sdr: -218.3193\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 195.17917\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 627.4130 - Si-sdr: -203.4842 - val_loss: 627.2692 - val_Si-sdr: -210.6172\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 195.17917\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 627.2983 - Si-sdr: -263.3452 - val_loss: 627.2786 - val_Si-sdr: -240.5372\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 195.17917\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 627.2828 - Si-sdr: -237.7106 - val_loss: 627.2220 - val_Si-sdr: -235.3653\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 195.17917\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 627.2344 - Si-sdr: -198.6513 - val_loss: 627.2791 - val_Si-sdr: -238.3445\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 195.17917\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 627.2850 - Si-sdr: -236.0925 - val_loss: 627.2099 - val_Si-sdr: -197.8125\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 195.17917\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 627.2422 - Si-sdr: -234.7274 - val_loss: 627.2089 - val_Si-sdr: -215.8006\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 195.17917\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 627.1718 - Si-sdr: -190.5273 - val_loss: 627.2165 - val_Si-sdr: -237.2900\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 195.17917\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 627.1985 - Si-sdr: -213.7334 - val_loss: 627.2169 - val_Si-sdr: -261.9888\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 195.17917\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 1s 776ms/step - loss: 627.1813 - Si-sdr: -191.2257 - val_loss: 627.1730 - val_Si-sdr: -218.7683\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 195.17917\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 627.1726 - Si-sdr: -191.3502 - val_loss: 627.2058 - val_Si-sdr: -238.0764\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 195.17917\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 1s 756ms/step - loss: 627.2256 - Si-sdr: -230.9361 - val_loss: 627.1539 - val_Si-sdr: -177.7588\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 195.17917\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 1s 664ms/step - loss: 627.1718 - Si-sdr: -186.2110 - val_loss: 627.1515 - val_Si-sdr: -174.2833\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 195.17917\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 1s 778ms/step - loss: 627.1636 - Si-sdr: -197.1904 - val_loss: 627.1259 - val_Si-sdr: -160.3945\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 195.17917\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 627.1639 - Si-sdr: -190.0477 - val_loss: 627.1190 - val_Si-sdr: -155.9424\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 195.17917\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 627.1100 - Si-sdr: -158.7117 - val_loss: 627.0524 - val_Si-sdr: -138.0680\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 195.17917\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 1s 752ms/step - loss: 627.0067 - Si-sdr: -129.1279 - val_loss: 626.9007 - val_Si-sdr: -119.4595\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 195.17917\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 626.7781 - Si-sdr: -109.0435 - val_loss: 626.3710 - val_Si-sdr: -92.8852\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 195.17917\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 626.0264 - Si-sdr: -86.7375 - val_loss: 624.6011 - val_Si-sdr: -68.9902\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 195.17917\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 623.8834 - Si-sdr: -66.5499 - val_loss: 619.5770 - val_Si-sdr: -56.9829\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 195.17917\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 617.8491 - Si-sdr: -56.6544 - val_loss: 609.7491 - val_Si-sdr: -53.5775\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 195.17917\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 605.9913 - Si-sdr: -52.9653 - val_loss: 596.9354 - val_Si-sdr: -49.7914\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 195.17917\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 593.0099 - Si-sdr: -47.5983 - val_loss: 588.6535 - val_Si-sdr: -47.5387\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 195.17917\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 1s 599ms/step - loss: 585.3380 - Si-sdr: -45.7776 - val_loss: 577.6317 - val_Si-sdr: -42.5899\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 195.17917\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 573.6407 - Si-sdr: -41.2524 - val_loss: 566.4425 - val_Si-sdr: -38.9023\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 195.17917\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 561.9274 - Si-sdr: -37.4915 - val_loss: 545.6960 - val_Si-sdr: -32.2067\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 195.17917\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 540.7829 - Si-sdr: -30.3906 - val_loss: 522.7748 - val_Si-sdr: -24.8697\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 195.17917\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 1s 586ms/step - loss: 514.1266 - Si-sdr: -22.4362 - val_loss: 487.6483 - val_Si-sdr: -19.0633\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 195.17917\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 476.2463 - Si-sdr: -17.9969 - val_loss: 447.4348 - val_Si-sdr: -15.1923\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 195.17917\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 435.1169 - Si-sdr: -13.7261 - val_loss: 407.3806 - val_Si-sdr: -10.3181\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 195.17917\n",
      "Epoch 36/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 709ms/step - loss: 403.5171 - Si-sdr: -9.5995 - val_loss: 386.2684 - val_Si-sdr: -7.7872\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 195.17917\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 380.5804 - Si-sdr: -7.0280 - val_loss: 364.3471 - val_Si-sdr: -5.0605\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 195.17917\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 361.8204 - Si-sdr: -4.6354 - val_loss: 352.9729 - val_Si-sdr: -3.8971\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 195.17917\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 353.5217 - Si-sdr: -3.9525 - val_loss: 348.6078 - val_Si-sdr: -3.4933\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 195.17917\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 340.6746 - Si-sdr: -2.1577 - val_loss: 335.9443 - val_Si-sdr: -1.6920\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 195.17917\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 1s 586ms/step - loss: 327.6705 - Si-sdr: -0.7973 - val_loss: 326.1641 - val_Si-sdr: -1.1514\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 195.17917\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 322.5411 - Si-sdr: -0.7830 - val_loss: 311.2930 - val_Si-sdr: 0.4094\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 195.17917\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 308.4051 - Si-sdr: 0.6042 - val_loss: 297.6673 - val_Si-sdr: 1.9474\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 195.17917\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 294.4622 - Si-sdr: 2.2530 - val_loss: 288.8670 - val_Si-sdr: 2.7038\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 195.17917\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 285.5450 - Si-sdr: 3.2523 - val_loss: 277.9379 - val_Si-sdr: 4.0543\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 195.17917\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 277.4567 - Si-sdr: 4.1093 - val_loss: 264.6551 - val_Si-sdr: 5.5475\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 195.17917\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 266.5580 - Si-sdr: 5.3597 - val_loss: 257.1528 - val_Si-sdr: 6.3305\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 195.17917\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 256.7792 - Si-sdr: 6.3280 - val_loss: 252.2909 - val_Si-sdr: 6.8770\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 195.17917\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 249.5350 - Si-sdr: 7.2210 - val_loss: 247.0685 - val_Si-sdr: 7.5174\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 195.17917\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 246.1312 - Si-sdr: 7.6926 - val_loss: 241.1621 - val_Si-sdr: 8.2432\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 195.17917\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 240.8431 - Si-sdr: 8.2883 - val_loss: 238.7005 - val_Si-sdr: 8.5255\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 195.17917\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 237.7288 - Si-sdr: 8.6852 - val_loss: 232.1540 - val_Si-sdr: 9.2965\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 195.17917\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 233.4956 - Si-sdr: 9.1920 - val_loss: 229.4006 - val_Si-sdr: 9.5722\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 195.17917\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 228.0050 - Si-sdr: 9.8636 - val_loss: 225.2629 - val_Si-sdr: 9.9885\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 195.17917\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 1s 666ms/step - loss: 226.5680 - Si-sdr: 9.7356 - val_loss: 222.4498 - val_Si-sdr: 10.3395\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 195.17917\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 221.3261 - Si-sdr: 10.4904 - val_loss: 218.9736 - val_Si-sdr: 10.7066\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 195.17917\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 219.4245 - Si-sdr: 10.6962 - val_loss: 216.7529 - val_Si-sdr: 10.9263\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 195.17917\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 217.5912 - Si-sdr: 10.8764 - val_loss: 214.7833 - val_Si-sdr: 11.2462\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 195.17917\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 214.6799 - Si-sdr: 11.2778 - val_loss: 213.1144 - val_Si-sdr: 11.3551\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 195.17917\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 212.5211 - Si-sdr: 11.5081 - val_loss: 211.1438 - val_Si-sdr: 11.6767\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 195.17917\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 210.3059 - Si-sdr: 11.7769 - val_loss: 210.8275 - val_Si-sdr: 11.8029\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 195.17917\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 210.1012 - Si-sdr: 11.7726 - val_loss: 207.9139 - val_Si-sdr: 12.1169\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 195.17917\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 207.8062 - Si-sdr: 12.1262 - val_loss: 205.6488 - val_Si-sdr: 12.3241\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 195.17917\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 206.0625 - Si-sdr: 12.2435 - val_loss: 204.3405 - val_Si-sdr: 12.4719\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 195.17917\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 1s 704ms/step - loss: 204.6293 - Si-sdr: 12.3959 - val_loss: 203.6234 - val_Si-sdr: 12.4925\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 195.17917\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 201.3556 - Si-sdr: 12.8928 - val_loss: 200.7445 - val_Si-sdr: 12.8487\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 195.17917\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 199.3044 - Si-sdr: 13.0100 - val_loss: 199.1080 - val_Si-sdr: 13.0522\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 195.17917\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 197.1423 - Si-sdr: 13.3519 - val_loss: 196.1758 - val_Si-sdr: 13.3399\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 195.17917\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 196.5272 - Si-sdr: 13.3245 - val_loss: 196.1247 - val_Si-sdr: 13.4055\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 195.17917\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 197.2190 - Si-sdr: 13.2302 - val_loss: 193.0811 - val_Si-sdr: 13.6982\n",
      "\n",
      "Epoch 00070: val_loss improved from 195.17917 to 193.08112, saving model to ./CKPT\\CKP_ep_70__loss_193.08112_.h5\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 194.5425 - Si-sdr: 13.5384 - val_loss: 192.7530 - val_Si-sdr: 13.7674\n",
      "\n",
      "Epoch 00071: val_loss improved from 193.08112 to 192.75301, saving model to ./CKPT\\CKP_ep_71__loss_192.75301_.h5\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 192.9008 - Si-sdr: 13.7806 - val_loss: 192.0782 - val_Si-sdr: 13.8606\n",
      "\n",
      "Epoch 00072: val_loss improved from 192.75301 to 192.07825, saving model to ./CKPT\\CKP_ep_72__loss_192.07825_.h5\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 190.7701 - Si-sdr: 13.9256 - val_loss: 189.0344 - val_Si-sdr: 14.1923\n",
      "\n",
      "Epoch 00073: val_loss improved from 192.07825 to 189.03439, saving model to ./CKPT\\CKP_ep_73__loss_189.03439_.h5\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 188.8091 - Si-sdr: 14.3296 - val_loss: 188.7973 - val_Si-sdr: 14.3195\n",
      "\n",
      "Epoch 00074: val_loss improved from 189.03439 to 188.79727, saving model to ./CKPT\\CKP_ep_74__loss_188.79727_.h5\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 1s 640ms/step - loss: 189.2231 - Si-sdr: 14.3290 - val_loss: 186.3569 - val_Si-sdr: 14.6737\n",
      "\n",
      "Epoch 00075: val_loss improved from 188.79727 to 186.35687, saving model to ./CKPT\\CKP_ep_75__loss_186.35687_.h5\n",
      "Epoch 76/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 740ms/step - loss: 186.8840 - Si-sdr: 14.4785 - val_loss: 186.7060 - val_Si-sdr: 14.5808\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 186.35687\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 187.2864 - Si-sdr: 14.5150 - val_loss: 184.9840 - val_Si-sdr: 14.7916\n",
      "\n",
      "Epoch 00077: val_loss improved from 186.35687 to 184.98401, saving model to ./CKPT\\CKP_ep_77__loss_184.98401_.h5\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 184.5532 - Si-sdr: 14.8324 - val_loss: 181.1822 - val_Si-sdr: 15.2458\n",
      "\n",
      "Epoch 00078: val_loss improved from 184.98401 to 181.18216, saving model to ./CKPT\\CKP_ep_78__loss_181.18216_.h5\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 183.4855 - Si-sdr: 14.9187 - val_loss: 180.9179 - val_Si-sdr: 15.3145\n",
      "\n",
      "Epoch 00079: val_loss improved from 181.18216 to 180.91794, saving model to ./CKPT\\CKP_ep_79__loss_180.91794_.h5\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 1s 685ms/step - loss: 181.5177 - Si-sdr: 15.2126 - val_loss: 181.8567 - val_Si-sdr: 15.1555\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 180.91794\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 1s 575ms/step - loss: 181.8965 - Si-sdr: 15.1285 - val_loss: 181.2556 - val_Si-sdr: 15.2352\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 180.91794\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 181.3654 - Si-sdr: 15.2365 - val_loss: 178.9953 - val_Si-sdr: 15.5571\n",
      "\n",
      "Epoch 00082: val_loss improved from 180.91794 to 178.99530, saving model to ./CKPT\\CKP_ep_82__loss_178.99530_.h5\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 177.6586 - Si-sdr: 15.7470 - val_loss: 177.1517 - val_Si-sdr: 15.7804\n",
      "\n",
      "Epoch 00083: val_loss improved from 178.99530 to 177.15173, saving model to ./CKPT\\CKP_ep_83__loss_177.15173_.h5\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 177.6411 - Si-sdr: 15.7153 - val_loss: 177.3513 - val_Si-sdr: 15.7732\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 177.15173\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 176.2948 - Si-sdr: 15.9127 - val_loss: 176.0790 - val_Si-sdr: 15.8933\n",
      "\n",
      "Epoch 00085: val_loss improved from 177.15173 to 176.07901, saving model to ./CKPT\\CKP_ep_85__loss_176.07901_.h5\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 1s 599ms/step - loss: 176.0435 - Si-sdr: 15.9568 - val_loss: 176.5117 - val_Si-sdr: 15.8435\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 176.07901\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 175.1492 - Si-sdr: 16.0615 - val_loss: 174.9055 - val_Si-sdr: 16.0923\n",
      "\n",
      "Epoch 00087: val_loss improved from 176.07901 to 174.90546, saving model to ./CKPT\\CKP_ep_87__loss_174.90546_.h5\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 175.5194 - Si-sdr: 15.9451 - val_loss: 173.8398 - val_Si-sdr: 16.1779\n",
      "\n",
      "Epoch 00088: val_loss improved from 174.90546 to 173.83983, saving model to ./CKPT\\CKP_ep_88__loss_173.83983_.h5\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 1s 786ms/step - loss: 173.8279 - Si-sdr: 16.2789 - val_loss: 173.3805 - val_Si-sdr: 16.3214\n",
      "\n",
      "Epoch 00089: val_loss improved from 173.83983 to 173.38054, saving model to ./CKPT\\CKP_ep_89__loss_173.38054_.h5\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 171.3892 - Si-sdr: 16.4937 - val_loss: 173.6801 - val_Si-sdr: 16.1918\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 173.38054\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 1s 812ms/step - loss: 173.6317 - Si-sdr: 16.0130 - val_loss: 176.2157 - val_Si-sdr: 15.7663\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 173.38054\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 173.0006 - Si-sdr: 16.3722 - val_loss: 170.6313 - val_Si-sdr: 16.7355\n",
      "\n",
      "Epoch 00092: val_loss improved from 173.38054 to 170.63129, saving model to ./CKPT\\CKP_ep_92__loss_170.63129_.h5\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 172.0205 - Si-sdr: 16.5033 - val_loss: 174.2515 - val_Si-sdr: 16.2144\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 170.63129\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 174.6178 - Si-sdr: 16.1450 - val_loss: 172.2663 - val_Si-sdr: 16.5297\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 170.63129\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 171.8402 - Si-sdr: 16.4772 - val_loss: 176.1559 - val_Si-sdr: 15.9305\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 170.63129\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 1s 730ms/step - loss: 178.9204 - Si-sdr: 15.6643 - val_loss: 172.5417 - val_Si-sdr: 16.3458\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 170.63129\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 1s 773ms/step - loss: 175.4640 - Si-sdr: 15.9693 - val_loss: 168.6415 - val_Si-sdr: 17.0099\n",
      "\n",
      "Epoch 00097: val_loss improved from 170.63129 to 168.64154, saving model to ./CKPT\\CKP_ep_97__loss_168.64154_.h5\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 171.6253 - Si-sdr: 16.5318 - val_loss: 172.3029 - val_Si-sdr: 16.3992\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 168.64154\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 1s 741ms/step - loss: 172.9801 - Si-sdr: 16.3217 - val_loss: 173.7915 - val_Si-sdr: 16.1258\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 168.64154\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 1s 743ms/step - loss: 173.6419 - Si-sdr: 16.1670 - val_loss: 168.9621 - val_Si-sdr: 16.8230\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 168.64154\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 170.4392 - Si-sdr: 16.6675 - val_loss: 171.4418 - val_Si-sdr: 16.4643\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 168.64154\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 169.2334 - Si-sdr: 16.7452 - val_loss: 168.7187 - val_Si-sdr: 16.8845\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 168.64154\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 1s 810ms/step - loss: 169.3083 - Si-sdr: 16.8104 - val_loss: 168.9000 - val_Si-sdr: 16.8026\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 168.64154\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 167.9711 - Si-sdr: 16.9304 - val_loss: 167.6640 - val_Si-sdr: 16.9932\n",
      "\n",
      "Epoch 00104: val_loss improved from 168.64154 to 167.66399, saving model to ./CKPT\\CKP_ep_104__loss_167.66399_.h5\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 166.6910 - Si-sdr: 17.2061 - val_loss: 166.3872 - val_Si-sdr: 17.1504\n",
      "\n",
      "Epoch 00105: val_loss improved from 167.66399 to 166.38718, saving model to ./CKPT\\CKP_ep_105__loss_166.38718_.h5\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 1s 767ms/step - loss: 169.1704 - Si-sdr: 16.7933 - val_loss: 168.3666 - val_Si-sdr: 16.8687\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 166.38718\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 1s 722ms/step - loss: 167.4722 - Si-sdr: 17.1153 - val_loss: 169.1218 - val_Si-sdr: 16.7968\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 166.38718\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 168.1255 - Si-sdr: 16.8903 - val_loss: 167.4055 - val_Si-sdr: 17.0039\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 166.38718\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 166.3847 - Si-sdr: 17.2306 - val_loss: 166.1053 - val_Si-sdr: 17.2034\n",
      "\n",
      "Epoch 00109: val_loss improved from 166.38718 to 166.10529, saving model to ./CKPT\\CKP_ep_109__loss_166.10529_.h5\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 165.9448 - Si-sdr: 17.2851 - val_loss: 166.6542 - val_Si-sdr: 17.1514\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 166.10529\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 1s 728ms/step - loss: 168.5628 - Si-sdr: 16.8477 - val_loss: 166.8873 - val_Si-sdr: 17.1678\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 166.10529\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 166.1677 - Si-sdr: 17.2007 - val_loss: 166.0217 - val_Si-sdr: 17.2112\n",
      "\n",
      "Epoch 00112: val_loss improved from 166.10529 to 166.02167, saving model to ./CKPT\\CKP_ep_112__loss_166.02167_.h5\n",
      "Epoch 113/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 654ms/step - loss: 168.1589 - Si-sdr: 16.9036 - val_loss: 166.6141 - val_Si-sdr: 17.2144\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 166.02167\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 164.0612 - Si-sdr: 17.5614 - val_loss: 163.9608 - val_Si-sdr: 17.5402\n",
      "\n",
      "Epoch 00114: val_loss improved from 166.02167 to 163.96085, saving model to ./CKPT\\CKP_ep_114__loss_163.96085_.h5\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 165.5387 - Si-sdr: 17.3323 - val_loss: 167.1889 - val_Si-sdr: 17.1178\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 163.96085\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 166.1367 - Si-sdr: 17.1806 - val_loss: 165.8854 - val_Si-sdr: 17.2199\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 163.96085\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 1s 707ms/step - loss: 164.9364 - Si-sdr: 17.3912 - val_loss: 164.4288 - val_Si-sdr: 17.5114\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 163.96085\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 165.4566 - Si-sdr: 17.2640 - val_loss: 162.7079 - val_Si-sdr: 17.7532\n",
      "\n",
      "Epoch 00118: val_loss improved from 163.96085 to 162.70789, saving model to ./CKPT\\CKP_ep_118__loss_162.70789_.h5\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 1s 574ms/step - loss: 163.0087 - Si-sdr: 17.6355 - val_loss: 164.1239 - val_Si-sdr: 17.5378\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 162.70789\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 164.4991 - Si-sdr: 17.5850 - val_loss: 163.2856 - val_Si-sdr: 17.6511\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 162.70789\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 163.1960 - Si-sdr: 17.7703 - val_loss: 163.8159 - val_Si-sdr: 17.5071\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 162.70789\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 162.3125 - Si-sdr: 17.7814 - val_loss: 164.1301 - val_Si-sdr: 17.4361\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 162.70789\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 162.9817 - Si-sdr: 17.6868 - val_loss: 163.4901 - val_Si-sdr: 17.5327\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 162.70789\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 163.7970 - Si-sdr: 17.6005 - val_loss: 162.8090 - val_Si-sdr: 17.5962\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 162.70789\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 162.3301 - Si-sdr: 17.8224 - val_loss: 162.4978 - val_Si-sdr: 17.8244\n",
      "\n",
      "Epoch 00125: val_loss improved from 162.70789 to 162.49783, saving model to ./CKPT\\CKP_ep_125__loss_162.49783_.h5\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 161.5829 - Si-sdr: 17.9160 - val_loss: 161.9579 - val_Si-sdr: 17.8062\n",
      "\n",
      "Epoch 00126: val_loss improved from 162.49783 to 161.95792, saving model to ./CKPT\\CKP_ep_126__loss_161.95792_.h5\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 1s 569ms/step - loss: 162.3917 - Si-sdr: 17.7722 - val_loss: 160.7437 - val_Si-sdr: 18.0637\n",
      "\n",
      "Epoch 00127: val_loss improved from 161.95792 to 160.74371, saving model to ./CKPT\\CKP_ep_127__loss_160.74371_.h5\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 161.9606 - Si-sdr: 17.8749 - val_loss: 162.5129 - val_Si-sdr: 17.7740\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 160.74371\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 163.1647 - Si-sdr: 17.6377 - val_loss: 159.9863 - val_Si-sdr: 18.1187\n",
      "\n",
      "Epoch 00129: val_loss improved from 160.74371 to 159.98625, saving model to ./CKPT\\CKP_ep_129__loss_159.98625_.h5\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 1s 584ms/step - loss: 160.4810 - Si-sdr: 18.1023 - val_loss: 161.7302 - val_Si-sdr: 17.8091\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 159.98625\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 160.8908 - Si-sdr: 17.9862 - val_loss: 161.7957 - val_Si-sdr: 17.8147\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 159.98625\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 1s 589ms/step - loss: 160.3792 - Si-sdr: 18.0530 - val_loss: 159.7904 - val_Si-sdr: 18.1192\n",
      "\n",
      "Epoch 00132: val_loss improved from 159.98625 to 159.79044, saving model to ./CKPT\\CKP_ep_132__loss_159.79044_.h5\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 160.3395 - Si-sdr: 18.0447 - val_loss: 158.9746 - val_Si-sdr: 18.2159\n",
      "\n",
      "Epoch 00133: val_loss improved from 159.79044 to 158.97461, saving model to ./CKPT\\CKP_ep_133__loss_158.97461_.h5\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 1s 604ms/step - loss: 160.3214 - Si-sdr: 18.0944 - val_loss: 158.7618 - val_Si-sdr: 18.3257\n",
      "\n",
      "Epoch 00134: val_loss improved from 158.97461 to 158.76181, saving model to ./CKPT\\CKP_ep_134__loss_158.76181_.h5\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 159.8933 - Si-sdr: 18.1460 - val_loss: 160.6328 - val_Si-sdr: 17.9968\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 158.76181\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 159.8565 - Si-sdr: 18.1696 - val_loss: 160.1318 - val_Si-sdr: 18.0844\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 158.76181\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 1s 580ms/step - loss: 159.4994 - Si-sdr: 18.2228 - val_loss: 160.2820 - val_Si-sdr: 18.1346\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 158.76181\n",
      "Epoch 138/300\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 161.3360 - Si-sdr: 17.9418 - val_loss: 160.2320 - val_Si-sdr: 18.1308\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 158.76181\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 1s 603ms/step - loss: 159.0202 - Si-sdr: 18.3041 - val_loss: 160.4632 - val_Si-sdr: 18.0332\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 158.76181\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 159.2670 - Si-sdr: 18.1822 - val_loss: 159.3276 - val_Si-sdr: 18.2768\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 158.76181\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 160.6830 - Si-sdr: 18.0642 - val_loss: 157.8994 - val_Si-sdr: 18.3415\n",
      "\n",
      "Epoch 00141: val_loss improved from 158.76181 to 157.89938, saving model to ./CKPT\\CKP_ep_141__loss_157.89938_.h5\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 160.8250 - Si-sdr: 18.0697 - val_loss: 159.5019 - val_Si-sdr: 18.1354\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 157.89938\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 159.5685 - Si-sdr: 18.1539 - val_loss: 163.1315 - val_Si-sdr: 17.6054\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 157.89938\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 162.4995 - Si-sdr: 17.6824 - val_loss: 159.6067 - val_Si-sdr: 18.0568\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 157.89938\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 1s 664ms/step - loss: 158.7370 - Si-sdr: 18.2176 - val_loss: 159.6837 - val_Si-sdr: 18.1875\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 157.89938\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 158.8394 - Si-sdr: 18.2455 - val_loss: 161.0106 - val_Si-sdr: 17.8527\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 157.89938\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 161.2199 - Si-sdr: 17.9090 - val_loss: 159.8808 - val_Si-sdr: 18.0919\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 157.89938\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 157.2722 - Si-sdr: 18.4221 - val_loss: 159.7652 - val_Si-sdr: 18.0554\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 157.89938\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 1s 568ms/step - loss: 160.3411 - Si-sdr: 18.0034 - val_loss: 157.2442 - val_Si-sdr: 18.5164\n",
      "\n",
      "Epoch 00149: val_loss improved from 157.89938 to 157.24419, saving model to ./CKPT\\CKP_ep_149__loss_157.24419_.h5\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 159.0623 - Si-sdr: 18.1557 - val_loss: 161.2426 - val_Si-sdr: 17.9314\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 157.24419\n",
      "Epoch 151/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 688ms/step - loss: 163.3508 - Si-sdr: 17.6601 - val_loss: 163.4140 - val_Si-sdr: 17.6630\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 157.24419\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 160.3772 - Si-sdr: 18.0884 - val_loss: 160.3331 - val_Si-sdr: 18.0709\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 157.24419\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 158.8344 - Si-sdr: 18.2909 - val_loss: 159.2824 - val_Si-sdr: 18.2248\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 157.24419\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 157.7155 - Si-sdr: 18.4700 - val_loss: 157.5298 - val_Si-sdr: 18.5226\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 157.24419\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 1s 556ms/step - loss: 158.0531 - Si-sdr: 18.4363 - val_loss: 158.5626 - val_Si-sdr: 18.3088\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 157.24419\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 158.1999 - Si-sdr: 18.3309 - val_loss: 158.5198 - val_Si-sdr: 18.2742\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 157.24419\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 157.0191 - Si-sdr: 18.5490 - val_loss: 158.0660 - val_Si-sdr: 18.3971\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 157.24419\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 1s 627ms/step - loss: 158.7026 - Si-sdr: 18.3514 - val_loss: 157.4867 - val_Si-sdr: 18.5207\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 157.24419\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 157.5948 - Si-sdr: 18.4812 - val_loss: 156.7004 - val_Si-sdr: 18.6901\n",
      "\n",
      "Epoch 00159: val_loss improved from 157.24419 to 156.70038, saving model to ./CKPT\\CKP_ep_159__loss_156.70038_.h5\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 1s 598ms/step - loss: 159.0528 - Si-sdr: 18.3755 - val_loss: 156.3185 - val_Si-sdr: 18.6329\n",
      "\n",
      "Epoch 00160: val_loss improved from 156.70038 to 156.31854, saving model to ./CKPT\\CKP_ep_160__loss_156.31854_.h5\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 156.4470 - Si-sdr: 18.5638 - val_loss: 156.7682 - val_Si-sdr: 18.4887\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 156.31854\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 1s 554ms/step - loss: 155.4918 - Si-sdr: 18.7133 - val_loss: 156.8708 - val_Si-sdr: 18.5747\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 156.31854\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 156.1379 - Si-sdr: 18.6969 - val_loss: 155.5891 - val_Si-sdr: 18.6380\n",
      "\n",
      "Epoch 00163: val_loss improved from 156.31854 to 155.58910, saving model to ./CKPT\\CKP_ep_163__loss_155.58910_.h5\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 154.2958 - Si-sdr: 19.0081 - val_loss: 154.7345 - val_Si-sdr: 18.8650\n",
      "\n",
      "Epoch 00164: val_loss improved from 155.58910 to 154.73450, saving model to ./CKPT\\CKP_ep_164__loss_154.73450_.h5\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 155.3150 - Si-sdr: 18.7139 - val_loss: 155.4403 - val_Si-sdr: 18.7722\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 154.73450\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 155.2309 - Si-sdr: 18.7652 - val_loss: 155.0604 - val_Si-sdr: 18.7913\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 154.73450\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 154.2426 - Si-sdr: 18.8670 - val_loss: 157.9467 - val_Si-sdr: 18.4549\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 154.73450\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 153.3197 - Si-sdr: 19.1314 - val_loss: 155.3040 - val_Si-sdr: 18.8200\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 154.73450\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 1s 582ms/step - loss: 153.1925 - Si-sdr: 19.0595 - val_loss: 154.0834 - val_Si-sdr: 18.8985\n",
      "\n",
      "Epoch 00169: val_loss improved from 154.73450 to 154.08344, saving model to ./CKPT\\CKP_ep_169__loss_154.08344_.h5\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 155.3531 - Si-sdr: 18.6834 - val_loss: 154.8897 - val_Si-sdr: 18.7768\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 154.08344\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 153.6425 - Si-sdr: 18.9356 - val_loss: 156.4037 - val_Si-sdr: 18.6215\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 154.08344\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 156.9008 - Si-sdr: 18.5971 - val_loss: 153.6346 - val_Si-sdr: 19.0975\n",
      "\n",
      "Epoch 00172: val_loss improved from 154.08344 to 153.63461, saving model to ./CKPT\\CKP_ep_172__loss_153.63461_.h5\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 154.9354 - Si-sdr: 18.8363 - val_loss: 153.4540 - val_Si-sdr: 19.1196\n",
      "\n",
      "Epoch 00173: val_loss improved from 153.63461 to 153.45396, saving model to ./CKPT\\CKP_ep_173__loss_153.45396_.h5\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 154.4977 - Si-sdr: 18.8361 - val_loss: 154.8081 - val_Si-sdr: 19.0020\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 153.45396\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 1s 671ms/step - loss: 153.9434 - Si-sdr: 18.9744 - val_loss: 152.7488 - val_Si-sdr: 19.1965\n",
      "\n",
      "Epoch 00175: val_loss improved from 153.45396 to 152.74878, saving model to ./CKPT\\CKP_ep_175__loss_152.74878_.h5\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 154.8144 - Si-sdr: 18.9248 - val_loss: 155.9787 - val_Si-sdr: 18.5495\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 152.74878\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 155.0034 - Si-sdr: 18.8178 - val_loss: 155.0885 - val_Si-sdr: 18.7388\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 152.74878\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 154.8391 - Si-sdr: 18.8372 - val_loss: 155.1705 - val_Si-sdr: 18.9240\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 152.74878\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 157.4842 - Si-sdr: 18.6009 - val_loss: 157.7800 - val_Si-sdr: 18.4452\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 152.74878\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 156.1544 - Si-sdr: 18.7750 - val_loss: 157.3057 - val_Si-sdr: 18.5620\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 152.74878\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 155.5408 - Si-sdr: 18.7666 - val_loss: 155.3206 - val_Si-sdr: 18.7232\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 152.74878\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 154.5195 - Si-sdr: 18.8429 - val_loss: 154.6823 - val_Si-sdr: 18.9088\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 152.74878\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 155.5558 - Si-sdr: 18.8045 - val_loss: 157.1881 - val_Si-sdr: 18.4766\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 152.74878\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 1s 654ms/step - loss: 155.0873 - Si-sdr: 18.8043 - val_loss: 156.1530 - val_Si-sdr: 18.7014\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 152.74878\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 155.6892 - Si-sdr: 18.6367 - val_loss: 154.7833 - val_Si-sdr: 18.8124\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 152.74878\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 155.7365 - Si-sdr: 18.6899 - val_loss: 154.6637 - val_Si-sdr: 18.8000\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 152.74878\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 156.1528 - Si-sdr: 18.6626 - val_loss: 155.2691 - val_Si-sdr: 18.7650\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 152.74878\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 159.0910 - Si-sdr: 18.2275 - val_loss: 154.2825 - val_Si-sdr: 18.9788\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 152.74878\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 1s 688ms/step - loss: 156.0361 - Si-sdr: 18.7375 - val_loss: 153.5009 - val_Si-sdr: 19.0475\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 152.74878\n",
      "Epoch 190/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 670ms/step - loss: 156.0547 - Si-sdr: 18.7317 - val_loss: 154.3344 - val_Si-sdr: 18.9764\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 152.74878\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 1s 659ms/step - loss: 157.1492 - Si-sdr: 18.5595 - val_loss: 152.2508 - val_Si-sdr: 19.2351\n",
      "\n",
      "Epoch 00191: val_loss improved from 152.74878 to 152.25079, saving model to ./CKPT\\CKP_ep_191__loss_152.25079_.h5\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 155.4266 - Si-sdr: 18.8515 - val_loss: 155.3374 - val_Si-sdr: 18.8601\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 152.25079\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 155.9257 - Si-sdr: 18.5935 - val_loss: 157.5730 - val_Si-sdr: 18.4001\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 152.25079\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 155.1710 - Si-sdr: 18.8159 - val_loss: 159.5416 - val_Si-sdr: 18.2026\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 152.25079\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 155.7562 - Si-sdr: 18.6038 - val_loss: 157.3565 - val_Si-sdr: 18.4915\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 152.25079\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 1s 575ms/step - loss: 156.4913 - Si-sdr: 18.5726 - val_loss: 155.6101 - val_Si-sdr: 18.6977\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 152.25079\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 155.6255 - Si-sdr: 18.6791 - val_loss: 154.3762 - val_Si-sdr: 18.8746\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 152.25079\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 1s 613ms/step - loss: 153.7932 - Si-sdr: 18.9986 - val_loss: 152.9284 - val_Si-sdr: 19.1512\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 152.25079\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 1s 634ms/step - loss: 152.8419 - Si-sdr: 19.1048 - val_loss: 155.6829 - val_Si-sdr: 18.6849\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 152.25079\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 153.9914 - Si-sdr: 18.9214 - val_loss: 154.7932 - val_Si-sdr: 18.8964\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 152.25079\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 153.9773 - Si-sdr: 19.0365 - val_loss: 153.2767 - val_Si-sdr: 19.1253\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 152.25079\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 154.4046 - Si-sdr: 18.8964 - val_loss: 150.7900 - val_Si-sdr: 19.4926\n",
      "\n",
      "Epoch 00202: val_loss improved from 152.25079 to 150.78998, saving model to ./CKPT\\CKP_ep_202__loss_150.78998_.h5\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 1s 591ms/step - loss: 150.9834 - Si-sdr: 19.4759 - val_loss: 151.4812 - val_Si-sdr: 19.4756\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 150.78998\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 154.6184 - Si-sdr: 18.8595 - val_loss: 151.8740 - val_Si-sdr: 19.2981\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 150.78998\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 153.8887 - Si-sdr: 18.9474 - val_loss: 153.4456 - val_Si-sdr: 19.0844\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 150.78998\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 1s 679ms/step - loss: 152.9410 - Si-sdr: 19.0856 - val_loss: 153.1238 - val_Si-sdr: 19.1348\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 150.78998\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 151.7437 - Si-sdr: 19.2986 - val_loss: 150.5558 - val_Si-sdr: 19.5598\n",
      "\n",
      "Epoch 00207: val_loss improved from 150.78998 to 150.55579, saving model to ./CKPT\\CKP_ep_207__loss_150.55579_.h5\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 151.9528 - Si-sdr: 19.3472 - val_loss: 152.3739 - val_Si-sdr: 19.2433\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 150.55579\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 153.7287 - Si-sdr: 18.9447 - val_loss: 151.6149 - val_Si-sdr: 19.3331\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 150.55579\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 1s 666ms/step - loss: 151.7310 - Si-sdr: 19.3917 - val_loss: 153.9189 - val_Si-sdr: 18.8177\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 150.55579\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 1s 602ms/step - loss: 152.0458 - Si-sdr: 19.2691 - val_loss: 153.0210 - val_Si-sdr: 19.0925\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 150.55579\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 152.9407 - Si-sdr: 19.1475 - val_loss: 152.2974 - val_Si-sdr: 19.2147\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 150.55579\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 1s 577ms/step - loss: 153.5745 - Si-sdr: 19.0871 - val_loss: 152.3992 - val_Si-sdr: 19.2550\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 150.55579\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 150.4712 - Si-sdr: 19.6190 - val_loss: 150.4602 - val_Si-sdr: 19.5845\n",
      "\n",
      "Epoch 00214: val_loss improved from 150.55579 to 150.46024, saving model to ./CKPT\\CKP_ep_214__loss_150.46024_.h5\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 150.0017 - Si-sdr: 19.6712 - val_loss: 149.1075 - val_Si-sdr: 19.8637\n",
      "\n",
      "Epoch 00215: val_loss improved from 150.46024 to 149.10751, saving model to ./CKPT\\CKP_ep_215__loss_149.10751_.h5\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 149.6518 - Si-sdr: 19.6431 - val_loss: 149.5836 - val_Si-sdr: 19.7449\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 149.10751\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 1s 623ms/step - loss: 149.7915 - Si-sdr: 19.7968 - val_loss: 148.8158 - val_Si-sdr: 19.8262\n",
      "\n",
      "Epoch 00217: val_loss improved from 149.10751 to 148.81577, saving model to ./CKPT\\CKP_ep_217__loss_148.81577_.h5\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 1s 583ms/step - loss: 148.6612 - Si-sdr: 19.8660 - val_loss: 149.4400 - val_Si-sdr: 19.6409\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 148.81577\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 149.8207 - Si-sdr: 19.6163 - val_loss: 149.9331 - val_Si-sdr: 19.5480\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 148.81577\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 149.7294 - Si-sdr: 19.5330 - val_loss: 149.4054 - val_Si-sdr: 19.6799\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 148.81577\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 150.8250 - Si-sdr: 19.4057 - val_loss: 151.7603 - val_Si-sdr: 19.3711\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 148.81577\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 150.1573 - Si-sdr: 19.5836 - val_loss: 149.4622 - val_Si-sdr: 19.6709\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 148.81577\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 1s 612ms/step - loss: 149.4017 - Si-sdr: 19.6914 - val_loss: 149.9531 - val_Si-sdr: 19.6771\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 148.81577\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 150.6738 - Si-sdr: 19.4894 - val_loss: 149.6225 - val_Si-sdr: 19.5665\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 148.81577\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 151.1703 - Si-sdr: 19.3821 - val_loss: 150.0438 - val_Si-sdr: 19.6695\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 148.81577\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 150.2120 - Si-sdr: 19.5918 - val_loss: 150.8618 - val_Si-sdr: 19.4270\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 148.81577\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 152.8668 - Si-sdr: 19.2100 - val_loss: 153.2442 - val_Si-sdr: 19.1888\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 148.81577\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 153.2435 - Si-sdr: 19.1623 - val_loss: 149.3290 - val_Si-sdr: 19.6444\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 148.81577\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 625ms/step - loss: 149.0880 - Si-sdr: 19.7289 - val_loss: 149.9406 - val_Si-sdr: 19.7042\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 148.81577\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 1s 581ms/step - loss: 149.1716 - Si-sdr: 19.7918 - val_loss: 147.0437 - val_Si-sdr: 20.0220\n",
      "\n",
      "Epoch 00230: val_loss improved from 148.81577 to 147.04370, saving model to ./CKPT\\CKP_ep_230__loss_147.04370_.h5\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 147.4637 - Si-sdr: 20.0601 - val_loss: 147.4376 - val_Si-sdr: 20.0778\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 147.04370\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 147.0106 - Si-sdr: 20.1286 - val_loss: 147.6100 - val_Si-sdr: 19.9743\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 147.04370\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 1s 677ms/step - loss: 148.6405 - Si-sdr: 19.8281 - val_loss: 147.9089 - val_Si-sdr: 19.8582\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 147.04370\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 149.0888 - Si-sdr: 19.8619 - val_loss: 146.1893 - val_Si-sdr: 20.2717\n",
      "\n",
      "Epoch 00234: val_loss improved from 147.04370 to 146.18925, saving model to ./CKPT\\CKP_ep_234__loss_146.18925_.h5\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 146.3497 - Si-sdr: 20.2946 - val_loss: 148.7356 - val_Si-sdr: 19.8948\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 146.18925\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 149.6834 - Si-sdr: 19.7159 - val_loss: 147.8943 - val_Si-sdr: 19.9578\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 146.18925\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 148.0422 - Si-sdr: 20.0127 - val_loss: 148.7275 - val_Si-sdr: 19.7655\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 146.18925\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 1s 588ms/step - loss: 151.2247 - Si-sdr: 19.4555 - val_loss: 150.4343 - val_Si-sdr: 19.5807\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 146.18925\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 1s 593ms/step - loss: 148.9952 - Si-sdr: 19.8218 - val_loss: 149.9233 - val_Si-sdr: 19.6677\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 146.18925\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 150.3089 - Si-sdr: 19.5300 - val_loss: 150.0345 - val_Si-sdr: 19.5536\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 146.18925\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 150.7842 - Si-sdr: 19.4774 - val_loss: 150.4164 - val_Si-sdr: 19.5936\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 146.18925\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 149.7737 - Si-sdr: 19.6937 - val_loss: 147.0581 - val_Si-sdr: 20.0795\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 146.18925\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 150.0298 - Si-sdr: 19.4765 - val_loss: 151.0980 - val_Si-sdr: 19.4449\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 146.18925\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 150.3096 - Si-sdr: 19.6409 - val_loss: 145.6240 - val_Si-sdr: 20.3113\n",
      "\n",
      "Epoch 00244: val_loss improved from 146.18925 to 145.62402, saving model to ./CKPT\\CKP_ep_244__loss_145.62402_.h5\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 1s 582ms/step - loss: 149.5885 - Si-sdr: 19.5998 - val_loss: 148.2092 - val_Si-sdr: 19.9652\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 145.62402\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 150.0515 - Si-sdr: 19.5969 - val_loss: 150.2014 - val_Si-sdr: 19.5058\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 145.62402\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 150.2344 - Si-sdr: 19.5040 - val_loss: 148.6207 - val_Si-sdr: 19.9067\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 145.62402\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 148.5010 - Si-sdr: 19.8407 - val_loss: 148.4253 - val_Si-sdr: 19.9930\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 145.62402\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 148.4292 - Si-sdr: 19.7671 - val_loss: 149.1813 - val_Si-sdr: 19.7733\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 145.62402\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 1s 594ms/step - loss: 149.7547 - Si-sdr: 19.7523 - val_loss: 145.9514 - val_Si-sdr: 20.3442\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 145.62402\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 148.0376 - Si-sdr: 19.9107 - val_loss: 153.6633 - val_Si-sdr: 19.1036\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 145.62402\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 152.8607 - Si-sdr: 19.2947 - val_loss: 148.8730 - val_Si-sdr: 19.9715\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 145.62402\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 150.2606 - Si-sdr: 19.6842 - val_loss: 148.2162 - val_Si-sdr: 19.9040\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 145.62402\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 1s 595ms/step - loss: 148.9117 - Si-sdr: 19.7665 - val_loss: 145.9027 - val_Si-sdr: 20.4624\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 145.62402\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 148.0013 - Si-sdr: 20.0375 - val_loss: 146.2915 - val_Si-sdr: 20.3175\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 145.62402\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 1s 582ms/step - loss: 146.3719 - Si-sdr: 20.2248 - val_loss: 147.0583 - val_Si-sdr: 20.0871\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 145.62402\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 145.5528 - Si-sdr: 20.3518 - val_loss: 146.6933 - val_Si-sdr: 20.2506\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 145.62402\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 1s 686ms/step - loss: 147.3869 - Si-sdr: 20.0736 - val_loss: 144.8177 - val_Si-sdr: 20.4775\n",
      "\n",
      "Epoch 00258: val_loss improved from 145.62402 to 144.81772, saving model to ./CKPT\\CKP_ep_258__loss_144.81772_.h5\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 147.4163 - Si-sdr: 20.1061 - val_loss: 149.1965 - val_Si-sdr: 19.8167\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 144.81772\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 147.6335 - Si-sdr: 20.0041 - val_loss: 149.9636 - val_Si-sdr: 19.8500\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 144.81772\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 147.8925 - Si-sdr: 19.9497 - val_loss: 146.2478 - val_Si-sdr: 20.1850\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 144.81772\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 1s 586ms/step - loss: 147.1526 - Si-sdr: 20.2047 - val_loss: 146.2984 - val_Si-sdr: 20.2358\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 144.81772\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 147.8083 - Si-sdr: 20.0931 - val_loss: 147.8832 - val_Si-sdr: 20.0045\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 144.81772\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 146.5554 - Si-sdr: 20.3038 - val_loss: 146.0495 - val_Si-sdr: 20.4288\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 144.81772\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 1s 655ms/step - loss: 147.3047 - Si-sdr: 20.0869 - val_loss: 144.6542 - val_Si-sdr: 20.6121\n",
      "\n",
      "Epoch 00265: val_loss improved from 144.81772 to 144.65421, saving model to ./CKPT\\CKP_ep_265__loss_144.65421_.h5\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 1s 579ms/step - loss: 146.6985 - Si-sdr: 20.3291 - val_loss: 146.4107 - val_Si-sdr: 20.3012\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 144.65421\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 147.3844 - Si-sdr: 20.1560 - val_loss: 148.3902 - val_Si-sdr: 19.8906\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 144.65421\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 1s 608ms/step - loss: 145.3471 - Si-sdr: 20.3866 - val_loss: 147.2589 - val_Si-sdr: 20.0657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00268: val_loss did not improve from 144.65421\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 1s 679ms/step - loss: 145.8702 - Si-sdr: 20.3036 - val_loss: 144.6022 - val_Si-sdr: 20.5515\n",
      "\n",
      "Epoch 00269: val_loss improved from 144.65421 to 144.60216, saving model to ./CKPT\\CKP_ep_269__loss_144.60216_.h5\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 1s 589ms/step - loss: 146.7184 - Si-sdr: 20.1377 - val_loss: 145.9048 - val_Si-sdr: 20.3916\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 144.60216\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 1s 617ms/step - loss: 145.0035 - Si-sdr: 20.5843 - val_loss: 146.8464 - val_Si-sdr: 20.1896\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 144.60216\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 144.1174 - Si-sdr: 20.7111 - val_loss: 143.6873 - val_Si-sdr: 20.6316\n",
      "\n",
      "Epoch 00272: val_loss improved from 144.60216 to 143.68735, saving model to ./CKPT\\CKP_ep_272__loss_143.68735_.h5\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 1s 667ms/step - loss: 144.4270 - Si-sdr: 20.5333 - val_loss: 143.5054 - val_Si-sdr: 20.6723\n",
      "\n",
      "Epoch 00273: val_loss improved from 143.68735 to 143.50540, saving model to ./CKPT\\CKP_ep_273__loss_143.50540_.h5\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 143.9305 - Si-sdr: 20.5829 - val_loss: 145.0695 - val_Si-sdr: 20.4471\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 143.50540\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 145.1395 - Si-sdr: 20.4190 - val_loss: 145.3434 - val_Si-sdr: 20.4260\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 143.50540\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 144.0087 - Si-sdr: 20.5286 - val_loss: 144.5164 - val_Si-sdr: 20.5001\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 143.50540\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 1s 576ms/step - loss: 144.1093 - Si-sdr: 20.7069 - val_loss: 143.8888 - val_Si-sdr: 20.7412\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 143.50540\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 1s 681ms/step - loss: 145.2732 - Si-sdr: 20.4598 - val_loss: 142.6326 - val_Si-sdr: 20.8605\n",
      "\n",
      "Epoch 00278: val_loss improved from 143.50540 to 142.63257, saving model to ./CKPT\\CKP_ep_278__loss_142.63257_.h5\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 144.4067 - Si-sdr: 20.5482 - val_loss: 143.5287 - val_Si-sdr: 20.7630\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 142.63257\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 146.0430 - Si-sdr: 20.3056 - val_loss: 144.2835 - val_Si-sdr: 20.5708\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 142.63257\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 143.8700 - Si-sdr: 20.6305 - val_loss: 143.6942 - val_Si-sdr: 20.7289\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 142.63257\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 1s 613ms/step - loss: 143.2056 - Si-sdr: 20.7543 - val_loss: 144.6647 - val_Si-sdr: 20.4920\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 142.63257\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 1s 610ms/step - loss: 141.2926 - Si-sdr: 21.1397 - val_loss: 141.7704 - val_Si-sdr: 21.0422\n",
      "\n",
      "Epoch 00283: val_loss improved from 142.63257 to 141.77045, saving model to ./CKPT\\CKP_ep_283__loss_141.77045_.h5\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 1s 609ms/step - loss: 142.8406 - Si-sdr: 20.8817 - val_loss: 143.1893 - val_Si-sdr: 20.7975\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 141.77045\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 144.9773 - Si-sdr: 20.6055 - val_loss: 145.8860 - val_Si-sdr: 20.4842\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 141.77045\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 146.2991 - Si-sdr: 20.3970 - val_loss: 142.8976 - val_Si-sdr: 20.9364\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 141.77045\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 147.0822 - Si-sdr: 20.3251 - val_loss: 145.6377 - val_Si-sdr: 20.5268\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 141.77045\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 1s 588ms/step - loss: 143.0803 - Si-sdr: 20.9894 - val_loss: 146.2050 - val_Si-sdr: 20.5497\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 141.77045\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 1s 613ms/step - loss: 147.0738 - Si-sdr: 20.4583 - val_loss: 142.9447 - val_Si-sdr: 20.8180\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 141.77045\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 1s 601ms/step - loss: 143.4773 - Si-sdr: 20.8140 - val_loss: 145.6115 - val_Si-sdr: 20.5614\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 141.77045\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 145.4841 - Si-sdr: 20.4307 - val_loss: 145.3083 - val_Si-sdr: 20.3261\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 141.77045\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 1s 593ms/step - loss: 144.3890 - Si-sdr: 20.5846 - val_loss: 145.5175 - val_Si-sdr: 20.5413\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 141.77045\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 147.2906 - Si-sdr: 20.1880 - val_loss: 144.0034 - val_Si-sdr: 20.6492\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 141.77045\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 1s 649ms/step - loss: 144.9239 - Si-sdr: 20.5722 - val_loss: 145.7261 - val_Si-sdr: 20.3607\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 141.77045\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 1s 615ms/step - loss: 145.6976 - Si-sdr: 20.3140 - val_loss: 144.1573 - val_Si-sdr: 20.5138\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 141.77045\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 1s 568ms/step - loss: 144.1532 - Si-sdr: 20.6603 - val_loss: 144.9274 - val_Si-sdr: 20.5027\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 141.77045\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 145.7708 - Si-sdr: 20.3809 - val_loss: 145.8256 - val_Si-sdr: 20.2784\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 141.77045\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 144.4121 - Si-sdr: 20.6586 - val_loss: 145.5633 - val_Si-sdr: 20.4557\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 141.77045\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 145.8239 - Si-sdr: 20.4981 - val_loss: 144.7654 - val_Si-sdr: 20.5627\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 141.77045\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 1s 675ms/step - loss: 144.1905 - Si-sdr: 20.7652 - val_loss: 144.8107 - val_Si-sdr: 20.6485\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 141.77045\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-terry",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "productive-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_25 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_29 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = np.random.rand(2, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=4, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.14930329 -0.05488385 -0.03165275  0.02511186]\n",
      "  [-0.04538564 -0.02340828 -0.13736942  0.03185076]\n",
      "  [-0.18286747 -0.01420305  0.02583414  0.20739384]\n",
      "  [-0.15128048 -0.15295134  0.00826486 -0.03651741]\n",
      "  [-0.19406578 -0.14820886 -0.24136184  0.01954714]\n",
      "  [-0.11608941  0.05977409 -0.05805521  0.08262399]\n",
      "  [-0.10437049  0.00281959  0.00744648  0.06939161]\n",
      "  [-0.32889026 -0.11109954 -0.06513101  0.02347144]\n",
      "  [-0.10914627  0.04005037 -0.11194003  0.34588984]\n",
      "  [-0.55321944 -0.24241613 -0.17234027  0.2255739 ]]\n",
      "\n",
      " [[-0.12665638 -0.02560038 -0.07357763 -0.03386865]\n",
      "  [-0.00296582  0.05382628 -0.02431939 -0.00124017]\n",
      "  [-0.152773    0.01729362 -0.02049777  0.00555693]\n",
      "  [-0.01630396 -0.0508009   0.00338947 -0.00536075]\n",
      "  [-0.11710438 -0.0411608   0.01888775  0.10048383]\n",
      "  [-0.1000828   0.06160894 -0.04584875  0.14583868]\n",
      "  [-0.29426795 -0.04820506 -0.1607864   0.03541403]\n",
      "  [-0.14156486  0.07812893 -0.13114211  0.12421213]\n",
      "  [-0.20461929  0.08133916 -0.08211927  0.03100684]\n",
      "  [-0.00948744 -0.11452891  0.03942408 -0.1281477 ]]]\n",
      "(2, 10, 4)\n",
      "(2, 9, 4)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "# target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "# noise = output_array2 - target\n",
    "# si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "# si_sdr = tf.reduce_mean(si_sdr)\n",
    "# print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
