{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {
    "id": "frank-northwest"
   },
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "requested-installation",
   "metadata": {
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1641547283433,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "requested-installation"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import librosa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import scipy.signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3fbd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "#         signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "        \n",
    "#         return signal[0]\n",
    "    \n",
    "        signal_rate, signal = wavfile.read(path)\n",
    "        number_of_samples = round(len(signal) * float(self.sample_rate) / signal_rate)\n",
    "        signal = scipy.signal.resample(signal, number_of_samples)\n",
    "        signal /= np.max(np.abs(signal),axis=0)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "#             return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "            return sour_pad, label_pad\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "architectural-safety",
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1641547287252,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "architectural-safety",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RawStackForVAEGenerator(Sequence):   ## sequence 상속\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=16, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):   ## 하나의 epoch이 끝나면 자동으로 실행\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:  ## 다른 샘플 shuffling\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):   ## 다른 폴더에 있는 오디어 파일 데이터를 읽어옴\n",
    "#         signal = librosa.load(path, sr=sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "#         return signal[0]\n",
    "    \n",
    "        signal_rate, signal = wavfile.read(path)\n",
    "        number_of_samples = round(len(signal) * float(self.sample_rate) / signal_rate)\n",
    "        signal = scipy.signal.resample(signal, number_of_samples)\n",
    "        signal /= np.max(np.abs(signal),axis=0)\n",
    "        \n",
    "        return signal\n",
    "    \n",
    "    def __padding__(self, data):   ## 짧은 오디오 데이터에 0을 붙여서 길이를 맞춰줌\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        pad = np.zeros((n_batch, max_len, data[0].shape[1]))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return pad\n",
    "        \n",
    "    def __data_generation__(self, source_list):   ## audio-read해서 읽어온 데이터를 전처리\n",
    "        L = 40\n",
    "        \n",
    "        wav_list = []\n",
    "        label_wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "#             print(s_wav.shape)\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- TIME AXIS CALCULATE -------\n",
    "            K = int(np.ceil(len(s_wav) / L))\n",
    "            \n",
    "            if K//8 != 0:          # stride=2일 때, K를 4의 배수로 만들어줌\n",
    "                K = ((K//8)+1)*8\n",
    "            # -----------------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "            pad_len = K * L\n",
    "            pad_s = np.concatenate([s_wav, np.zeros([pad_len - len(s_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            # ------- RESHAPE -------\n",
    "            s = np.reshape(pad_s, [K, L])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s)\n",
    "            label_wav_list.append(np.expand_dims(s_wav, 1))\n",
    "        \n",
    "        return wav_list, label_wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)   ## 데이터 받아오기\n",
    "            \n",
    "            # Get Lengths(label length)\n",
    "            lengths = np.array([m.shape[0] for m in labels])\n",
    "            tiled = np.tile(np.expand_dims(lengths, 1), [1, labels[0].shape[1]])\n",
    "            tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step_for_stack, Dimension(=40)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, sour_pad\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "#             lengths = np.array([m.shape[0] for m in mix])\n",
    "#             tiled = np.tile(np.expand_dims(l bvengths, 1), [1, labels[0].shape[1]])\n",
    "#             tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension]\n",
    "            \n",
    "            return sour_pad, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {
    "id": "taken-league"
   },
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "martial-allocation",
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1641547292579,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "martial-allocation"
   },
   "outputs": [],
   "source": [
    "# WAV_DIR = './mycode_before/wsj0_2mix/use_this/'\n",
    "# LIST_DIR = './mycode_before/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "52eb5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "attached-debate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17855,
     "status": "ok",
     "timestamp": 1641547313592,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "attached-debate",
    "outputId": "94bfe083-c2a7-4971-fa74-e89a48a8a1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "comparable-tiger",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547316527,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "comparable-tiger"
   },
   "outputs": [],
   "source": [
    "batch_size_for_generator = 1   # mycode_before\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "af27c272",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4969,
     "status": "ok",
     "timestamp": 1641547324696,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "af27c272",
    "outputId": "6b3ddea0-efad-4668-ad05-33090b5d2fbe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 672, 40)\n",
      "['8463-287645-0008_4507-16021-0001.wav\\n']\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = next(iter(test_dataset))\n",
    "print(x1.shape)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "51923d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2624, 40)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "674407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = train_dataset[0][0]\n",
    "x2 = train_dataset[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {
    "id": "human-russian"
   },
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "environmental-revision",
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1641547328656,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "environmental-revision"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "sexual-ordering",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547329495,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "sexual-ordering"
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "piano-height",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1641547329496,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "piano-height"
   },
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "printable-tennis",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547329930,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "printable-tennis"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb3284d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547332674,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "fb3284d8"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):  \n",
    "    def __init__(self, num_embeddings, embedding_dim, name='embedding_vqvae', beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = (beta)\n",
    "\n",
    "        # Initialize the embeddings which we will quantize\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "        \n",
    "        # Quantization\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer\n",
    "        commitment_loss = self.beta * tf.reduce_mean(\n",
    "            (tf.stop_gradient(quantized) - x) ** 2\n",
    "        )\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        quantization_loss = commitment_loss + codebook_loss\n",
    "        \n",
    "        self.add_loss(0.5 * quantization_loss)\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        \n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        \n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d0e63776",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547333677,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "d0e63776"
   },
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "class Encoder(layers.Layer):  \n",
    "    def __init__(self, kernel, latent_dim, strides, name = 'encoder',**kwargs):  ## ak==latendt dim : 40 -> latent dim\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=2*latent_dim, kernel_size=kernel, strides=strides, activation='tanh', padding='same')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=latent_dim, kernel_size=kernel, strides=strides, activation='tanh', padding='same')\n",
    "        self.dropout_1 = layers.Dropout(0.2)\n",
    "        self.resblock_1 = ResBlock(filters=latent_dim)\n",
    "#         self.maxpooling1d_1 = layers.MaxPooling1D()\n",
    "#         self.upsampling1d_1 = layers.UpSampling1D()\n",
    "#         self.batchnorm1d_1 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.conv1d_2(inputs)\n",
    "        print('conv1d ', logit1.shape)\n",
    "#         logit2 = self.conv1d_2(logit1)\n",
    "#         print('Encoder_conv1d ', logit2.shape)\n",
    "#         logit2 = self.resblock_1(logit1)  \n",
    "        \n",
    "#         logit2 = self.dropout_1(logit1)\n",
    "#         print('Encoder_dropout1d ', logit2.shape)\n",
    "#         logit3 = self.conv1d_2(logit2)\n",
    "#         print('Encoder_conv1d ', logit3.shape)  \n",
    "        \n",
    "        return logit1\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):  \n",
    "    def __init__(self, kernel, latent_dim, strides, activation, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=2*latent_dim, kernel_size=kernel, strides=strides, activation=activation, padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=latent_dim, kernel_size=kernel, strides=strides, activation=activation, padding='same')\n",
    "        self.resblock_1 = ResBlock(filters=latent_dim)\n",
    "#         self.upsampling_1 = layers.UpSampling1D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.trans_conv1d_2(inputs)\n",
    "        print('transconv1d ', logit1.shape)\n",
    "#         logit2 = self.resblock_1(logit1)\n",
    "#         logit3 = self.trans_conv1d_2(logit)\n",
    "#         print('Decoder_transconv1d ', x.shape)\n",
    "        \n",
    "        return logit1\n",
    "    \n",
    "    \n",
    "class ResBlock(layers.Layer):  \n",
    "    def __init__(self, filters, name = 'resblock', **kwargs):\n",
    "        super(ResBlock, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=filters, kernel_size=1, strides=1, activation='tanh')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.conv1d_1(inputs)\n",
    "        logit2 = inputs + logit1\n",
    "        \n",
    "        return logit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30fcfafc",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547334774,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "30fcfafc"
   },
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):  \n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        y_true_flatten = self.overlab_add(y_true)\n",
    "        y_pred_flatten = self.overlab_add(y_pred)\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        labels = y_true_flatten\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred_flatten)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred_flatten = tf.slice(y_pred_flatten, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred_flatten, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred_flatten - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def overlab_add(self, y_pred):\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "        return tf.reshape(y_pred, [batch_size, -1, 1])\n",
    "\n",
    "    def result(self):\n",
    "        print(self.sdr / self.count)  ##\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd48a2a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547336653,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "fd48a2a7"
   },
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - y_true, 2), axis=[1, 2])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91a366d5",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547339090,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "91a366d5"
   },
   "outputs": [],
   "source": [
    "# Here vnsmsrj\n",
    "\n",
    "def overlab_add(y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "    \n",
    "    return tf.reshape(y_pred, [batch_size, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "renewable-discharge",
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1641547340148,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "renewable-discharge"
   },
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):   ####\n",
    "    def __init__(self, embedding_dim, num_embeddings, gumbel_hard=False, for_predict=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.for_predict = for_predict\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        self.encoder1 = Encoder(4, 128, 2, name='encoder1')\n",
    "        self.encoder2 = Encoder(4, embedding_dim, 2, name='encoder2')\n",
    "        self.encoder3 = Encoder(4, embedding_dim, 2, name='encoder3')  # 추가\n",
    "        self.quantize1 = VectorQuantizer(num_embeddings, embedding_dim, name='embedding_vqvae1')  \n",
    "        \n",
    "        self.decoder1 = Decoder(4, embedding_dim, 2, 'relu', name='decoder1')\n",
    "        self.decoder2 = Decoder(4, 128, 2, 'relu', name='decoder2')  # 추가\n",
    "        self.decoder3 = Decoder(4, 40, 2, None, name='decoder3')\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 40))\n",
    "               \n",
    "        if self.for_predict:\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            one_hot_enc = tf.cast(tf.equal(encode2, tf.math.reduce_max(encode2, 2, keepdims=True)), encode2.dtype)\n",
    "            sample = self.sampled(one_hot_enc)\n",
    "            decode = self.decoder(sample)\n",
    "            \n",
    "        else:\n",
    "            print(inputs.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            print('\\n# Encoder2')\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            print('\\n# Encoder3')\n",
    "            encode3 = self.encoder3(encode2)  # 추가\n",
    "            print('\\n# Quantization1')\n",
    "            quantization1 = self.quantize1(encode3)   # encode2 -> 3\n",
    "            print('encoder3 ', quantization1.shape)            \n",
    "            print('\\n# Decoder1')\n",
    "            decode1 = self.decoder1(quantization1)\n",
    "            print('\\n# Decoder2')\n",
    "            decode2 = self.decoder2(decode1)  # 추가\n",
    "            print('\\n# Enc')\n",
    "            enc = layers.Concatenate()([encode1, decode2])  # decode1 -> 2\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "            print('encoder1+decoder2 ', enc.shape)\n",
    "            print('\\n# Decoder3')\n",
    "            decode3 = self.decoder3(enc)\n",
    "            \n",
    "            \"\"\"\n",
    "            print(inputs.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            print('####')\n",
    "            \n",
    "            print(b.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1_2 = self.encoder1(b)\n",
    "            print('\\n# Encoder2')\n",
    "            encode2 = self.encoder2(encode1_2)\n",
    "            print('\\n# Encoder3')\n",
    "            encode3 = self.encoder3(encode2)  # 추가\n",
    "            print('\\n# Quantization1')\n",
    "            quantization1 = self.quantize1(encode3)   # encode2 -> 3\n",
    "            print('encoder3 ', quantization1.shape)            \n",
    "            print('\\n# Decoder1')\n",
    "            decode1 = self.decoder1(quantization1)\n",
    "            print('\\n# Decoder2')\n",
    "            decode2 = self.decoder2(decode1)  # 추가\n",
    "            \n",
    "            print('\\n# Enc')\n",
    "            enc = layers.Concatenate()([encode1, decode2])  # decode1 -> 2\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "            print('encoder1+decoder2 ', enc.shape)\n",
    "            print('\\n# Decoder3')\n",
    "            decode3 = self.decoder3(enc)\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\"\n",
    "            print(inputs.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            print('\\n# Encoder2')\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            print('\\n# Encoder3')\n",
    "            encode3 = self.encoder3(encode2)  \n",
    "            print('\\n# Quantization1')\n",
    "            quantization1 = self.quantize1(encode1)  \n",
    "            print('encoder1 ', quantization1.shape)\n",
    "            print('\\n# Quantization2')\n",
    "            quantization2 = self.quantize2(encode3)   # 추가          \n",
    "            print('encoder3 ', quantization2.shape)            \n",
    "            print('\\n# Decoder1')\n",
    "            decode1 = self.decoder1(quantization2)\n",
    "            print('\\n# Decoder2')\n",
    "            decode2 = self.decoder2(decode1)  \n",
    "            print('\\n# Enc')\n",
    "            enc = layers.Concatenate()([quantization1, decode2])  # decode1 -> 2\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "            print('quantization1+decoder2 ', enc.shape)\n",
    "            print('\\n# Decoder3')\n",
    "            decode3 = self.decoder3(enc)\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\" 1\n",
    "            print('# Input')\n",
    "            print(inputs.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            print('\\n# Encoder2')\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            print('\\n# Encoder3')\n",
    "            encode3 = self.encoder3(encode2)\n",
    "            print('\\n# QuSantization1')\n",
    "            quantization1 = self.quantize1(encode2)\n",
    "            print('encoder2 ', quantization1.shape)\n",
    "            print('\\n# Quantization2')\n",
    "            quantization2 = self.quantize2(encode3)\n",
    "            print('encoder3 ', quantization2.shape)\n",
    "            print('\\n# Decoder1')\n",
    "            decode1 = self.decoder1(quantization2)\n",
    "            print('\\n# Enc')\n",
    "            enc = layers.Concatenate()([quantization1, decode1])\n",
    "            print('quantization1 + decoder1 ', enc.shape)\n",
    "            print('\\n# Decoder2')\n",
    "            decode2 = self.decoder2(enc)\n",
    "            print('\\n# Decoder3')\n",
    "            decode3 = self.decoder3(decode2)\n",
    "            \"\"\"\n",
    "            \n",
    "            \"\"\" original code \"\"\"\n",
    "#             print('# Encoder1')\n",
    "#             encode1 = self.encoder1(inputs)\n",
    "#             print('\\n# Encoder2')\n",
    "#             encode2 = self.encoder2(encode1)\n",
    "# #             print('# Encoder2-2')  \n",
    "# #             encode2_2 = self.encoder2_2(encode2)   # top level 추가\n",
    "#             print('\\n# Quantization1')\n",
    "#             quantization1 = self.quantize1(encode2)  \n",
    "#             print('encoder2 ', quantization1.shape)\n",
    "#             print('\\n# Decoder1')\n",
    "#             decode1 = self.decoder1(quantization1)  \n",
    "# #             print('# Decoder1_2')\n",
    "# #             decode1_2 = self.decoder1(decode1)   # new bottom level\n",
    "            \n",
    "#             print('\\n# Enc')\n",
    "# #             enc = layers.Concatenate()([encode1, decode1])\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "#             print('encoder1+decoder1 ', enc.shape)\n",
    "#             print('\\n# Encoder3')\n",
    "#             encode3 = self.encoder3(enc)\n",
    "#             print('\\n# Quantization2')\n",
    "#             quantization2 = self.quantize2(encode3)\n",
    "#             print('encoder3 ', quantization2.shape)\n",
    "#             print('\\n# Decoder2')\n",
    "#             decode2 = self.decoder2(quantization2)\n",
    "            \n",
    "#             print('\\n# Quant')\n",
    "#             quant = layers.Concatenate()([decode2, quantization2])\n",
    "#             print('decoder2+quantization2 ', quant.shape)\n",
    "#             print('\\n# Decoder3')\n",
    "#             decode3 = self.decoder3(quant)\n",
    "            \n",
    "            \n",
    "        return decode3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "adaptive-barbados",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547342306,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "adaptive-barbados"
   },
   "outputs": [],
   "source": [
    "def gen_train_data_generator():\n",
    "    for i in range(train_dataset.__len__()):\n",
    "        data = np.squeeze(train_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(train_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)\n",
    "\n",
    "def gen_valid_data_generator():\n",
    "    for i in range(valid_dataset.__len__()):\n",
    "        data = np.squeeze(valid_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(valid_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-frequency",
   "metadata": {
    "id": "vocal-frequency"
   },
   "source": [
    "# 여기는 기존의 .fit() 함수를 사용해서 학습하는 부분임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "referenced-bangkok",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1367135,
     "status": "ok",
     "timestamp": 1641548715874,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "referenced-bangkok",
    "outputId": "86f0bdd5-75fb-4c6a-9bce-d47078edd458",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "(1, 1160, 40)\n",
      "# Encoder1\n",
      "conv1d  (1, 580, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (1, 290, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (1, 145, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (1, 145, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (1, 290, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (1, 580, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (1, 580, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d  (1, 1160, 40)\n",
      "Epoch 1/5\n",
      "(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (None, None, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d  (None, None, 40)\n",
      "Tensor(\"truediv_3:0\", shape=(), dtype=float32)\n",
      "(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (None, None, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d  (None, None, 40)\n",
      "Tensor(\"truediv_3:0\", shape=(), dtype=float32)\n",
      "1913/1923 [============================>.] - ETA: 0s - loss: 19.7885 - Si-sdr: 21.1210(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (None, None, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (None, None, 40)\n",
      "Tensor(\"truediv_3:0\", shape=(), dtype=float32)\n",
      "1923/1923 [==============================] - 14s 6ms/step - loss: 19.6900 - Si-sdr: 21.1629 - val_loss: 1.0312 - val_Si-sdr: 27.0577\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.03124, saving model to ./CKPT\\CKP_ep_1__loss_1.03124_.h5\n",
      "Epoch 2/5\n",
      "1923/1923 [==============================] - 10s 5ms/step - loss: 0.8639 - Si-sdr: 30.4011 - val_loss: 0.3680 - val_Si-sdr: 31.4210\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.03124 to 0.36796, saving model to ./CKPT\\CKP_ep_2__loss_0.36796_.h5\n",
      "Epoch 3/5\n",
      "1923/1923 [==============================] - 10s 5ms/step - loss: 1.2273 - Si-sdr: 30.8012 - val_loss: 0.2681 - val_Si-sdr: 32.9533\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36796 to 0.26813, saving model to ./CKPT\\CKP_ep_3__loss_0.26813_.h5\n",
      "Epoch 4/5\n",
      "1923/1923 [==============================] - 10s 5ms/step - loss: 1.1819 - Si-sdr: 31.4792 - val_loss: 2.6593 - val_Si-sdr: 23.4374\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26813\n",
      "Epoch 5/5\n",
      "1923/1923 [==============================] - 10s 5ms/step - loss: 0.9551 - Si-sdr: 31.9020 - val_loss: 0.3381 - val_Si-sdr: 31.9489\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26813\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "epoch = 5\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['/cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_594__loss_229.89435_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(embedding_dim, num_embeddings, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "#     vq_vae(0, True)\n",
    "#     vq_vae.summary()\n",
    "    \n",
    "#     사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "    \n",
    "history = vq_vae.fit_generator(\n",
    "generator=train_dataset,\n",
    "validation_data=valid_dataset,\n",
    "epochs=epoch,\n",
    "use_multiprocessing=False,\n",
    "shuffle=True,\n",
    "callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7461738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sisdr = history.history['Si-sdr'] # Training loss.\n",
    "val_sisdr = history.history['val_Si-sdr'] # Validation loss.\n",
    "num_epochs = range(1, 1 + len(history.history['Si-sdr'])) # Number of training epochs.\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(num_epochs, sisdr, label='Training Si-sdr') # Plot training loss.\n",
    "plt.plot(num_epochs, val_sisdr, label='Validation Si-sdr') # Plot validation loss.\n",
    "\n",
    "plt.title('Training and validation Si-sdr')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b726db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sisdr32 = history.history['Si-sdr']\n",
    "val_sisdr32= history.history['val_Si-sdr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {
    "id": "amber-overhead"
   },
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-architect",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "error",
     "timestamp": 1641453129862,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "annoying-architect",
    "outputId": "2f6b0d49-da97-48b5-8d53-e9fb91969713"
   },
   "outputs": [],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, num_embeddings, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {
    "id": "19cf7a69"
   },
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3c06eb91",
   "metadata": {
    "id": "3c06eb91"
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create .//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2e73b6a2",
   "metadata": {
    "id": "2e73b6a2"
   },
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "64be7f5d",
   "metadata": {
    "id": "64be7f5d"
   },
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a7cc186e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "error",
     "timestamp": 1641453176295,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "a7cc186e",
    "outputId": "eca7491b-132c-4df3-dd9e-0263dae19bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 40)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (None, None, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d  (None, None, 40)\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder1 (Encoder)           (None, None, 128)         20608     \n",
      "_________________________________________________________________\n",
      "encoder2 (Encoder)           (None, None, 64)          32832     \n",
      "_________________________________________________________________\n",
      "encoder3 (Encoder)           (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "vector_quantizer_14 (VectorQ (None, None, 64)          32768     \n",
      "_________________________________________________________________\n",
      "decoder1 (Decoder)           (None, None, 64)          16448     \n",
      "_________________________________________________________________\n",
      "decoder2 (Decoder)           (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "decoder3 (Decoder)           (None, None, 40)          41000     \n",
      "=================================================================\n",
      "Total params: 193,000\n",
      "Trainable params: 193,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(None, 672, 40)\n",
      "# Encoder1\n",
      "conv1d  (None, 336, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, 168, 64)\n",
      "\n",
      "# Encoder3\n",
      "conv1d  (None, 84, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder3  (None, 84, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, 168, 64)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, 336, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder2  (None, 336, 256)\n",
      "\n",
      "# Decoder3\n",
      "transconv1d  (None, 672, 40)\n",
      "WARNING:tensorflow:6 out of the last 552 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000224BD478510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 26880, 1)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    embedding_dim = 64\n",
    "    num_embeddings = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_3__loss_0.26813_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(embedding_dim, num_embeddings, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        print(type(result))\n",
    "        result = overlab_add(result).numpy()\n",
    "        print(type(result))\n",
    "        print(result.shape)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)\n",
    "        \n",
    "        break # 이거 지우면 전체 테스트 셋에 대해서 오디오를 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {
    "id": "7745f214"
   },
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-notebook",
   "metadata": {
    "id": "temporal-notebook"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=20, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-highlight",
   "metadata": {
    "id": "baking-highlight",
    "outputId": "58f8e3dc-7afd-443f-d7cf-a54911883f35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 9.221684, 10.636035], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(output_array, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-circular",
   "metadata": {
    "id": "separate-circular",
    "outputId": "1c43ebfa-8049-4f80-bc60-923564a4f038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.05127785,  0.04667316,  0.02635628, ...,  0.04573939,\n",
       "          0.01375185, -0.01533533],\n",
       "        [ 0.01121127,  0.06675138,  0.0021627 , ...,  0.07490488,\n",
       "          0.07613068,  0.02180689]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.08263874,  0.03258345,  0.01568549, ...,  0.03589562,\n",
       "         -0.01625197, -0.03177397],\n",
       "        [ 0.13691738,  0.09397386, -0.01036285, ..., -0.00614451,\n",
       "          0.05317175, -0.01284541]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07028106,  0.00527185,  0.01043324, ...,  0.0617263 ,\n",
       "         -0.03244079, -0.01786789],\n",
       "        [ 0.10520705,  0.042405  ,  0.03726472, ...,  0.07816655,\n",
       "          0.03663039, -0.03122897]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07235897,  0.00636261, -0.03309729, ...,  0.05743181,\n",
       "         -0.01674554, -0.01104611],\n",
       "        [ 0.11989237,  0.04727669,  0.04978402, ...,  0.10254725,\n",
       "         -0.0322747 , -0.04379546]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.06034716,  0.02196031, -0.03286755, ...,  0.04720719,\n",
       "          0.02011653,  0.01457734],\n",
       "        [ 0.08640987,  0.05295723, -0.05331329, ...,  0.05878888,\n",
       "         -0.11385743, -0.11239739]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.03044085,  0.01922614, -0.01506157, ...,  0.0429484 ,\n",
       "          0.03525994,  0.01746666],\n",
       "        [ 0.1105442 ,  0.06415009, -0.11616933, ...,  0.12055975,\n",
       "         -0.0501071 , -0.02813609]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 5.68038262e-02,  5.34826368e-02,  1.02059479e-04, ...,\n",
       "          2.20218338e-02,  5.25824353e-02,  1.43096512e-02],\n",
       "        [ 1.03822105e-01,  5.07594347e-02, -1.07106149e-01, ...,\n",
       "          1.95540398e-01,  1.40878245e-01,  3.50491852e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[-0.01148595,  0.03309207, -0.02289026, ...,  0.02078854,\n",
       "         -0.06813245, -0.09970599],\n",
       "        [ 0.04621019,  0.06871783,  0.00798892, ...,  0.01603857,\n",
       "          0.04825678,  0.01381727]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07415138,  0.0991153 , -0.03032039, ...,  0.01240078,\n",
       "         -0.04231954, -0.04340101],\n",
       "        [ 0.0583101 ,  0.0205835 , -0.01229584, ..., -0.01952541,\n",
       "          0.00030562, -0.05487346]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.02075784,  0.00618587, -0.07708566, ...,  0.10809869,\n",
       "          0.01363952,  0.02574195],\n",
       "        [-0.00304264, -0.00650905, -0.00512165, ...,  0.01797911,\n",
       "         -0.00719647, -0.02386911]], dtype=float32)>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(output_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba32a5d",
   "metadata": {
    "id": "4ba32a5d",
    "outputId": "5ef9064e-4583-4776-f190-a3e7f0d15979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.05127785  0.04667316  0.02635628 ...  0.04573939  0.01375185\n",
      "   -0.01533533]\n",
      "  [ 0.08263874  0.03258345  0.01568549 ...  0.03589562 -0.01625197\n",
      "   -0.03177397]\n",
      "  [ 0.07028106  0.00527185  0.01043324 ...  0.0617263  -0.03244079\n",
      "   -0.01786789]\n",
      "  ...\n",
      "  [-0.01148595  0.03309207 -0.02289026 ...  0.02078854 -0.06813245\n",
      "   -0.09970599]\n",
      "  [ 0.07415138  0.0991153  -0.03032039 ...  0.01240078 -0.04231954\n",
      "   -0.04340101]\n",
      "  [ 0.02075784  0.00618587 -0.07708566 ...  0.10809869  0.01363952\n",
      "    0.02574195]]\n",
      "\n",
      " [[ 0.01121127  0.06675138  0.0021627  ...  0.07490488  0.07613068\n",
      "    0.02180689]\n",
      "  [ 0.13691738  0.09397386 -0.01036285 ... -0.00614451  0.05317175\n",
      "   -0.01284541]\n",
      "  [ 0.10520705  0.042405    0.03726472 ...  0.07816655  0.03663039\n",
      "   -0.03122897]\n",
      "  ...\n",
      "  [ 0.04621019  0.06871783  0.00798892 ...  0.01603857  0.04825678\n",
      "    0.01381727]\n",
      "  [ 0.0583101   0.0205835  -0.01229584 ... -0.01952541  0.00030562\n",
      "   -0.05487346]\n",
      "  [-0.00304264 -0.00650905 -0.00512165 ...  0.01797911 -0.00719647\n",
      "   -0.02386911]]]\n",
      "(2, 10, 512)\n",
      "(2, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-dispatch",
   "metadata": {
    "id": "backed-dispatch",
    "outputId": "903333a9-a2c2-4a47-fded-0e3941b1a314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[259 259  34  50   0 110   0 305 200 287]\n",
      " [257   0 200 259  34 200 509 110 200 287]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype), axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)\n",
    "# layers.Embedding(512, 512)(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-coupon",
   "metadata": {
    "id": "liable-coupon",
    "outputId": "504f0d56-afee-4014-d8e6-b85117827ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[228 249 283 206  20 206 435  32 270  30]\n",
      " [428 206  20 244 357 289 324 249 498 134]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(output_array, axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-trading",
   "metadata": {
    "id": "bacterial-trading",
    "outputId": "3de03c4a-928a-4b5a-eba2-50fe561b5321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.012074914, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "qy = tf.nn.softmax(output_array)\n",
    "log_qy = tf.math.log(qy + 1e-10)\n",
    "log_uniform = qy * (log_qy - tf.math.log(1.0 / 512))\n",
    "kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8e699",
   "metadata": {
    "id": "fbc8e699",
    "outputId": "238a3c2b-5691-4eca-e8ed-7acd245d470e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n",
      "tf.Tensor(2.8309882, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "noise = output_array2 - target\n",
    "si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "si_sdr = tf.reduce_mean(si_sdr)\n",
    "print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-drink",
   "metadata": {
    "id": "clean-drink",
    "outputId": "1c57884c-ed2f-4cd9-eb7f-791ceaee09a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f9e7c",
   "metadata": {
    "id": "153f9e7c",
    "outputId": "814970f9-a907-474c-f4b1-6ec3445d138c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[-0.03009652, -0.03612775, -0.06680483, -0.03670201],\n",
       "        [-0.04768711, -0.12344762, -0.03924457, -0.11762322],\n",
       "        [ 0.01808495, -0.16106637, -0.19467078, -0.15282159],\n",
       "        [-0.0986427 , -0.08625205, -0.12661007, -0.16366175],\n",
       "        [-0.09758376, -0.08886974, -0.0433558 , -0.19985165],\n",
       "        [-0.06933096, -0.03154394, -0.13725929, -0.20143284],\n",
       "        [ 0.03375649,  0.00182091, -0.01022564, -0.35924646],\n",
       "        [-0.01645333, -0.10466891, -0.13975918, -0.12066491],\n",
       "        [-0.13588801, -0.08173112, -0.00253745, -0.28615874]],\n",
       "\n",
       "       [[ 0.04865369, -0.02880372, -0.06414615, -0.07730438],\n",
       "        [-0.08225074, -0.03192509, -0.06216412, -0.08035193],\n",
       "        [-0.09515338,  0.04221668,  0.14230826, -0.23082384],\n",
       "        [-0.00094383,  0.05597762, -0.09290768, -0.08630683],\n",
       "        [-0.09894791, -0.04727853, -0.01004983, -0.30325216],\n",
       "        [ 0.01705559, -0.16948727, -0.08829505, -0.16453639],\n",
       "        [-0.07230186, -0.15348263, -0.06832955, -0.09588489],\n",
       "        [-0.1258373 ,  0.02068143,  0.07559443, -0.19079593],\n",
       "        [-0.08558049, -0.07712768, -0.07924994, -0.07352428]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044d110",
   "metadata": {
    "id": "5044d110",
    "outputId": "45895281-ca9c-4fb8-c823-d0dac1afe7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4122a51",
   "metadata": {
    "id": "c4122a51",
    "outputId": "d9fedcb5-b498-4c12-ada8-f3f5ab813904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31fcf6",
   "metadata": {
    "id": "1d31fcf6",
    "outputId": "f19a2c99-c184-4df2-cf6e-95078bf110ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c293c1",
   "metadata": {
    "id": "01c293c1",
    "outputId": "0aaa46e4-44e8-40d1-8d8e-7eef008a5664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703002f",
   "metadata": {
    "id": "7703002f",
    "outputId": "2b2ee4ed-0bc6-49f7-c43e-e109c44d3c1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-coral",
   "metadata": {
    "id": "constitutional-coral",
    "outputId": "18feced3-6818-4fa7-badb-06330d71e01e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-distinction",
   "metadata": {
    "id": "prompt-distinction",
    "outputId": "b52d84f6-5c22-4780-9fca-a37770195961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-flash",
   "metadata": {
    "id": "treated-flash",
    "outputId": "3d37a8f1-53f9-4a9f-cf62-a15e4b1861e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-citizen",
   "metadata": {
    "id": "fifth-citizen",
    "outputId": "7efecbef-fb5e-4f4a-8be6-66413cf6d84a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {
    "id": "dac75a02"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7745f214"
   ],
   "name": "Engineering_vq-vae_for_1d_data_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
