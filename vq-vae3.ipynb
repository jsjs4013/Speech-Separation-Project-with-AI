{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad , exp#, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-8): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs)\n",
    "        gumbel = self.gumbel(encode)\n",
    "        decode = self.decoder(gumbel)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-8)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_10 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - 13s 7s/step - loss: 752.4975 - Si-sdr: -60.1861 - val_loss: 686.4703 - val_Si-sdr: -59.2822\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 619.74805\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 660.0311 - Si-sdr: -55.7755 - val_loss: 644.1715 - val_Si-sdr: -58.4167\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 619.74805\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 646.4332 - Si-sdr: -58.5283 - val_loss: 650.0015 - val_Si-sdr: -59.9821\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 619.74805\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 644.3671 - Si-sdr: -72.9323 - val_loss: 633.5944 - val_Si-sdr: -58.7370\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 619.74805\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 632.4194 - Si-sdr: -52.9936 - val_loss: 633.9359 - val_Si-sdr: -48.5238\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 619.74805\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 635.7373 - Si-sdr: -47.2113 - val_loss: 636.8821 - val_Si-sdr: -44.7777\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 619.74805\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 635.7107 - Si-sdr: -46.8456 - val_loss: 630.7891 - val_Si-sdr: -41.7221\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 619.74805\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 629.4974 - Si-sdr: -39.7545 - val_loss: 628.0092 - val_Si-sdr: -34.4656\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 619.74805\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 628.2498 - Si-sdr: -34.6416 - val_loss: 628.2256 - val_Si-sdr: -31.9893\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 619.74805\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 627.9285 - Si-sdr: -30.7223 - val_loss: 626.5044 - val_Si-sdr: -26.9269\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 619.74805\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 625.0856 - Si-sdr: -24.9492 - val_loss: 621.2245 - val_Si-sdr: -19.6155\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 619.74805\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 619.3551 - Si-sdr: -17.3386 - val_loss: 612.4089 - val_Si-sdr: -12.2296\n",
      "\n",
      "Epoch 00012: val_loss improved from 619.74805 to 612.40887, saving model to ./CKPT\\CKP_ep_12__loss_612.40887_.h5\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 608.2740 - Si-sdr: -10.4491 - val_loss: 594.9966 - val_Si-sdr: -6.8300\n",
      "\n",
      "Epoch 00013: val_loss improved from 612.40887 to 594.99658, saving model to ./CKPT\\CKP_ep_13__loss_594.99658_.h5\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 586.3142 - Si-sdr: -5.8544 - val_loss: 556.8434 - val_Si-sdr: -3.2600\n",
      "\n",
      "Epoch 00014: val_loss improved from 594.99658 to 556.84344, saving model to ./CKPT\\CKP_ep_14__loss_556.84344_.h5\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 539.6095 - Si-sdr: -3.1191 - val_loss: 492.8746 - val_Si-sdr: -2.9468\n",
      "\n",
      "Epoch 00015: val_loss improved from 556.84344 to 492.87463, saving model to ./CKPT\\CKP_ep_15__loss_492.87463_.h5\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 472.8008 - Si-sdr: -2.1600 - val_loss: 415.8585 - val_Si-sdr: -1.9257\n",
      "\n",
      "Epoch 00016: val_loss improved from 492.87463 to 415.85852, saving model to ./CKPT\\CKP_ep_16__loss_415.85852_.h5\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 395.9409 - Si-sdr: -2.0034 - val_loss: 372.2086 - val_Si-sdr: -1.6690\n",
      "\n",
      "Epoch 00017: val_loss improved from 415.85852 to 372.20856, saving model to ./CKPT\\CKP_ep_17__loss_372.20856_.h5\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 374.1346 - Si-sdr: -1.4989 - val_loss: 375.0024 - val_Si-sdr: -1.1499\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 372.20856\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 359.7844 - Si-sdr: -0.9159 - val_loss: 322.4318 - val_Si-sdr: -0.3058\n",
      "\n",
      "Epoch 00019: val_loss improved from 372.20856 to 322.43176, saving model to ./CKPT\\CKP_ep_19__loss_322.43176_.h5\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 312.0168 - Si-sdr: 0.0987 - val_loss: 305.9833 - val_Si-sdr: 0.6154\n",
      "\n",
      "Epoch 00020: val_loss improved from 322.43176 to 305.98328, saving model to ./CKPT\\CKP_ep_20__loss_305.98328_.h5\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 303.6197 - Si-sdr: 0.6486 - val_loss: 272.9866 - val_Si-sdr: 1.5672\n",
      "\n",
      "Epoch 00021: val_loss improved from 305.98328 to 272.98657, saving model to ./CKPT\\CKP_ep_21__loss_272.98657_.h5\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 261.2507 - Si-sdr: 1.5387 - val_loss: 238.4739 - val_Si-sdr: 2.0979\n",
      "\n",
      "Epoch 00022: val_loss improved from 272.98657 to 238.47391, saving model to ./CKPT\\CKP_ep_22__loss_238.47391_.h5\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 237.2706 - Si-sdr: 2.2675 - val_loss: 220.5052 - val_Si-sdr: 2.8880\n",
      "\n",
      "Epoch 00023: val_loss improved from 238.47391 to 220.50523, saving model to ./CKPT\\CKP_ep_23__loss_220.50523_.h5\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 209.3102 - Si-sdr: 3.1741 - val_loss: 186.9618 - val_Si-sdr: 3.7523\n",
      "\n",
      "Epoch 00024: val_loss improved from 220.50523 to 186.96178, saving model to ./CKPT\\CKP_ep_24__loss_186.96178_.h5\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 186.8692 - Si-sdr: 3.9053 - val_loss: 176.4088 - val_Si-sdr: 4.3902\n",
      "\n",
      "Epoch 00025: val_loss improved from 186.96178 to 176.40881, saving model to ./CKPT\\CKP_ep_25__loss_176.40881_.h5\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 169.8997 - Si-sdr: 4.5752 - val_loss: 149.6103 - val_Si-sdr: 5.0720\n",
      "\n",
      "Epoch 00026: val_loss improved from 176.40881 to 149.61032, saving model to ./CKPT\\CKP_ep_26__loss_149.61032_.h5\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 151.0586 - Si-sdr: 5.0646 - val_loss: 143.5043 - val_Si-sdr: 5.3305\n",
      "\n",
      "Epoch 00027: val_loss improved from 149.61032 to 143.50430, saving model to ./CKPT\\CKP_ep_27__loss_143.50430_.h5\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 141.0128 - Si-sdr: 5.3997 - val_loss: 138.0187 - val_Si-sdr: 5.4826\n",
      "\n",
      "Epoch 00028: val_loss improved from 143.50430 to 138.01868, saving model to ./CKPT\\CKP_ep_28__loss_138.01868_.h5\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 136.1016 - Si-sdr: 5.6371 - val_loss: 126.2933 - val_Si-sdr: 5.9476\n",
      "\n",
      "Epoch 00029: val_loss improved from 138.01868 to 126.29326, saving model to ./CKPT\\CKP_ep_29__loss_126.29326_.h5\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 125.8688 - Si-sdr: 6.0381 - val_loss: 127.7521 - val_Si-sdr: 6.0630\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 126.29326\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 122.6675 - Si-sdr: 6.2003 - val_loss: 115.0088 - val_Si-sdr: 6.4532\n",
      "\n",
      "Epoch 00031: val_loss improved from 126.29326 to 115.00878, saving model to ./CKPT\\CKP_ep_31__loss_115.00878_.h5\n",
      "Epoch 32/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 8s 5s/step - loss: 114.8168 - Si-sdr: 6.5179 - val_loss: 110.1330 - val_Si-sdr: 6.6855\n",
      "\n",
      "Epoch 00032: val_loss improved from 115.00878 to 110.13300, saving model to ./CKPT\\CKP_ep_32__loss_110.13300_.h5\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 108.2927 - Si-sdr: 6.7360 - val_loss: 105.2470 - val_Si-sdr: 6.9967\n",
      "\n",
      "Epoch 00033: val_loss improved from 110.13300 to 105.24696, saving model to ./CKPT\\CKP_ep_33__loss_105.24696_.h5\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 102.3685 - Si-sdr: 7.1257 - val_loss: 98.2765 - val_Si-sdr: 7.2410\n",
      "\n",
      "Epoch 00034: val_loss improved from 105.24696 to 98.27646, saving model to ./CKPT\\CKP_ep_34__loss_98.27646_.h5\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 98.8632 - Si-sdr: 7.2961 - val_loss: 98.3646 - val_Si-sdr: 7.4200\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 98.27646\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 96.5387 - Si-sdr: 7.4998 - val_loss: 90.6874 - val_Si-sdr: 7.6632\n",
      "\n",
      "Epoch 00036: val_loss improved from 98.27646 to 90.68739, saving model to ./CKPT\\CKP_ep_36__loss_90.68739_.h5\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 92.2359 - Si-sdr: 7.5847 - val_loss: 88.8555 - val_Si-sdr: 7.7755\n",
      "\n",
      "Epoch 00037: val_loss improved from 90.68739 to 88.85553, saving model to ./CKPT\\CKP_ep_37__loss_88.85553_.h5\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 88.7526 - Si-sdr: 7.7857 - val_loss: 86.9693 - val_Si-sdr: 7.9334\n",
      "\n",
      "Epoch 00038: val_loss improved from 88.85553 to 86.96925, saving model to ./CKPT\\CKP_ep_38__loss_86.96925_.h5\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 85.1895 - Si-sdr: 8.0303 - val_loss: 83.5257 - val_Si-sdr: 8.0364\n",
      "\n",
      "Epoch 00039: val_loss improved from 86.96925 to 83.52572, saving model to ./CKPT\\CKP_ep_39__loss_83.52572_.h5\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 83.6219 - Si-sdr: 8.0698 - val_loss: 81.0453 - val_Si-sdr: 8.2081\n",
      "\n",
      "Epoch 00040: val_loss improved from 83.52572 to 81.04535, saving model to ./CKPT\\CKP_ep_40__loss_81.04535_.h5\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 81.1510 - Si-sdr: 8.2220 - val_loss: 79.5743 - val_Si-sdr: 8.3295\n",
      "\n",
      "Epoch 00041: val_loss improved from 81.04535 to 79.57431, saving model to ./CKPT\\CKP_ep_41__loss_79.57431_.h5\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 79.6754 - Si-sdr: 8.2788 - val_loss: 78.2592 - val_Si-sdr: 8.3892\n",
      "\n",
      "Epoch 00042: val_loss improved from 79.57431 to 78.25919, saving model to ./CKPT\\CKP_ep_42__loss_78.25919_.h5\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 78.6755 - Si-sdr: 8.4217 - val_loss: 77.2363 - val_Si-sdr: 8.4428\n",
      "\n",
      "Epoch 00043: val_loss improved from 78.25919 to 77.23634, saving model to ./CKPT\\CKP_ep_43__loss_77.23634_.h5\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 75.6803 - Si-sdr: 8.6143 - val_loss: 75.2591 - val_Si-sdr: 8.5623\n",
      "\n",
      "Epoch 00044: val_loss improved from 77.23634 to 75.25912, saving model to ./CKPT\\CKP_ep_44__loss_75.25912_.h5\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 74.9112 - Si-sdr: 8.6152 - val_loss: 76.2456 - val_Si-sdr: 8.5572\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 75.25912\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 74.4280 - Si-sdr: 8.6766 - val_loss: 73.5263 - val_Si-sdr: 8.6898\n",
      "\n",
      "Epoch 00046: val_loss improved from 75.25912 to 73.52632, saving model to ./CKPT\\CKP_ep_46__loss_73.52632_.h5\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 73.4182 - Si-sdr: 8.6606 - val_loss: 73.4807 - val_Si-sdr: 8.7145\n",
      "\n",
      "Epoch 00047: val_loss improved from 73.52632 to 73.48068, saving model to ./CKPT\\CKP_ep_47__loss_73.48068_.h5\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 72.2554 - Si-sdr: 8.8117 - val_loss: 71.8026 - val_Si-sdr: 8.8520\n",
      "\n",
      "Epoch 00048: val_loss improved from 73.48068 to 71.80262, saving model to ./CKPT\\CKP_ep_48__loss_71.80262_.h5\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 72.0392 - Si-sdr: 8.8822 - val_loss: 72.4429 - val_Si-sdr: 8.8408\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 71.80262\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 70.9054 - Si-sdr: 8.8691 - val_loss: 69.7838 - val_Si-sdr: 8.9507\n",
      "\n",
      "Epoch 00050: val_loss improved from 71.80262 to 69.78377, saving model to ./CKPT\\CKP_ep_50__loss_69.78377_.h5\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 70.7073 - Si-sdr: 8.8812 - val_loss: 67.7062 - val_Si-sdr: 9.0667\n",
      "\n",
      "Epoch 00051: val_loss improved from 69.78377 to 67.70618, saving model to ./CKPT\\CKP_ep_51__loss_67.70618_.h5\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 67.8095 - Si-sdr: 9.0709 - val_loss: 65.9653 - val_Si-sdr: 9.2177\n",
      "\n",
      "Epoch 00052: val_loss improved from 67.70618 to 65.96531, saving model to ./CKPT\\CKP_ep_52__loss_65.96531_.h5\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 65.4920 - Si-sdr: 9.2731 - val_loss: 64.9836 - val_Si-sdr: 9.2388\n",
      "\n",
      "Epoch 00053: val_loss improved from 65.96531 to 64.98358, saving model to ./CKPT\\CKP_ep_53__loss_64.98358_.h5\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 64.6119 - Si-sdr: 9.3031 - val_loss: 63.7800 - val_Si-sdr: 9.3607\n",
      "\n",
      "Epoch 00054: val_loss improved from 64.98358 to 63.78004, saving model to ./CKPT\\CKP_ep_54__loss_63.78004_.h5\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 63.8551 - Si-sdr: 9.4250 - val_loss: 62.5144 - val_Si-sdr: 9.4595\n",
      "\n",
      "Epoch 00055: val_loss improved from 63.78004 to 62.51439, saving model to ./CKPT\\CKP_ep_55__loss_62.51439_.h5\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 62.1790 - Si-sdr: 9.4782 - val_loss: 61.3460 - val_Si-sdr: 9.5712\n",
      "\n",
      "Epoch 00056: val_loss improved from 62.51439 to 61.34597, saving model to ./CKPT\\CKP_ep_56__loss_61.34597_.h5\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 61.4521 - Si-sdr: 9.5657 - val_loss: 61.0023 - val_Si-sdr: 9.5809\n",
      "\n",
      "Epoch 00057: val_loss improved from 61.34597 to 61.00228, saving model to ./CKPT\\CKP_ep_57__loss_61.00228_.h5\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 60.5019 - Si-sdr: 9.5705 - val_loss: 59.4551 - val_Si-sdr: 9.6973\n",
      "\n",
      "Epoch 00058: val_loss improved from 61.00228 to 59.45509, saving model to ./CKPT\\CKP_ep_58__loss_59.45509_.h5\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 59.8355 - Si-sdr: 9.6666 - val_loss: 58.1011 - val_Si-sdr: 9.8297\n",
      "\n",
      "Epoch 00059: val_loss improved from 59.45509 to 58.10106, saving model to ./CKPT\\CKP_ep_59__loss_58.10106_.h5\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 58.5408 - Si-sdr: 9.7683 - val_loss: 57.5225 - val_Si-sdr: 9.8448\n",
      "\n",
      "Epoch 00060: val_loss improved from 58.10106 to 57.52248, saving model to ./CKPT\\CKP_ep_60__loss_57.52248_.h5\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 57.0618 - Si-sdr: 9.9108 - val_loss: 56.1931 - val_Si-sdr: 9.9460\n",
      "\n",
      "Epoch 00061: val_loss improved from 57.52248 to 56.19310, saving model to ./CKPT\\CKP_ep_61__loss_56.19310_.h5\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 55.5384 - Si-sdr: 10.0306 - val_loss: 54.6548 - val_Si-sdr: 10.0467\n",
      "\n",
      "Epoch 00062: val_loss improved from 56.19310 to 54.65482, saving model to ./CKPT\\CKP_ep_62__loss_54.65482_.h5\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 54.1409 - Si-sdr: 10.1178 - val_loss: 53.9191 - val_Si-sdr: 10.1575\n",
      "\n",
      "Epoch 00063: val_loss improved from 54.65482 to 53.91908, saving model to ./CKPT\\CKP_ep_63__loss_53.91908_.h5\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 53.3039 - Si-sdr: 10.1912 - val_loss: 54.2636 - val_Si-sdr: 10.1109\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 53.91908\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 53.1171 - Si-sdr: 10.2207 - val_loss: 52.3498 - val_Si-sdr: 10.3092\n",
      "\n",
      "Epoch 00065: val_loss improved from 53.91908 to 52.34985, saving model to ./CKPT\\CKP_ep_65__loss_52.34985_.h5\n",
      "Epoch 66/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 9s 5s/step - loss: 51.6749 - Si-sdr: 10.3580 - val_loss: 51.6227 - val_Si-sdr: 10.3567\n",
      "\n",
      "Epoch 00066: val_loss improved from 52.34985 to 51.62273, saving model to ./CKPT\\CKP_ep_66__loss_51.62273_.h5\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 51.0420 - Si-sdr: 10.4106 - val_loss: 49.8303 - val_Si-sdr: 10.5057\n",
      "\n",
      "Epoch 00067: val_loss improved from 51.62273 to 49.83034, saving model to ./CKPT\\CKP_ep_67__loss_49.83034_.h5\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 49.5238 - Si-sdr: 10.5400 - val_loss: 49.5135 - val_Si-sdr: 10.5340\n",
      "\n",
      "Epoch 00068: val_loss improved from 49.83034 to 49.51345, saving model to ./CKPT\\CKP_ep_68__loss_49.51345_.h5\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 48.7194 - Si-sdr: 10.6453 - val_loss: 48.8410 - val_Si-sdr: 10.6333\n",
      "\n",
      "Epoch 00069: val_loss improved from 49.51345 to 48.84103, saving model to ./CKPT\\CKP_ep_69__loss_48.84103_.h5\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 48.0791 - Si-sdr: 10.6891 - val_loss: 46.7757 - val_Si-sdr: 10.8017\n",
      "\n",
      "Epoch 00070: val_loss improved from 48.84103 to 46.77575, saving model to ./CKPT\\CKP_ep_70__loss_46.77575_.h5\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 47.1421 - Si-sdr: 10.7442 - val_loss: 46.6073 - val_Si-sdr: 10.8017\n",
      "\n",
      "Epoch 00071: val_loss improved from 46.77575 to 46.60735, saving model to ./CKPT\\CKP_ep_71__loss_46.60735_.h5\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 46.8524 - Si-sdr: 10.7853 - val_loss: 46.9318 - val_Si-sdr: 10.8022\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 46.60735\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 45.7062 - Si-sdr: 10.9284 - val_loss: 44.9559 - val_Si-sdr: 10.9834\n",
      "\n",
      "Epoch 00073: val_loss improved from 46.60735 to 44.95594, saving model to ./CKPT\\CKP_ep_73__loss_44.95594_.h5\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 45.3479 - Si-sdr: 11.0121 - val_loss: 43.5718 - val_Si-sdr: 11.1240\n",
      "\n",
      "Epoch 00074: val_loss improved from 44.95594 to 43.57182, saving model to ./CKPT\\CKP_ep_74__loss_43.57182_.h5\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 43.3263 - Si-sdr: 11.1492 - val_loss: 42.8607 - val_Si-sdr: 11.1893\n",
      "\n",
      "Epoch 00075: val_loss improved from 43.57182 to 42.86073, saving model to ./CKPT\\CKP_ep_75__loss_42.86073_.h5\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 43.6534 - Si-sdr: 11.1257 - val_loss: 43.0972 - val_Si-sdr: 11.1907\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 42.86073\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 42.5440 - Si-sdr: 11.2614 - val_loss: 42.4191 - val_Si-sdr: 11.3022\n",
      "\n",
      "Epoch 00077: val_loss improved from 42.86073 to 42.41907, saving model to ./CKPT\\CKP_ep_77__loss_42.41907_.h5\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 41.6446 - Si-sdr: 11.3551 - val_loss: 43.6578 - val_Si-sdr: 11.2146\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 42.41907\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 41.6003 - Si-sdr: 11.4428 - val_loss: 43.9015 - val_Si-sdr: 11.1987\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 42.41907\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 42.7810 - Si-sdr: 11.2820 - val_loss: 41.3143 - val_Si-sdr: 11.4379\n",
      "\n",
      "Epoch 00080: val_loss improved from 42.41907 to 41.31435, saving model to ./CKPT\\CKP_ep_80__loss_41.31435_.h5\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 41.1710 - Si-sdr: 11.4438 - val_loss: 40.0953 - val_Si-sdr: 11.5706\n",
      "\n",
      "Epoch 00081: val_loss improved from 41.31435 to 40.09528, saving model to ./CKPT\\CKP_ep_81__loss_40.09528_.h5\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 39.5898 - Si-sdr: 11.5947 - val_loss: 39.6809 - val_Si-sdr: 11.5993\n",
      "\n",
      "Epoch 00082: val_loss improved from 40.09528 to 39.68086, saving model to ./CKPT\\CKP_ep_82__loss_39.68086_.h5\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 40.5541 - Si-sdr: 11.5134 - val_loss: 40.7615 - val_Si-sdr: 11.6648\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 39.68086\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 39.1863 - Si-sdr: 11.7852 - val_loss: 39.4710 - val_Si-sdr: 11.7566\n",
      "\n",
      "Epoch 00084: val_loss improved from 39.68086 to 39.47102, saving model to ./CKPT\\CKP_ep_84__loss_39.47102_.h5\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 37.8153 - Si-sdr: 11.8595 - val_loss: 37.6076 - val_Si-sdr: 11.9486\n",
      "\n",
      "Epoch 00085: val_loss improved from 39.47102 to 37.60756, saving model to ./CKPT\\CKP_ep_85__loss_37.60756_.h5\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 37.0579 - Si-sdr: 11.9814 - val_loss: 36.6852 - val_Si-sdr: 11.9823\n",
      "\n",
      "Epoch 00086: val_loss improved from 37.60756 to 36.68521, saving model to ./CKPT\\CKP_ep_86__loss_36.68521_.h5\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 36.8214 - Si-sdr: 11.9443 - val_loss: 34.9817 - val_Si-sdr: 12.1115\n",
      "\n",
      "Epoch 00087: val_loss improved from 36.68521 to 34.98172, saving model to ./CKPT\\CKP_ep_87__loss_34.98172_.h5\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 35.0879 - Si-sdr: 12.0993 - val_loss: 35.4835 - val_Si-sdr: 12.0658\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 34.98172\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 35.2906 - Si-sdr: 12.1337 - val_loss: 34.5987 - val_Si-sdr: 12.2175\n",
      "\n",
      "Epoch 00089: val_loss improved from 34.98172 to 34.59873, saving model to ./CKPT\\CKP_ep_89__loss_34.59873_.h5\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 34.2007 - Si-sdr: 12.2295 - val_loss: 34.1385 - val_Si-sdr: 12.2668\n",
      "\n",
      "Epoch 00090: val_loss improved from 34.59873 to 34.13854, saving model to ./CKPT\\CKP_ep_90__loss_34.13854_.h5\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 33.7906 - Si-sdr: 12.2749 - val_loss: 33.3877 - val_Si-sdr: 12.3690\n",
      "\n",
      "Epoch 00091: val_loss improved from 34.13854 to 33.38775, saving model to ./CKPT\\CKP_ep_91__loss_33.38775_.h5\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 33.2204 - Si-sdr: 12.3735 - val_loss: 32.6914 - val_Si-sdr: 12.4497\n",
      "\n",
      "Epoch 00092: val_loss improved from 33.38775 to 32.69144, saving model to ./CKPT\\CKP_ep_92__loss_32.69144_.h5\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 32.7086 - Si-sdr: 12.4373 - val_loss: 32.4547 - val_Si-sdr: 12.4881\n",
      "\n",
      "Epoch 00093: val_loss improved from 32.69144 to 32.45469, saving model to ./CKPT\\CKP_ep_93__loss_32.45469_.h5\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 32.5220 - Si-sdr: 12.4842 - val_loss: 31.8144 - val_Si-sdr: 12.5885\n",
      "\n",
      "Epoch 00094: val_loss improved from 32.45469 to 31.81438, saving model to ./CKPT\\CKP_ep_94__loss_31.81438_.h5\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 31.9761 - Si-sdr: 12.5596 - val_loss: 32.2324 - val_Si-sdr: 12.5412\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 31.81438\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 31.8969 - Si-sdr: 12.5661 - val_loss: 32.3625 - val_Si-sdr: 12.5557\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 31.81438\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 31.9571 - Si-sdr: 12.6442 - val_loss: 31.4990 - val_Si-sdr: 12.7147\n",
      "\n",
      "Epoch 00097: val_loss improved from 31.81438 to 31.49899, saving model to ./CKPT\\CKP_ep_97__loss_31.49899_.h5\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 31.3238 - Si-sdr: 12.7893 - val_loss: 29.7016 - val_Si-sdr: 12.8760\n",
      "\n",
      "Epoch 00098: val_loss improved from 31.49899 to 29.70157, saving model to ./CKPT\\CKP_ep_98__loss_29.70157_.h5\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 31.5509 - Si-sdr: 12.7792 - val_loss: 29.7108 - val_Si-sdr: 12.9099\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 29.70157\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 30.7686 - Si-sdr: 12.8764 - val_loss: 29.5124 - val_Si-sdr: 12.9217\n",
      "\n",
      "Epoch 00100: val_loss improved from 29.70157 to 29.51239, saving model to ./CKPT\\CKP_ep_100__loss_29.51239_.h5\n",
      "Epoch 101/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 9s 5s/step - loss: 30.0199 - Si-sdr: 12.9013 - val_loss: 29.5561 - val_Si-sdr: 12.9833\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 29.51239\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 29.1878 - Si-sdr: 12.9706 - val_loss: 29.3225 - val_Si-sdr: 12.9985\n",
      "\n",
      "Epoch 00102: val_loss improved from 29.51239 to 29.32246, saving model to ./CKPT\\CKP_ep_102__loss_29.32246_.h5\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 29.3299 - Si-sdr: 12.9891 - val_loss: 29.0089 - val_Si-sdr: 13.0460\n",
      "\n",
      "Epoch 00103: val_loss improved from 29.32246 to 29.00892, saving model to ./CKPT\\CKP_ep_103__loss_29.00892_.h5\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 29.1943 - Si-sdr: 13.0273 - val_loss: 28.4286 - val_Si-sdr: 13.0825\n",
      "\n",
      "Epoch 00104: val_loss improved from 29.00892 to 28.42862, saving model to ./CKPT\\CKP_ep_104__loss_28.42862_.h5\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 29.7791 - Si-sdr: 12.9752 - val_loss: 27.9959 - val_Si-sdr: 13.1666\n",
      "\n",
      "Epoch 00105: val_loss improved from 28.42862 to 27.99593, saving model to ./CKPT\\CKP_ep_105__loss_27.99593_.h5\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 28.2789 - Si-sdr: 13.1494 - val_loss: 27.9980 - val_Si-sdr: 13.2136\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 27.99593\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 28.2834 - Si-sdr: 13.1894 - val_loss: 30.1594 - val_Si-sdr: 13.1050\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 27.99593\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 28.8880 - Si-sdr: 13.1702 - val_loss: 30.0726 - val_Si-sdr: 13.2170\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 27.99593\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 28.7031 - Si-sdr: 13.2787 - val_loss: 28.6406 - val_Si-sdr: 13.3019\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 27.99593\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 28.3164 - Si-sdr: 13.2932 - val_loss: 27.1071 - val_Si-sdr: 13.3595\n",
      "\n",
      "Epoch 00110: val_loss improved from 27.99593 to 27.10705, saving model to ./CKPT\\CKP_ep_110__loss_27.10705_.h5\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 28.8773 - Si-sdr: 13.2900 - val_loss: 26.6979 - val_Si-sdr: 13.3915\n",
      "\n",
      "Epoch 00111: val_loss improved from 27.10705 to 26.69794, saving model to ./CKPT\\CKP_ep_111__loss_26.69794_.h5\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 27.5201 - Si-sdr: 13.3916 - val_loss: 27.5682 - val_Si-sdr: 13.4201\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 26.69794\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 27.1991 - Si-sdr: 13.4648 - val_loss: 26.6750 - val_Si-sdr: 13.4637\n",
      "\n",
      "Epoch 00113: val_loss improved from 26.69794 to 26.67501, saving model to ./CKPT\\CKP_ep_113__loss_26.67501_.h5\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 26.0489 - Si-sdr: 13.5145 - val_loss: 26.6323 - val_Si-sdr: 13.5290\n",
      "\n",
      "Epoch 00114: val_loss improved from 26.67501 to 26.63226, saving model to ./CKPT\\CKP_ep_114__loss_26.63226_.h5\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 26.1476 - Si-sdr: 13.5312 - val_loss: 25.7915 - val_Si-sdr: 13.6142\n",
      "\n",
      "Epoch 00115: val_loss improved from 26.63226 to 25.79153, saving model to ./CKPT\\CKP_ep_115__loss_25.79153_.h5\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 26.6785 - Si-sdr: 13.6056 - val_loss: 25.9809 - val_Si-sdr: 13.4837\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 25.79153\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 26.1044 - Si-sdr: 13.5807 - val_loss: 25.9674 - val_Si-sdr: 13.6317\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 25.79153\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 25.3580 - Si-sdr: 13.6302 - val_loss: 27.3810 - val_Si-sdr: 13.6795\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 25.79153\n",
      "Epoch 119/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 26.6210 - Si-sdr: 13.6557 - val_loss: 26.1511 - val_Si-sdr: 13.6845\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 25.79153\n",
      "Epoch 120/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 26.0265 - Si-sdr: 13.7195 - val_loss: 24.8272 - val_Si-sdr: 13.7518\n",
      "\n",
      "Epoch 00120: val_loss improved from 25.79153 to 24.82716, saving model to ./CKPT\\CKP_ep_120__loss_24.82716_.h5\n",
      "Epoch 121/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 25.5863 - Si-sdr: 13.7074 - val_loss: 24.3424 - val_Si-sdr: 13.7767\n",
      "\n",
      "Epoch 00121: val_loss improved from 24.82716 to 24.34243, saving model to ./CKPT\\CKP_ep_121__loss_24.34243_.h5\n",
      "Epoch 122/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 25.4464 - Si-sdr: 13.5892 - val_loss: 26.2028 - val_Si-sdr: 13.7715\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 24.34243\n",
      "Epoch 123/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 25.2177 - Si-sdr: 13.7811 - val_loss: 25.9139 - val_Si-sdr: 13.7614\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 24.34243\n",
      "Epoch 124/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 25.5370 - Si-sdr: 13.7912 - val_loss: 24.1907 - val_Si-sdr: 13.8895\n",
      "\n",
      "Epoch 00124: val_loss improved from 24.34243 to 24.19072, saving model to ./CKPT\\CKP_ep_124__loss_24.19072_.h5\n",
      "Epoch 125/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 25.3291 - Si-sdr: 13.8152 - val_loss: 23.5916 - val_Si-sdr: 13.9486\n",
      "\n",
      "Epoch 00125: val_loss improved from 24.19072 to 23.59161, saving model to ./CKPT\\CKP_ep_125__loss_23.59161_.h5\n",
      "Epoch 126/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 23.8605 - Si-sdr: 13.8952 - val_loss: 24.6303 - val_Si-sdr: 13.8359\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 23.59161\n",
      "Epoch 127/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 23.7977 - Si-sdr: 13.9103 - val_loss: 24.4140 - val_Si-sdr: 13.8447\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 23.59161\n",
      "Epoch 128/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 24.2040 - Si-sdr: 13.8643 - val_loss: 22.7249 - val_Si-sdr: 14.1056\n",
      "\n",
      "Epoch 00128: val_loss improved from 23.59161 to 22.72486, saving model to ./CKPT\\CKP_ep_128__loss_22.72486_.h5\n",
      "Epoch 129/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 22.8084 - Si-sdr: 14.0845 - val_loss: 22.8672 - val_Si-sdr: 14.0717\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 22.72486\n",
      "Epoch 130/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 23.0856 - Si-sdr: 14.0982 - val_loss: 22.8289 - val_Si-sdr: 14.1206\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 22.72486\n",
      "Epoch 131/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 22.5578 - Si-sdr: 14.1471 - val_loss: 22.5948 - val_Si-sdr: 14.1447\n",
      "\n",
      "Epoch 00131: val_loss improved from 22.72486 to 22.59475, saving model to ./CKPT\\CKP_ep_131__loss_22.59475_.h5\n",
      "Epoch 132/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 22.3935 - Si-sdr: 14.1908 - val_loss: 21.9697 - val_Si-sdr: 14.2431\n",
      "\n",
      "Epoch 00132: val_loss improved from 22.59475 to 21.96968, saving model to ./CKPT\\CKP_ep_132__loss_21.96968_.h5\n",
      "Epoch 133/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 22.2923 - Si-sdr: 14.2618 - val_loss: 21.8889 - val_Si-sdr: 14.2819\n",
      "\n",
      "Epoch 00133: val_loss improved from 21.96968 to 21.88894, saving model to ./CKPT\\CKP_ep_133__loss_21.88894_.h5\n",
      "Epoch 134/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 22.2755 - Si-sdr: 14.2625 - val_loss: 22.4371 - val_Si-sdr: 14.2708\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 21.88894\n",
      "Epoch 135/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 22.2622 - Si-sdr: 14.3176 - val_loss: 22.9840 - val_Si-sdr: 14.1789\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 21.88894\n",
      "Epoch 136/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 22.7325 - Si-sdr: 14.2234 - val_loss: 21.4580 - val_Si-sdr: 14.3961\n",
      "\n",
      "Epoch 00136: val_loss improved from 21.88894 to 21.45797, saving model to ./CKPT\\CKP_ep_136__loss_21.45797_.h5\n",
      "Epoch 137/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 23.0359 - Si-sdr: 14.3059 - val_loss: 21.6153 - val_Si-sdr: 14.3104\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 21.45797\n",
      "Epoch 138/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 9s 5s/step - loss: 22.3072 - Si-sdr: 14.2769 - val_loss: 22.4385 - val_Si-sdr: 14.3118\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 21.45797\n",
      "Epoch 139/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 21.8652 - Si-sdr: 14.2876 - val_loss: 21.5409 - val_Si-sdr: 14.4248\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 21.45797\n",
      "Epoch 140/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 21.7546 - Si-sdr: 14.3746 - val_loss: 21.1814 - val_Si-sdr: 14.4456\n",
      "\n",
      "Epoch 00140: val_loss improved from 21.45797 to 21.18142, saving model to ./CKPT\\CKP_ep_140__loss_21.18142_.h5\n",
      "Epoch 141/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 21.6353 - Si-sdr: 14.3940 - val_loss: 21.1338 - val_Si-sdr: 14.4077\n",
      "\n",
      "Epoch 00141: val_loss improved from 21.18142 to 21.13377, saving model to ./CKPT\\CKP_ep_141__loss_21.13377_.h5\n",
      "Epoch 142/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 20.9825 - Si-sdr: 14.4688 - val_loss: 20.8523 - val_Si-sdr: 14.4880\n",
      "\n",
      "Epoch 00142: val_loss improved from 21.13377 to 20.85234, saving model to ./CKPT\\CKP_ep_142__loss_20.85234_.h5\n",
      "Epoch 143/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 20.7807 - Si-sdr: 14.5364 - val_loss: 21.2046 - val_Si-sdr: 14.4230\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 20.85234\n",
      "Epoch 144/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 20.8393 - Si-sdr: 14.4955 - val_loss: 20.4931 - val_Si-sdr: 14.5990\n",
      "\n",
      "Epoch 00144: val_loss improved from 20.85234 to 20.49308, saving model to ./CKPT\\CKP_ep_144__loss_20.49308_.h5\n",
      "Epoch 145/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 20.7959 - Si-sdr: 14.5375 - val_loss: 20.5717 - val_Si-sdr: 14.5517\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 20.49308\n",
      "Epoch 146/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 20.4467 - Si-sdr: 14.5871 - val_loss: 20.4417 - val_Si-sdr: 14.5824\n",
      "\n",
      "Epoch 00146: val_loss improved from 20.49308 to 20.44168, saving model to ./CKPT\\CKP_ep_146__loss_20.44168_.h5\n",
      "Epoch 147/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 20.2558 - Si-sdr: 14.6526 - val_loss: 20.6049 - val_Si-sdr: 14.5680\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 20.44168\n",
      "Epoch 148/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 20.2368 - Si-sdr: 14.6162 - val_loss: 19.8188 - val_Si-sdr: 14.7475\n",
      "\n",
      "Epoch 00148: val_loss improved from 20.44168 to 19.81876, saving model to ./CKPT\\CKP_ep_148__loss_19.81876_.h5\n",
      "Epoch 149/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 20.1539 - Si-sdr: 14.6930 - val_loss: 19.6777 - val_Si-sdr: 14.7394\n",
      "\n",
      "Epoch 00149: val_loss improved from 19.81876 to 19.67775, saving model to ./CKPT\\CKP_ep_149__loss_19.67775_.h5\n",
      "Epoch 150/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 20.1087 - Si-sdr: 14.6937 - val_loss: 19.8967 - val_Si-sdr: 14.7188\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 19.67775\n",
      "Epoch 151/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 20.1714 - Si-sdr: 14.6986 - val_loss: 20.2436 - val_Si-sdr: 14.7537\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 19.67775\n",
      "Epoch 152/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 19.9026 - Si-sdr: 14.7557 - val_loss: 20.3668 - val_Si-sdr: 14.6895\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 19.67775\n",
      "Epoch 153/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 20.0666 - Si-sdr: 14.7377 - val_loss: 19.5252 - val_Si-sdr: 14.8088\n",
      "\n",
      "Epoch 00153: val_loss improved from 19.67775 to 19.52518, saving model to ./CKPT\\CKP_ep_153__loss_19.52518_.h5\n",
      "Epoch 154/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 19.5135 - Si-sdr: 14.7937 - val_loss: 19.3290 - val_Si-sdr: 14.8094\n",
      "\n",
      "Epoch 00154: val_loss improved from 19.52518 to 19.32905, saving model to ./CKPT\\CKP_ep_154__loss_19.32905_.h5\n",
      "Epoch 155/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 19.3226 - Si-sdr: 14.8337 - val_loss: 19.7416 - val_Si-sdr: 14.7503\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 19.32905\n",
      "Epoch 156/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 19.2201 - Si-sdr: 14.8625 - val_loss: 19.0516 - val_Si-sdr: 14.9073\n",
      "\n",
      "Epoch 00156: val_loss improved from 19.32905 to 19.05161, saving model to ./CKPT\\CKP_ep_156__loss_19.05161_.h5\n",
      "Epoch 157/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 19.2659 - Si-sdr: 14.9026 - val_loss: 18.8981 - val_Si-sdr: 14.9179\n",
      "\n",
      "Epoch 00157: val_loss improved from 19.05161 to 18.89809, saving model to ./CKPT\\CKP_ep_157__loss_18.89809_.h5\n",
      "Epoch 158/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 19.1562 - Si-sdr: 14.8882 - val_loss: 19.1043 - val_Si-sdr: 14.9321\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 18.89809\n",
      "Epoch 159/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 19.2794 - Si-sdr: 14.9512 - val_loss: 19.2944 - val_Si-sdr: 14.9346\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 18.89809\n",
      "Epoch 160/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 19.1544 - Si-sdr: 14.9340 - val_loss: 18.9001 - val_Si-sdr: 14.9748\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 18.89809\n",
      "Epoch 161/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 19.1005 - Si-sdr: 14.9083 - val_loss: 18.4578 - val_Si-sdr: 15.0353\n",
      "\n",
      "Epoch 00161: val_loss improved from 18.89809 to 18.45779, saving model to ./CKPT\\CKP_ep_161__loss_18.45779_.h5\n",
      "Epoch 162/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 18.6026 - Si-sdr: 14.9992 - val_loss: 18.9538 - val_Si-sdr: 14.9429\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 18.45779\n",
      "Epoch 163/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 18.8561 - Si-sdr: 14.9847 - val_loss: 18.6694 - val_Si-sdr: 15.0505\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 18.45779\n",
      "Epoch 164/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 18.7872 - Si-sdr: 14.9839 - val_loss: 18.6569 - val_Si-sdr: 15.0669\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 18.45779\n",
      "Epoch 165/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 18.8393 - Si-sdr: 15.0245 - val_loss: 18.2119 - val_Si-sdr: 15.0914\n",
      "\n",
      "Epoch 00165: val_loss improved from 18.45779 to 18.21191, saving model to ./CKPT\\CKP_ep_165__loss_18.21191_.h5\n",
      "Epoch 166/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 18.6030 - Si-sdr: 14.9944 - val_loss: 18.3039 - val_Si-sdr: 15.0757\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 18.21191\n",
      "Epoch 167/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 18.1082 - Si-sdr: 15.1199 - val_loss: 18.5737 - val_Si-sdr: 15.0330\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 18.21191\n",
      "Epoch 168/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 18.7029 - Si-sdr: 14.9655 - val_loss: 18.0957 - val_Si-sdr: 15.1294\n",
      "\n",
      "Epoch 00168: val_loss improved from 18.21191 to 18.09572, saving model to ./CKPT\\CKP_ep_168__loss_18.09572_.h5\n",
      "Epoch 169/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 18.5039 - Si-sdr: 15.0523 - val_loss: 20.1525 - val_Si-sdr: 14.6385\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 18.09572\n",
      "Epoch 170/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 21.3545 - Si-sdr: 14.4143 - val_loss: 19.7438 - val_Si-sdr: 14.7214\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 18.09572\n",
      "Epoch 171/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 19.4677 - Si-sdr: 14.8761 - val_loss: 20.4927 - val_Si-sdr: 14.6417\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 18.09572\n",
      "Epoch 172/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 21.1100 - Si-sdr: 14.4688 - val_loss: 18.7202 - val_Si-sdr: 14.9949\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 18.09572\n",
      "Epoch 173/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 18.7482 - Si-sdr: 14.9726 - val_loss: 18.5892 - val_Si-sdr: 15.0043\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 18.09572\n",
      "Epoch 174/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 18.8089 - Si-sdr: 14.9606 - val_loss: 18.4146 - val_Si-sdr: 15.0539\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 18.09572\n",
      "Epoch 175/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 18.8981 - Si-sdr: 14.9680 - val_loss: 18.2835 - val_Si-sdr: 15.1222\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 18.09572\n",
      "Epoch 176/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 18.2076 - Si-sdr: 15.1215 - val_loss: 18.0047 - val_Si-sdr: 15.1440\n",
      "\n",
      "Epoch 00176: val_loss improved from 18.09572 to 18.00467, saving model to ./CKPT\\CKP_ep_176__loss_18.00467_.h5\n",
      "Epoch 177/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 18.0602 - Si-sdr: 15.1343 - val_loss: 17.6399 - val_Si-sdr: 15.2442\n",
      "\n",
      "Epoch 00177: val_loss improved from 18.00467 to 17.63990, saving model to ./CKPT\\CKP_ep_177__loss_17.63990_.h5\n",
      "Epoch 178/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 17.8814 - Si-sdr: 15.1704 - val_loss: 17.6587 - val_Si-sdr: 15.2217\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 17.63990\n",
      "Epoch 179/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 17.7251 - Si-sdr: 15.2041 - val_loss: 17.7734 - val_Si-sdr: 15.2102\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 17.63990\n",
      "Epoch 180/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 17.6549 - Si-sdr: 15.2332 - val_loss: 17.1913 - val_Si-sdr: 15.3427\n",
      "\n",
      "Epoch 00180: val_loss improved from 17.63990 to 17.19134, saving model to ./CKPT\\CKP_ep_180__loss_17.19134_.h5\n",
      "Epoch 181/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 17.4212 - Si-sdr: 15.2951 - val_loss: 17.0655 - val_Si-sdr: 15.3773\n",
      "\n",
      "Epoch 00181: val_loss improved from 17.19134 to 17.06550, saving model to ./CKPT\\CKP_ep_181__loss_17.06550_.h5\n",
      "Epoch 182/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 17.0720 - Si-sdr: 15.3789 - val_loss: 17.3398 - val_Si-sdr: 15.2792\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 17.06550\n",
      "Epoch 183/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 17.2560 - Si-sdr: 15.3220 - val_loss: 17.0046 - val_Si-sdr: 15.3837\n",
      "\n",
      "Epoch 00183: val_loss improved from 17.06550 to 17.00457, saving model to ./CKPT\\CKP_ep_183__loss_17.00457_.h5\n",
      "Epoch 184/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 17.1651 - Si-sdr: 15.3633 - val_loss: 17.0849 - val_Si-sdr: 15.3878\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 17.00457\n",
      "Epoch 185/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.8216 - Si-sdr: 15.4497 - val_loss: 16.8956 - val_Si-sdr: 15.4278\n",
      "\n",
      "Epoch 00185: val_loss improved from 17.00457 to 16.89556, saving model to ./CKPT\\CKP_ep_185__loss_16.89556_.h5\n",
      "Epoch 186/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 17.3066 - Si-sdr: 15.3411 - val_loss: 16.7811 - val_Si-sdr: 15.4579\n",
      "\n",
      "Epoch 00186: val_loss improved from 16.89556 to 16.78106, saving model to ./CKPT\\CKP_ep_186__loss_16.78106_.h5\n",
      "Epoch 187/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 16.9642 - Si-sdr: 15.4192 - val_loss: 16.7420 - val_Si-sdr: 15.4634\n",
      "\n",
      "Epoch 00187: val_loss improved from 16.78106 to 16.74199, saving model to ./CKPT\\CKP_ep_187__loss_16.74199_.h5\n",
      "Epoch 188/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.5611 - Si-sdr: 15.5156 - val_loss: 16.7133 - val_Si-sdr: 15.4997\n",
      "\n",
      "Epoch 00188: val_loss improved from 16.74199 to 16.71327, saving model to ./CKPT\\CKP_ep_188__loss_16.71327_.h5\n",
      "Epoch 189/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 16.4149 - Si-sdr: 15.5692 - val_loss: 16.3404 - val_Si-sdr: 15.5958\n",
      "\n",
      "Epoch 00189: val_loss improved from 16.71327 to 16.34045, saving model to ./CKPT\\CKP_ep_189__loss_16.34045_.h5\n",
      "Epoch 190/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 16.4235 - Si-sdr: 15.5565 - val_loss: 16.5547 - val_Si-sdr: 15.5159\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 16.34045\n",
      "Epoch 191/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 16.4081 - Si-sdr: 15.5662 - val_loss: 16.2652 - val_Si-sdr: 15.5920\n",
      "\n",
      "Epoch 00191: val_loss improved from 16.34045 to 16.26522, saving model to ./CKPT\\CKP_ep_191__loss_16.26522_.h5\n",
      "Epoch 192/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 16.0845 - Si-sdr: 15.6435 - val_loss: 16.0896 - val_Si-sdr: 15.6362\n",
      "\n",
      "Epoch 00192: val_loss improved from 16.26522 to 16.08960, saving model to ./CKPT\\CKP_ep_192__loss_16.08960_.h5\n",
      "Epoch 193/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.2874 - Si-sdr: 15.6184 - val_loss: 16.1438 - val_Si-sdr: 15.6359\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 16.08960\n",
      "Epoch 194/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.1938 - Si-sdr: 15.6225 - val_loss: 16.3026 - val_Si-sdr: 15.5941\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 16.08960\n",
      "Epoch 195/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.2301 - Si-sdr: 15.6443 - val_loss: 16.1287 - val_Si-sdr: 15.6319\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 16.08960\n",
      "Epoch 196/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 16.2761 - Si-sdr: 15.6419 - val_loss: 15.9167 - val_Si-sdr: 15.7014\n",
      "\n",
      "Epoch 00196: val_loss improved from 16.08960 to 15.91668, saving model to ./CKPT\\CKP_ep_196__loss_15.91668_.h5\n",
      "Epoch 197/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 16.1567 - Si-sdr: 15.7167 - val_loss: 16.3096 - val_Si-sdr: 15.6659\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 15.91668\n",
      "Epoch 198/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 16.4699 - Si-sdr: 15.6386 - val_loss: 16.4686 - val_Si-sdr: 15.6485\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 15.91668\n",
      "Epoch 199/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 16.3479 - Si-sdr: 15.6344 - val_loss: 16.4938 - val_Si-sdr: 15.6589\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 15.91668\n",
      "Epoch 200/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 16.1946 - Si-sdr: 15.7019 - val_loss: 15.7696 - val_Si-sdr: 15.7656\n",
      "\n",
      "Epoch 00200: val_loss improved from 15.91668 to 15.76955, saving model to ./CKPT\\CKP_ep_200__loss_15.76955_.h5\n",
      "Epoch 201/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 15.9266 - Si-sdr: 15.7401 - val_loss: 15.9975 - val_Si-sdr: 15.6692\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 15.76955\n",
      "Epoch 202/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 15.8985 - Si-sdr: 15.7265 - val_loss: 15.6265 - val_Si-sdr: 15.8047\n",
      "\n",
      "Epoch 00202: val_loss improved from 15.76955 to 15.62655, saving model to ./CKPT\\CKP_ep_202__loss_15.62655_.h5\n",
      "Epoch 203/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 15.7966 - Si-sdr: 15.7760 - val_loss: 15.8197 - val_Si-sdr: 15.7171\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 15.62655\n",
      "Epoch 204/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 15.5968 - Si-sdr: 15.7877 - val_loss: 15.8175 - val_Si-sdr: 15.7452\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 15.62655\n",
      "Epoch 205/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 15.7764 - Si-sdr: 15.7738 - val_loss: 15.5150 - val_Si-sdr: 15.8099\n",
      "\n",
      "Epoch 00205: val_loss improved from 15.62655 to 15.51499, saving model to ./CKPT\\CKP_ep_205__loss_15.51499_.h5\n",
      "Epoch 206/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.6544 - Si-sdr: 15.7762 - val_loss: 15.3868 - val_Si-sdr: 15.8563\n",
      "\n",
      "Epoch 00206: val_loss improved from 15.51499 to 15.38681, saving model to ./CKPT\\CKP_ep_206__loss_15.38681_.h5\n",
      "Epoch 207/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.5871 - Si-sdr: 15.8330 - val_loss: 15.5710 - val_Si-sdr: 15.8124\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 15.38681\n",
      "Epoch 208/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.4294 - Si-sdr: 15.8333 - val_loss: 15.6177 - val_Si-sdr: 15.8161\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 15.38681\n",
      "Epoch 209/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.5444 - Si-sdr: 15.8315 - val_loss: 15.3661 - val_Si-sdr: 15.8893\n",
      "\n",
      "Epoch 00209: val_loss improved from 15.38681 to 15.36607, saving model to ./CKPT\\CKP_ep_209__loss_15.36607_.h5\n",
      "Epoch 210/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.5024 - Si-sdr: 15.8269 - val_loss: 15.5995 - val_Si-sdr: 15.8155\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 15.36607\n",
      "Epoch 211/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.6214 - Si-sdr: 15.7998 - val_loss: 15.2876 - val_Si-sdr: 15.8588\n",
      "\n",
      "Epoch 00211: val_loss improved from 15.36607 to 15.28763, saving model to ./CKPT\\CKP_ep_211__loss_15.28763_.h5\n",
      "Epoch 212/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.3582 - Si-sdr: 15.8705 - val_loss: 15.1678 - val_Si-sdr: 15.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00212: val_loss improved from 15.28763 to 15.16775, saving model to ./CKPT\\CKP_ep_212__loss_15.16775_.h5\n",
      "Epoch 213/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.5373 - Si-sdr: 15.8249 - val_loss: 14.9541 - val_Si-sdr: 15.9763\n",
      "\n",
      "Epoch 00213: val_loss improved from 15.16775 to 14.95414, saving model to ./CKPT\\CKP_ep_213__loss_14.95414_.h5\n",
      "Epoch 214/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.5985 - Si-sdr: 15.8352 - val_loss: 15.3842 - val_Si-sdr: 15.8664\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 14.95414\n",
      "Epoch 215/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.3016 - Si-sdr: 15.9108 - val_loss: 15.0332 - val_Si-sdr: 15.9497\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 14.95414\n",
      "Epoch 216/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.0256 - Si-sdr: 15.9648 - val_loss: 14.9396 - val_Si-sdr: 15.9660\n",
      "\n",
      "Epoch 00216: val_loss improved from 14.95414 to 14.93959, saving model to ./CKPT\\CKP_ep_216__loss_14.93959_.h5\n",
      "Epoch 217/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 14.9802 - Si-sdr: 15.9504 - val_loss: 15.0134 - val_Si-sdr: 15.9703\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 14.93959\n",
      "Epoch 218/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.9795 - Si-sdr: 15.9826 - val_loss: 14.7941 - val_Si-sdr: 16.0056\n",
      "\n",
      "Epoch 00218: val_loss improved from 14.93959 to 14.79410, saving model to ./CKPT\\CKP_ep_218__loss_14.79410_.h5\n",
      "Epoch 219/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.1770 - Si-sdr: 15.9439 - val_loss: 15.1372 - val_Si-sdr: 15.9061\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 14.79410\n",
      "Epoch 220/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 14.8809 - Si-sdr: 15.9824 - val_loss: 15.5909 - val_Si-sdr: 15.8533\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 14.79410\n",
      "Epoch 221/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.5132 - Si-sdr: 15.8636 - val_loss: 15.0817 - val_Si-sdr: 15.9591\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 14.79410\n",
      "Epoch 222/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.4545 - Si-sdr: 15.9040 - val_loss: 15.2026 - val_Si-sdr: 15.9015\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 14.79410\n",
      "Epoch 223/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 15.8106 - Si-sdr: 15.8032 - val_loss: 15.7048 - val_Si-sdr: 15.8100\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 14.79410\n",
      "Epoch 224/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.4247 - Si-sdr: 15.9120 - val_loss: 15.9800 - val_Si-sdr: 15.8054\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 14.79410\n",
      "Epoch 225/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.7957 - Si-sdr: 15.7759 - val_loss: 15.5952 - val_Si-sdr: 15.7968\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 14.79410\n",
      "Epoch 226/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.3668 - Si-sdr: 15.8603 - val_loss: 15.2101 - val_Si-sdr: 15.9072\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 14.79410\n",
      "Epoch 227/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 15.2245 - Si-sdr: 15.8987 - val_loss: 15.5533 - val_Si-sdr: 15.7878\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 14.79410\n",
      "Epoch 228/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 15.4055 - Si-sdr: 15.8885 - val_loss: 14.8602 - val_Si-sdr: 16.0050\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 14.79410\n",
      "Epoch 229/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.4175 - Si-sdr: 15.8944 - val_loss: 15.2261 - val_Si-sdr: 15.8864\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 14.79410\n",
      "Epoch 230/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 15.3972 - Si-sdr: 15.8166 - val_loss: 14.7211 - val_Si-sdr: 16.0473\n",
      "\n",
      "Epoch 00230: val_loss improved from 14.79410 to 14.72108, saving model to ./CKPT\\CKP_ep_230__loss_14.72108_.h5\n",
      "Epoch 231/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.9833 - Si-sdr: 15.9694 - val_loss: 15.2802 - val_Si-sdr: 15.8672\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 14.72108\n",
      "Epoch 232/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 15.1118 - Si-sdr: 15.9650 - val_loss: 14.8102 - val_Si-sdr: 16.0254\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 14.72108\n",
      "Epoch 233/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.9550 - Si-sdr: 15.9919 - val_loss: 14.6049 - val_Si-sdr: 16.0766\n",
      "\n",
      "Epoch 00233: val_loss improved from 14.72108 to 14.60493, saving model to ./CKPT\\CKP_ep_233__loss_14.60493_.h5\n",
      "Epoch 234/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.7367 - Si-sdr: 16.0668 - val_loss: 14.7056 - val_Si-sdr: 16.0527\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 14.60493\n",
      "Epoch 235/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 14.6005 - Si-sdr: 16.0808 - val_loss: 14.3445 - val_Si-sdr: 16.1599\n",
      "\n",
      "Epoch 00235: val_loss improved from 14.60493 to 14.34452, saving model to ./CKPT\\CKP_ep_235__loss_14.34452_.h5\n",
      "Epoch 236/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.4251 - Si-sdr: 16.1243 - val_loss: 14.4277 - val_Si-sdr: 16.1282\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 14.34452\n",
      "Epoch 237/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.4200 - Si-sdr: 16.1651 - val_loss: 14.3018 - val_Si-sdr: 16.1835\n",
      "\n",
      "Epoch 00237: val_loss improved from 14.34452 to 14.30185, saving model to ./CKPT\\CKP_ep_237__loss_14.30185_.h5\n",
      "Epoch 238/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 14.2194 - Si-sdr: 16.2140 - val_loss: 14.2981 - val_Si-sdr: 16.1719\n",
      "\n",
      "Epoch 00238: val_loss improved from 14.30185 to 14.29810, saving model to ./CKPT\\CKP_ep_238__loss_14.29810_.h5\n",
      "Epoch 239/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.2968 - Si-sdr: 16.1777 - val_loss: 13.9959 - val_Si-sdr: 16.2636\n",
      "\n",
      "Epoch 00239: val_loss improved from 14.29810 to 13.99588, saving model to ./CKPT\\CKP_ep_239__loss_13.99588_.h5\n",
      "Epoch 240/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.9527 - Si-sdr: 16.2880 - val_loss: 14.1069 - val_Si-sdr: 16.2477\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 13.99588\n",
      "Epoch 241/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.9828 - Si-sdr: 16.3100 - val_loss: 14.0892 - val_Si-sdr: 16.2368\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 13.99588\n",
      "Epoch 242/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.9189 - Si-sdr: 16.2961 - val_loss: 13.6508 - val_Si-sdr: 16.3824\n",
      "\n",
      "Epoch 00242: val_loss improved from 13.99588 to 13.65084, saving model to ./CKPT\\CKP_ep_242__loss_13.65084_.h5\n",
      "Epoch 243/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.7277 - Si-sdr: 16.3471 - val_loss: 13.9009 - val_Si-sdr: 16.2962\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 13.65084\n",
      "Epoch 244/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.9680 - Si-sdr: 16.2911 - val_loss: 13.5324 - val_Si-sdr: 16.4133\n",
      "\n",
      "Epoch 00244: val_loss improved from 13.65084 to 13.53243, saving model to ./CKPT\\CKP_ep_244__loss_13.53243_.h5\n",
      "Epoch 245/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.9672 - Si-sdr: 16.3299 - val_loss: 14.3706 - val_Si-sdr: 16.1354\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 13.53243\n",
      "Epoch 246/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 14.7333 - Si-sdr: 16.0809 - val_loss: 13.8865 - val_Si-sdr: 16.3096\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 13.53243\n",
      "Epoch 247/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.1346 - Si-sdr: 16.2231 - val_loss: 15.0955 - val_Si-sdr: 15.9472\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 13.53243\n",
      "Epoch 248/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 14.5718 - Si-sdr: 16.0799 - val_loss: 13.8423 - val_Si-sdr: 16.3212\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 13.53243\n",
      "Epoch 249/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.3778 - Si-sdr: 16.1949 - val_loss: 13.8234 - val_Si-sdr: 16.3150\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 13.53243\n",
      "Epoch 250/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 14.1530 - Si-sdr: 16.3159 - val_loss: 14.5722 - val_Si-sdr: 16.0852\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 13.53243\n",
      "Epoch 251/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.5708 - Si-sdr: 16.1349 - val_loss: 13.9718 - val_Si-sdr: 16.2910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00251: val_loss did not improve from 13.53243\n",
      "Epoch 252/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.9810 - Si-sdr: 16.3122 - val_loss: 13.9082 - val_Si-sdr: 16.3245\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 13.53243\n",
      "Epoch 253/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.8345 - Si-sdr: 16.3577 - val_loss: 13.6144 - val_Si-sdr: 16.4052\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 13.53243\n",
      "Epoch 254/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.7487 - Si-sdr: 16.3632 - val_loss: 14.3818 - val_Si-sdr: 16.1254\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 13.53243\n",
      "Epoch 255/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 14.0560 - Si-sdr: 16.2311 - val_loss: 13.3835 - val_Si-sdr: 16.4592\n",
      "\n",
      "Epoch 00255: val_loss improved from 13.53243 to 13.38348, saving model to ./CKPT\\CKP_ep_255__loss_13.38348_.h5\n",
      "Epoch 256/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.8734 - Si-sdr: 16.2786 - val_loss: 13.8256 - val_Si-sdr: 16.3261\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 13.38348\n",
      "Epoch 257/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.6759 - Si-sdr: 16.3884 - val_loss: 13.4782 - val_Si-sdr: 16.4476\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 13.38348\n",
      "Epoch 258/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.7643 - Si-sdr: 16.4170 - val_loss: 13.7535 - val_Si-sdr: 16.3405\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 13.38348\n",
      "Epoch 259/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 14.0305 - Si-sdr: 16.2923 - val_loss: 13.5528 - val_Si-sdr: 16.4205\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 13.38348\n",
      "Epoch 260/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.7261 - Si-sdr: 16.4134 - val_loss: 14.0981 - val_Si-sdr: 16.3186\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 13.38348\n",
      "Epoch 261/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.5989 - Si-sdr: 16.4167 - val_loss: 13.6411 - val_Si-sdr: 16.4698\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 13.38348\n",
      "Epoch 262/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.5568 - Si-sdr: 16.4502 - val_loss: 13.3632 - val_Si-sdr: 16.4871\n",
      "\n",
      "Epoch 00262: val_loss improved from 13.38348 to 13.36317, saving model to ./CKPT\\CKP_ep_262__loss_13.36317_.h5\n",
      "Epoch 263/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.3803 - Si-sdr: 16.4923 - val_loss: 13.4503 - val_Si-sdr: 16.4496\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 13.36317\n",
      "Epoch 264/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.2187 - Si-sdr: 16.5196 - val_loss: 13.4171 - val_Si-sdr: 16.4501\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 13.36317\n",
      "Epoch 265/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.3504 - Si-sdr: 16.4835 - val_loss: 13.0593 - val_Si-sdr: 16.5827\n",
      "\n",
      "Epoch 00265: val_loss improved from 13.36317 to 13.05928, saving model to ./CKPT\\CKP_ep_265__loss_13.05928_.h5\n",
      "Epoch 266/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 13.1112 - Si-sdr: 16.5647 - val_loss: 13.1205 - val_Si-sdr: 16.5481\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 13.05928\n",
      "Epoch 267/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.3024 - Si-sdr: 16.4885 - val_loss: 12.8958 - val_Si-sdr: 16.6372\n",
      "\n",
      "Epoch 00267: val_loss improved from 13.05928 to 12.89584, saving model to ./CKPT\\CKP_ep_267__loss_12.89584_.h5\n",
      "Epoch 268/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 12.9801 - Si-sdr: 16.6065 - val_loss: 13.2015 - val_Si-sdr: 16.5410\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 12.89584\n",
      "Epoch 269/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.3680 - Si-sdr: 16.4883 - val_loss: 12.8399 - val_Si-sdr: 16.6602\n",
      "\n",
      "Epoch 00269: val_loss improved from 12.89584 to 12.83994, saving model to ./CKPT\\CKP_ep_269__loss_12.83994_.h5\n",
      "Epoch 270/300\n",
      "2/2 [==============================] - 9s 5s/step - loss: 13.0098 - Si-sdr: 16.5778 - val_loss: 12.7277 - val_Si-sdr: 16.6931\n",
      "\n",
      "Epoch 00270: val_loss improved from 12.83994 to 12.72766, saving model to ./CKPT\\CKP_ep_270__loss_12.72766_.h5\n",
      "Epoch 271/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.0079 - Si-sdr: 16.6252 - val_loss: 12.6605 - val_Si-sdr: 16.7145\n",
      "\n",
      "Epoch 00271: val_loss improved from 12.72766 to 12.66051, saving model to ./CKPT\\CKP_ep_271__loss_12.66051_.h5\n",
      "Epoch 272/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 12.6899 - Si-sdr: 16.7170 - val_loss: 12.7421 - val_Si-sdr: 16.6768\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 12.66051\n",
      "Epoch 273/300\n",
      "2/2 [==============================] - 9s 6s/step - loss: 12.7979 - Si-sdr: 16.7076 - val_loss: 12.5669 - val_Si-sdr: 16.7376\n",
      "\n",
      "Epoch 00273: val_loss improved from 12.66051 to 12.56686, saving model to ./CKPT\\CKP_ep_273__loss_12.56686_.h5\n",
      "Epoch 274/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.8194 - Si-sdr: 16.7082 - val_loss: 12.5705 - val_Si-sdr: 16.7486\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 12.56686\n",
      "Epoch 275/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.5516 - Si-sdr: 16.7411 - val_loss: 12.5060 - val_Si-sdr: 16.7898\n",
      "\n",
      "Epoch 00275: val_loss improved from 12.56686 to 12.50599, saving model to ./CKPT\\CKP_ep_275__loss_12.50599_.h5\n",
      "Epoch 276/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.7127 - Si-sdr: 16.7177 - val_loss: 12.6329 - val_Si-sdr: 16.7383\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 12.50599\n",
      "Epoch 277/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 12.5854 - Si-sdr: 16.7834 - val_loss: 12.4607 - val_Si-sdr: 16.7844\n",
      "\n",
      "Epoch 00277: val_loss improved from 12.50599 to 12.46074, saving model to ./CKPT\\CKP_ep_277__loss_12.46074_.h5\n",
      "Epoch 278/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 12.4546 - Si-sdr: 16.7855 - val_loss: 12.5892 - val_Si-sdr: 16.7548\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 12.46074\n",
      "Epoch 279/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.5470 - Si-sdr: 16.7687 - val_loss: 12.4492 - val_Si-sdr: 16.8062\n",
      "\n",
      "Epoch 00279: val_loss improved from 12.46074 to 12.44916, saving model to ./CKPT\\CKP_ep_279__loss_12.44916_.h5\n",
      "Epoch 280/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 12.5646 - Si-sdr: 16.7804 - val_loss: 12.4389 - val_Si-sdr: 16.7966\n",
      "\n",
      "Epoch 00280: val_loss improved from 12.44916 to 12.43894, saving model to ./CKPT\\CKP_ep_280__loss_12.43894_.h5\n",
      "Epoch 281/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.6733 - Si-sdr: 16.8034 - val_loss: 12.4968 - val_Si-sdr: 16.7832\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 12.43894\n",
      "Epoch 282/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 12.6490 - Si-sdr: 16.8485 - val_loss: 12.7906 - val_Si-sdr: 16.6568\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 12.43894\n",
      "Epoch 283/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.2109 - Si-sdr: 16.6059 - val_loss: 12.5122 - val_Si-sdr: 16.8029\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 12.43894\n",
      "Epoch 284/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.6340 - Si-sdr: 16.7302 - val_loss: 13.6164 - val_Si-sdr: 16.4847\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 12.43894\n",
      "Epoch 285/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.5537 - Si-sdr: 16.4564 - val_loss: 12.8110 - val_Si-sdr: 16.6973\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 12.43894\n",
      "Epoch 286/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.1992 - Si-sdr: 16.5359 - val_loss: 14.4530 - val_Si-sdr: 16.1125\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 12.43894\n",
      "Epoch 287/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 14.1549 - Si-sdr: 16.1965 - val_loss: 13.0944 - val_Si-sdr: 16.5680\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 12.43894\n",
      "Epoch 288/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.7376 - Si-sdr: 16.3653 - val_loss: 14.4924 - val_Si-sdr: 16.1184\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 12.43894\n",
      "Epoch 289/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.8135 - Si-sdr: 16.3038 - val_loss: 13.2956 - val_Si-sdr: 16.4957\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 12.43894\n",
      "Epoch 290/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.7808 - Si-sdr: 16.3264 - val_loss: 12.9756 - val_Si-sdr: 16.6051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00290: val_loss did not improve from 12.43894\n",
      "Epoch 291/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 12.9078 - Si-sdr: 16.6156 - val_loss: 13.4512 - val_Si-sdr: 16.4674\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 12.43894\n",
      "Epoch 292/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.3552 - Si-sdr: 16.4854 - val_loss: 12.8826 - val_Si-sdr: 16.6765\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 12.43894\n",
      "Epoch 293/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.0623 - Si-sdr: 16.5937 - val_loss: 13.1231 - val_Si-sdr: 16.5545\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 12.43894\n",
      "Epoch 294/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 12.9494 - Si-sdr: 16.6356 - val_loss: 13.1100 - val_Si-sdr: 16.5718\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 12.43894\n",
      "Epoch 295/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 13.2423 - Si-sdr: 16.5414 - val_loss: 12.4387 - val_Si-sdr: 16.8022\n",
      "\n",
      "Epoch 00295: val_loss improved from 12.43894 to 12.43871, saving model to ./CKPT\\CKP_ep_295__loss_12.43871_.h5\n",
      "Epoch 296/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.1265 - Si-sdr: 16.6504 - val_loss: 14.5632 - val_Si-sdr: 16.0870\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 12.43871\n",
      "Epoch 297/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.7215 - Si-sdr: 16.3707 - val_loss: 13.1175 - val_Si-sdr: 16.5760\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 12.43871\n",
      "Epoch 298/300\n",
      "2/2 [==============================] - 8s 5s/step - loss: 14.3022 - Si-sdr: 16.1480 - val_loss: 15.0939 - val_Si-sdr: 15.9401\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 12.43871\n",
      "Epoch 299/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 14.2520 - Si-sdr: 16.2112 - val_loss: 13.4362 - val_Si-sdr: 16.4243\n",
      "\n",
      "Epoch 00299: val_loss did not improve from 12.43871\n",
      "Epoch 300/300\n",
      "2/2 [==============================] - 8s 6s/step - loss: 13.1838 - Si-sdr: 16.5244 - val_loss: 14.2271 - val_Si-sdr: 16.2750\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 12.43871\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_12 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023DC399B5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000023DC399B5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_295__loss_12.43871_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar, length=None):\n",
    "    enc_padding_mask = tf.cast(tf.sequence_mask(tf.repeat(tf.shape(inp)[1],tf.shape(inp)[0]), tf.shape(inp)[1]), tf.float32)\n",
    "    dec_padding_mask = tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)\n",
    "    \n",
    "    return enc_padding_mask, dec_padding_mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cheap-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = vq_vae.encoder(next(iter(train_dataset))[0]).numpy()\n",
    "encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5000, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 5000, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = vq_vae.encoder(next(iter(train_dataset))[0]).numpy()\n",
    "target_onehot = tf.cast(tf.equal(target, tf.math.reduce_max(target, 2, keepdims=True)), target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e8de4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_onehot[0][100][324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target_onehot[0][0]\n",
    "a = tf.cast(a, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "785acc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5000, 2]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tf.shape(encode_onehot)[0].numpy(), tf.shape(encode_onehot)[1].numpy(), 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5044d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = tf.zeros([2,5000,2], a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.concat([tf.cast(target_onehot,tf.int32), zeros], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5000, 512), dtype=int32, numpy=\n",
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]])>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[:,:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encode_onehot, decoder_input, next(iter(train_dataset))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5001, 514), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "start = tf.one_hot(tf.repeat([512], tf.shape(encode_onehot)[0]),tf.shape(encode_onehot)[-1]+2)\n",
    "start = tf.expand_dims(start, 1)\n",
    "zeros = tf.zeros([tf.shape(encode_onehot)[0].numpy(), tf.shape(encode_onehot)[1].numpy(), 2], encode_onehot.dtype)\n",
    "encode_added = tf.concat([encode_onehot, zeros], -1)\n",
    "\n",
    "tf.concat([start, encode_added],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 56000, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000,), dtype=float32, numpy=array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_padding_mask[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 5000), dtype=float32, numpy=\n",
       " array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(5000, 5000), dtype=float32, numpy=\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 1., 1., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 0., 0.],\n",
       "        [1., 1., 1., ..., 1., 1., 0.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(encode_onehot, decoder_input, next(iter(train_dataset))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([5000, 5000])>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.repeat(tf.shape(encode_onehot)[1],tf.shape(encode_onehot)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaab3f0",
   "metadata": {},
   "source": [
    "# 여기서부터 Transformer 임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4fbbc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.util.math_function import create_padding_mask, create_look_ahead_mask\n",
    "from src.losses.custom_loss import mse_with_proper_loss, MSE_Custom_Loss_No_Length, pit_with_outputsize, pit_with_stft_trace\n",
    "from src.models.Layers import TransformerSpeechSep\n",
    "from src.models.Schedulers import CustomSchedule\n",
    "from src.models.Real_Layers import T5Model, T5ModelNoMaskCreationModel, T5ModelYesMaskCreationModel\n",
    "from src.pre_processing.data_pre_processing import load_data\n",
    "import tensorflow_addons as tfa\n",
    "from collections import namedtuple\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "7792969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Config = namedtuple('Config',  \n",
    "    field_names=\"d_ff,     d_kv,     d_model,              dropout, feed_forward_proj, num_layers, init_factor,\" \n",
    "                \"layer_norm_epsilon, model_type, num_heads, positional_embedding, n_epochs, vocab_size, relative_attention_num_buckets,\"\n",
    "                    \"model_path, wav_type, size_type, train_type, loss_type, learning_rate_type,\"\n",
    "                    \"input_size, output_size, batch_size, case, ckpt_path, tr_path, val_path, tt_path,\"\n",
    "                    \"test_wav_dir, is_load_model\")\n",
    "args = Config( 1024      , 64      , 512              , 0.1 , \"gated-gelu\", 1       , 1.,\n",
    "                1e-06    , \"t5\"             , 8 , \"absolute\" , 5     , 512   , 32,\n",
    "                \"CKPT\", \"wav8k\", \"min\", \"train-360\", \"mse\", \"inverse_root\",\n",
    "                512, 514, 25, 'trace', 'C:/J_and_J_Research/CKPT/gen_code2', \n",
    "                'C:/J_and_J_Research/mycode/wsj0_2mix/use_this/tr/', \n",
    "                'C:/J_and_J_Research/mycode/wsj0_2mix/use_this/cv/',\n",
    "                'C:/J_and_J_Research/mycode/wsj0_2mix/use_this/tt/', \n",
    "                'C:/J_and_J_Research/test_wav/gen2',\n",
    "                True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d5a6205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_addons in c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages (from tensorflow_addons) (2.13.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.1.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the 'c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cd0c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar, length=None):\n",
    "    enc_padding_mask = tf.cast(tf.sequence_mask(tf.repeat(tf.shape(inp)[1],tf.shape(inp)[0]), tf.shape(inp)[1]), tf.float32)\n",
    "    #dec_padding_mask = tf.linalg.band_part(tf.ones((tf.shape(tar)[1], tf.shape(tar)[1])), -1, 0)\n",
    "    \n",
    "    return enc_padding_mask#, dec_padding_mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ff50a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5VQ_VAE(tf.keras.Model):\n",
    "    def train_step(self, data):\n",
    "        \"\"\"print('inp',inp.shape) \n",
    "        startMask = tf.cast(tf.fill([1,258],-1),dtype=tf.float32)\n",
    "        endMask = tf.cast(tf.fill([1,258],-2),dtype=tf.float32)\n",
    "        tar_inp = tf.concat([startMask, tar],0)\n",
    "        tar_real = tf.concat([tar, endMask],0)\n",
    "        \"\"\"\n",
    "        inp, tar, length = data\n",
    "        \"\"\"start = tf.repeat([tf.shape(inp)[-1]], tf.shape(inp)[0])\n",
    "        start = tf.cast(tf.one_hot(start,tf.shape(inp)[-1]+2),dtype=tf.float32)\n",
    "        start = tf.expand_dims(start, 1)\n",
    "        end = tf.repeat([tf.shape(inp)[-1]+1], tf.shape(inp)[0])\n",
    "        end = tf.cast(tf.one_hot(end,tf.shape(inp)[-1]+2),dtype=tf.float32)\n",
    "        end = tf.expand_dims(end, 1)\n",
    "        \n",
    "        tar = tf.concat([start, tar],1)\n",
    "        tar = tf.concat([tar, end],1)\"\"\"\n",
    "\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "        \n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = self((inp, tar_inp, length), training=True)\n",
    "            \n",
    "            loss = self.compiled_loss(tar_real, prediction, regularization_losses=self.losses)\n",
    "        \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        self.compiled_metrics.update_state(tar_real, prediction)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "        #train_accuracy(accuracy_function(tar_real, predictions))\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        inp, tar, length = data\n",
    "        \"\"\"start = tf.repeat([tf.shape(inp)[-1]], tf.shape(inp)[0])\n",
    "        start = tf.cast(tf.one_hot(start,tf.shape(inp)[-1]+2),dtype=tf.float32)\n",
    "        start = tf.expand_dims(start, 1)\n",
    "        end = tf.repeat([tf.shape(inp)[-1]+1], tf.shape(inp)[0])\n",
    "        end = tf.cast(tf.one_hot(end,tf.shape(inp)[-1]+2),dtype=tf.float32)\n",
    "        end = tf.expand_dims(end, 1)\n",
    "        \n",
    "        tar = tf.concat([start, tar],1)\n",
    "        tar = tf.concat([tar, end],1)\"\"\"\n",
    "\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        predictions = self((inp, tar_inp, length), training=False)\n",
    "\n",
    "        # Updates stateful loss metrics.\n",
    "        self.compiled_loss(tar_real, predictions, regularization_losses=self.losses)\n",
    "\n",
    "        self.compiled_metrics.update_state(tar_real, predictions)\n",
    "        # Collect metrics to return\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "        return_metrics = {}\n",
    "        for metric in self.metrics:\n",
    "            result = metric.result()\n",
    "            if isinstance(result, dict):\n",
    "                return_metrics.update(result)\n",
    "            else:\n",
    "                return_metrics[metric.name] = result\n",
    "        return return_metrics\n",
    "    \n",
    "    def predict_step(self, data):\n",
    "        inp, tar, length = data\n",
    "        startMask = tf.cast(tf.fill([tf.shape(tar)[0], 1, tf.shape(tar)[-1]],-1),dtype=tf.float32)\n",
    "        tar = tf.concat([startMask, tar],1)\n",
    "\n",
    "        tar_inp = tar[:, :-1, :]\n",
    "\n",
    "        return self((inp, tar_inp, length), training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c390d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_real_T5(input_size, output_size, args):\n",
    "    inputs = (tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Input(shape=(1)) )\n",
    "    # targets, length\n",
    "    transformer = T5ModelNoMaskCreationModel(vocab_size = args.vocab_size, num_layers=args.num_layers, d_model=args.d_model, num_heads=args.num_heads, d_ff=args.d_ff, d_kv = args.d_kv, feed_forward_proj = args.feed_forward_proj, \n",
    "            relative_attention_num_buckets=args.relative_attention_num_buckets, eps=args.layer_norm_epsilon, dropout=args.dropout, factor=args.init_factor,\n",
    "            embed_or_dense=\"embed\", target_size= args.output_size)\n",
    "\n",
    "    inp, tar, length = inputs\n",
    "    enc_padding_mask = create_masks(inp, tar, length)\n",
    "    #dec_padding_mask = tf.squeeze(dec_padding_mask)\n",
    "    outputs = transformer(input_ids=inp, attention_mask=enc_padding_mask, \n",
    "            decoder_input_ids=tar, \n",
    "             training=False) # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    model = T5VQ_VAE(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "    learning_rate = CustomSchedule(args.d_model)\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-8)\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999,epsilon=1e-8, weight_decay = 0.01)\n",
    "    #model.add_metric(tf.keras.metrics.Mean(name='train_loss')(outputs))\n",
    "    #model.compile(loss=mse_with_proper_loss(output_size), optimizer=optimizer)\n",
    "    model.compile(loss=pit_with_stft_trace(output_size), optimizer=optimizer)\n",
    "#     model.compile(loss=keras.losses.mean_squared_error, optimizer=adam)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144ae522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"t5vq_vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 512)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape (TFOpLambda) (3,)                 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_1 (TFOpLambd (3,)                 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici ()                   0           tf.compat.v1.shape[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli ()                   0           tf.compat.v1.shape_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_2 (TFOpLambd (3,)                 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.repeat (TFOpLambda)          (None,)              0           tf.__operators__.getitem[0][0]   \n",
      "                                                                 tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_2 (Sli ()                   0           tf.compat.v1.shape_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.sequence_mask (TFOpLambda)   (None, None)         0           tf.repeat[0][0]                  \n",
      "                                                                 tf.__operators__.getitem_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (None, None)         0           tf.sequence_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 514)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "t5_model_no_mask_creation_model (None, None, 514, 12 19478657    input_1[0][0]                    \n",
      "                                                                 tf.cast[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 19,478,657\n",
      "Trainable params: 19,478,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "INFO:tensorflow:Error reported to Coordinator: not enough values to unpack (expected 3, got 2)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 692, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 382, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 463, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py\", line 835, in run_step\n",
      "    outputs = model.train_step(data)\n",
      "  File \"<ipython-input-21-bb0f33be6f31>\", line 9, in train_step\n",
      "    inp, tar, length = data\n",
      "ValueError: not enough values to unpack (expected 3, got 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py:671 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:104 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:246 _call_for_each_replica\n        coord.join(threads)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\six.py:703 reraise\n        raise value\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py:297 stop_on_exception\n        yield\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:346 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-21-bb0f33be6f31>:9 train_step\n        inp, tar, length = data\n\n    ValueError: not enough values to unpack (expected 3, got 2)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-0150d7118137>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint_cb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_cb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 760\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_strategy.py:671 _call_for_each_replica\n        self._container_strategy(), fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:104 call_for_each_replica\n        return _call_for_each_replica(strategy, fn, args, kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:246 _call_for_each_replica\n        coord.join(threads)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py:389 join\n        six.reraise(*self._exc_info_to_raise)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\six.py:703 reraise\n        raise value\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\training\\coordinator.py:297 stop_on_exception\n        yield\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\distribute\\mirrored_run.py:346 run\n        self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n    c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    <ipython-input-21-bb0f33be6f31>:9 train_step\n        inp, tar, length = data\n\n    ValueError: not enough values to unpack (expected 3, got 2)\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = args.ckpt_path\n",
    "mkdir_p(ckpt_path) # model check point 폴더 만드는 코드\n",
    "\n",
    "filepath = ckpt_path + \"/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\"\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Training part\n",
    "\n",
    "epoch = args.n_epochs\n",
    "strategy = tf.distribute.MirroredStrategy() # '/gpu:0','/gpu:1','/gpu:2','/gpu:4','/gpu:5','/gpu:6','/gpu:7'\n",
    "#physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#tf.config.set_visible_devices(physical_devices[0:7], 'GPU')\n",
    "#strategy =  tf.distribute.MultiWorkerMirroredStrategy()\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     model = load_model('./CKPT/CKP_ep_29__loss_102.63367_.h5', custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "\n",
    "    model = build_real_T5(args.input_size, args.output_size, args)\n",
    "    #if args.is_load_model is True:\n",
    "\n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42455682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_33 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"t5vq_vae_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_3 (TFOpLambd (3,)                 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_4 (TFOpLambd (3,)                 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_3 (Sli ()                   0           tf.compat.v1.shape_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_4 (Sli ()                   0           tf.compat.v1.shape_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.shape_5 (TFOpLambd (3,)                 0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.repeat_1 (TFOpLambda)        (None,)              0           tf.__operators__.getitem_3[0][0] \n",
      "                                                                 tf.__operators__.getitem_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_5 (Sli ()                   0           tf.compat.v1.shape_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.sequence_mask_1 (TFOpLambda) (None, None)         0           tf.repeat_1[0][0]                \n",
      "                                                                 tf.__operators__.getitem_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_1 (TFOpLambda)          (None, None)         0           tf.sequence_mask_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, None, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "t5_model_no_mask_creation_model (None, None, 514)    13384706    input_6[0][0]                    \n",
      "                                                                 tf.cast_1[0][0]                  \n",
      "                                                                 input_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,384,706\n",
      "Trainable params: 13,384,706\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [7000,8,1,1] vs. [2,8,1,3500] [Op:AddV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9f19ab68e19b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mdecode_inp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdecode_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencode_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_batch_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreconstructed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m--> 415\u001b[1;33m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[1;31m# Update tensor_dict.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m         outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, \n\u001b[1;32m-> 1134\u001b[1;33m              training=False) # (batch_size, tar_seq_len, target_vocab_size)\n\u001b[0m\u001b[0;32m   1135\u001b[0m         \u001b[1;31m#inputs = tf.concat([inp,inp],2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m         \u001b[1;31m#print('mask_output',final_output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1043\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1045\u001b[1;33m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1046\u001b[0m             )\n\u001b[0;32m   1047\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    763\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             )\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    474\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m         )\n\u001b[0;32m    478\u001b[0m         \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpresent_key_value_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         )\n\u001b[0;32m    388\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[0mposition_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposition_bias\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmask\u001b[0m  \u001b[1;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1368\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1698\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1699\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1700\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    452\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6940\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6941\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6942\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [7000,8,1,1] vs. [2,8,1,3500] [Op:AddV2]"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epochs = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model_path = './CKPT/vqvae_same/CKP_ep_291__loss_89.49190_.h5'\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    sisdr_Metric = SiSdr()\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "\n",
    "    model = build_real_T5(args.input_size, args.output_size, args)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, x_batch_train in enumerate(train_dataset):\n",
    "            train_inputs = tf.cast(x_batch_train[0], dtype=tf.float32)\n",
    "            train_labels = tf.cast(x_batch_train[0], dtype=tf.float32)\n",
    "\n",
    "            \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                vqvae_encode = vq_vae.encoder(train_inputs).numpy()\n",
    "                #encode_onehot = tf.cast(tf.equal(vqvae_encode, tf.math.reduce_max(vqvae_encode, 2, keepdims=True)), vqvae_encode.dtype)\n",
    "                encode_inp = tf.math.argmax(vqvae_encode, -1)\n",
    "                \n",
    "                #zeros = tf.zeros([tf.shape(encode_onehot)[0].numpy(), tf.shape(encode_onehot)[1].numpy(), 2], encode_onehot.dtype)\n",
    "                #decode_onehot = tf.concat([encode_onehot, zeros],-1)\n",
    "                start = tf.cast(tf.repeat(tf.constant([[args.vocab_size]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "                end = tf.cast(tf.repeat(tf.constant([[args.vocab_size+1]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "                decode_inp = tf.concat([start, encode_inp],1)\n",
    "                decode_inp = tf.concat([decode_inp, end],1)\n",
    "                \n",
    "                reconstructed = model((encode_inp, decode_inp, x_batch_train[1]))\n",
    "                reconstructed = reconstructed[:,:-1,:-1]\n",
    "                reconstructed = tf.one_hot(reconstructed, 512)\n",
    "                vqvae_decode = vq_vae.decoder(reconstructed).numpy()\n",
    "                \n",
    "                # Compute reconstruction loss\n",
    "                loss = custom_mse(vqvae_encode, reconstructed)\n",
    "                loss += sum(ae.losses)  # Add KL loss\n",
    "\n",
    "            #grads = tape.gradient(loss, ae.trainable_weights)\n",
    "            #optimizer.apply_gradients(zip(grads, ae.trainable_weights))\n",
    "\n",
    "            loss_metric(loss)\n",
    "#             sisdr_Metric.update_state(x_batch_train[0], x_batch_train[0])\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(\"step %d: mean loss = %.4f, Si-sdr = %.4f\" % (step, loss_metric.result(), sisdr_Metric()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d75c02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 55992, 512), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros(encode_onehot.shape, encode_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9dbdb3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3502), dtype=int64, numpy=\n",
       "array([[512, 324, 324, ..., 324, 176, 513],\n",
       "       [512, 324, 324, ..., 324, 176, 513]], dtype=int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq_vae.encoder(train_inputs)\n",
    "decode_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d289a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_inp = tf.math.argmax(encode_onehot,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fbfc657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[512, 513],\n",
       "       [512, 513]])>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.repeat(tf.constant([[args.vocab_size,args.vocab_size+1]]), tf.shape(encode_inp)[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4249e172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[512, 513]])>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[args.vocab_size,args.vocab_size+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d59e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tf.cast(tf.repeat(tf.constant([[args.vocab_size]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "end = tf.cast(tf.repeat(tf.constant([[args.vocab_size+1]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "deocde_inp = tf.concat([start, encode_inp],1)\n",
    "deocde_inp = tf.concat([deocde_inp, end],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60f09e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5002), dtype=float32, numpy=\n",
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_masks(deocde_inp,_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "db416f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5002, 512), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(deocde_inp, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3cc347aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5000), dtype=int64, numpy=\n",
       "array([[324, 324, 324, ..., 324, 324, 176],\n",
       "       [324, 324, 324, ..., 324, 324, 176]], dtype=int64)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(vqvae_encode, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f45b8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "emb_layer = tf.keras.layers.Embedding(args.vocab_size, args.d_model)\n",
    "dec_emb_layer = tf.keras.layers.Embedding(args.vocab_size+2, args.d_model)\n",
    "enc_mask = create_masks(encode_inp,_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60025d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3502, 512), dtype=float32, numpy=\n",
       "array([[[-0.04950542, -0.00063734, -0.00788502, ..., -0.03748046,\n",
       "         -0.03154826, -0.03086646],\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        ...,\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        [ 0.00895997, -0.03796548,  0.03192111, ...,  0.03491438,\n",
       "         -0.04272968, -0.02189927],\n",
       "        [ 0.03629345,  0.00849929, -0.03875323, ..., -0.02384899,\n",
       "         -0.02035632, -0.01134278]],\n",
       "\n",
       "       [[-0.04950542, -0.00063734, -0.00788502, ..., -0.03748046,\n",
       "         -0.03154826, -0.03086646],\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        ...,\n",
       "        [ 0.04070134, -0.02675874,  0.01688543, ..., -0.00723369,\n",
       "          0.04475541, -0.01825378],\n",
       "        [ 0.00895997, -0.03796548,  0.03192111, ...,  0.03491438,\n",
       "         -0.04272968, -0.02189927],\n",
       "        [ 0.03629345,  0.00849929, -0.03875323, ..., -0.02384899,\n",
       "         -0.02035632, -0.01134278]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_layer(encode_inp)\n",
    "dec_emb_layer(decode_inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "863ad2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.Real_Layers import T5Stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a561e827",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = T5Stack(args.num_layers, args.d_model, args.d_ff, args.d_kv, args.feed_forward_proj, args.num_heads, is_decoder = False, relative_attention_num_buckets=args.relative_attention_num_buckets, eps = args.layer_norm_epsilon, dropout=args.dropout, embed_tokens = emb_layer, factor=args.init_factor)\n",
    "decoder = T5Stack(args.num_layers, args.d_model, args.d_ff, args.d_kv, args.feed_forward_proj, args.num_heads, is_decoder = True, relative_attention_num_buckets=args.relative_attention_num_buckets, eps = args.layer_norm_epsilon, dropout=args.dropout, embed_tokens = dec_emb_layer, factor=args.init_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4be4264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder(\n",
    "    input_ids=encode_inp,\n",
    "    attention_mask=enc_mask,\n",
    ")\n",
    "\n",
    "dec_output = decoder(\n",
    "    input_ids=decode_inp,\n",
    "    encoder_hidden_states=output[0],\n",
    "    encoder_attention_mask=enc_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efa36d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 3502, 512), dtype=float32, numpy=\n",
       " array([[[ 1.2077979 , -0.30607352,  0.5464545 , ..., -0.31013697,\n",
       "          -0.9685639 ,  2.2401168 ],\n",
       "         [ 1.2952253 , -0.8128181 , -1.0662067 , ..., -1.1302086 ,\n",
       "          -0.7823541 ,  0.8705663 ],\n",
       "         [ 1.0742295 , -0.8347188 , -1.2404468 , ..., -1.1410517 ,\n",
       "          -0.75787365,  0.6026806 ],\n",
       "         ...,\n",
       "         [ 0.56603163, -0.46112776, -1.6801546 , ..., -0.97513324,\n",
       "          -0.43376756, -0.35038418],\n",
       "         [ 0.18591672, -0.58123124, -1.4556829 , ..., -0.9629582 ,\n",
       "          -0.5377447 , -0.27069557],\n",
       "         [ 0.22506867, -0.30955017, -1.7474878 , ..., -0.96178037,\n",
       "          -0.28729254, -0.05892743]],\n",
       " \n",
       "        [[ 1.0260235 , -0.2839064 ,  0.5034074 , ..., -0.3286717 ,\n",
       "          -1.0567868 ,  1.9603367 ],\n",
       "         [ 0.9995614 , -0.7665592 , -1.071817  , ..., -1.1798645 ,\n",
       "          -0.96889806,  0.87005365],\n",
       "         [ 0.8442797 , -0.7402053 , -1.2602735 , ..., -1.1824063 ,\n",
       "          -0.9689222 ,  0.6384095 ],\n",
       "         ...,\n",
       "         [ 0.5491018 , -0.4698024 , -1.6499015 , ..., -1.1016709 ,\n",
       "          -0.78877753, -0.26691222],\n",
       "         [ 0.20383656, -0.5444725 , -1.4244307 , ..., -1.0627505 ,\n",
       "          -0.93312025, -0.2685795 ],\n",
       "         [ 0.21610135, -0.25803772, -1.7038133 , ..., -1.0525019 ,\n",
       "          -0.7949479 ,  0.0028645 ]]], dtype=float32)>,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d9e54a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = T5ModelNoMaskCreationModel(vocab_size = args.vocab_size, num_layers=args.num_layers, d_model=args.d_model, num_heads=args.num_heads, d_ff=args.d_ff, d_kv = args.d_kv, feed_forward_proj = args.feed_forward_proj, \n",
    "            relative_attention_num_buckets=args.relative_attention_num_buckets, eps=args.layer_norm_epsilon, dropout=args.dropout, factor=args.init_factor,\n",
    "            embed_or_dense=\"embed\", target_size= args.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0e062a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_fin = test_model(\n",
    "        input_ids=encode_inp, #\n",
    "        attention_mask=enc_mask, #\n",
    "        decoder_input_ids=decode_inp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "28311376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3501), dtype=int64, numpy=\n",
       "array([[ 98, 359,  61, ...,  61,  61, 275],\n",
       "       [420, 222,  61, ...,  61,  61, 275]], dtype=int64)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(output_fin, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d84449ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3501, 514), dtype=float32, numpy=\n",
       "array([[[ 0.869631  ,  1.1690212 ,  0.35820168, ...,  0.6019451 ,\n",
       "          1.0112873 ,  2.1435962 ],\n",
       "        [ 0.80149245,  2.3579922 , -0.4399348 , ...,  0.60069275,\n",
       "         -0.3548818 ,  1.9100547 ],\n",
       "        [ 0.68666923,  2.3211918 , -0.5342808 , ...,  0.41276547,\n",
       "         -0.68009174,  1.6850523 ],\n",
       "        ...,\n",
       "        [ 0.911541  ,  2.128042  , -0.632104  , ..., -0.6814339 ,\n",
       "         -0.76991796,  1.3301015 ],\n",
       "        [ 0.9115375 ,  2.1280565 , -0.63212836, ..., -0.6814034 ,\n",
       "         -0.76994693,  1.3300073 ],\n",
       "        [ 1.1326382 ,  0.8186412 ,  0.89920235, ..., -0.2691553 ,\n",
       "          0.5539378 ,  1.2252066 ]],\n",
       "\n",
       "       [[ 0.7201735 ,  1.2553155 ,  0.17787439, ...,  0.42479405,\n",
       "          1.0152178 ,  1.9904431 ],\n",
       "        [ 0.8037424 ,  2.2758029 , -0.5985501 , ...,  0.302431  ,\n",
       "         -0.13018467,  1.7118871 ],\n",
       "        [ 0.7113944 ,  2.2694674 , -0.67817116, ...,  0.13510275,\n",
       "         -0.41518915,  1.5366485 ],\n",
       "        ...,\n",
       "        [ 0.8473494 ,  2.1253388 , -0.69891053, ..., -0.64458525,\n",
       "         -0.5177855 ,  1.2185    ],\n",
       "        [ 0.8473899 ,  2.1253135 , -0.69889605, ..., -0.64459485,\n",
       "         -0.5178282 ,  1.2184553 ],\n",
       "        [ 0.95999277,  1.039394  ,  0.5658205 , ..., -0.26614067,\n",
       "          0.56958723,  1.3339732 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cfc6295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_96 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"t5_model_no_mask_creation_model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "t5_model_12 (T5Model)        multiple                  13121024  \n",
      "_________________________________________________________________\n",
      "dense_566 (Dense)            multiple                  263682    \n",
      "=================================================================\n",
      "Total params: 13,384,706\n",
      "Trainable params: 13,384,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Start of epoch 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-5ded8e2cd383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m                                 \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_inp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                                 \u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_inp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                             )\n\u001b[0;32m     59\u001b[0m                 \u001b[0mreconstructed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m         outputs = self.t5(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, \n\u001b[1;32m-> 1134\u001b[1;33m              training=False) # (batch_size, tar_seq_len, target_vocab_size)\n\u001b[0m\u001b[0;32m   1135\u001b[0m         \u001b[1;31m#inputs = tf.concat([inp,inp],2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m         \u001b[1;31m#print('mask_output',final_output.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m             \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m         )\n\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    763\u001b[0m                 \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             )\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[0;32m    502\u001b[0m                 \u001b[0mquery_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m                 \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m             )\n\u001b[0;32m    506\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[0mquery_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         )\n\u001b[0;32m    423\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\J_and_J_Research\\src\\models\\Real_Layers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mposition_bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (batch_size, n_heads, seq_length, key_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;31m# Mask heads if we want to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1037\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\layers\\advanced_activations.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    346\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   4757\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4758\u001b[0m   \"\"\"\n\u001b[1;32m-> 4759\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_v2\u001b[1;34m(logits, axis, name)\u001b[0m\n\u001b[0;32m   3818\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3819\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3820\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_wrap_2d_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_wrap_2d_function\u001b[1;34m(inputs, compute_op, dim, name)\u001b[0m\n\u001b[0;32m   3737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3738\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mis_last_dim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3739\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcompute_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3741\u001b[0m   \u001b[0mdim_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(logits, name)\u001b[0m\n\u001b[0;32m  10858\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10859\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m> 10860\u001b[1;33m         _ctx, \"Softmax\", name, logits)\n\u001b[0m\u001b[0;32m  10861\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10862\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epochs = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model_path = './CKPT/vqvae_same/CKP_ep_291__loss_89.49190_.h5'\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    sisdr_Metric = SiSdr()\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "\n",
    "    transformer = T5ModelNoMaskCreationModel(vocab_size = args.vocab_size, num_layers=args.num_layers, d_model=args.d_model, num_heads=args.num_heads, d_ff=args.d_ff, d_kv = args.d_kv, feed_forward_proj = args.feed_forward_proj, \n",
    "            relative_attention_num_buckets=args.relative_attention_num_buckets, eps=args.layer_norm_epsilon, dropout=args.dropout, factor=args.init_factor,\n",
    "            embed_or_dense=\"embed\", target_size= args.output_size)\n",
    "    ce_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    inputs = tf.keras.layers.Input(shape=(None, 1))\n",
    "    dec_inputs = tf.keras.layers.Input(shape=(None, 1))\n",
    "    transformer(input_ids=inputs, decoder_input_ids = dec_inputs)\n",
    "    transformer.summary()\n",
    "    \n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        for step, x_batch_train in enumerate(train_dataset):\n",
    "            train_inputs = tf.cast(x_batch_train[0], dtype=tf.float32)\n",
    "            train_labels = tf.cast(x_batch_train[0], dtype=tf.float32)\n",
    "\n",
    "            \n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                vqvae_encode = vq_vae.encoder(train_inputs).numpy()\n",
    "                #encode_onehot = tf.cast(tf.equal(vqvae_encode, tf.math.reduce_max(vqvae_encode, 2, keepdims=True)), vqvae_encode.dtype)\n",
    "                encode_inp = tf.math.argmax(vqvae_encode, -1)\n",
    "                \n",
    "                #zeros = tf.zeros([tf.shape(encode_onehot)[0].numpy(), tf.shape(encode_onehot)[1].numpy(), 2], encode_onehot.dtype)\n",
    "                #decode_onehot = tf.concat([encode_onehot, zeros],-1)\n",
    "                start = tf.cast(tf.repeat(tf.constant([[args.vocab_size]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "                end = tf.cast(tf.repeat(tf.constant([[args.vocab_size+1]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "                decode_inp = tf.concat([start, encode_inp],1)\n",
    "                decode_label = tf.concat([encode_inp, end],1)\n",
    "                \n",
    "                enc_mask = create_masks(encode_inp, _)\n",
    "                \n",
    "                prediction = transformer(\n",
    "                                input_ids=encode_inp, #\n",
    "                                attention_mask=enc_mask, #\n",
    "                                decoder_input_ids=decode_inp\n",
    "                            )\n",
    "                reconstructed = prediction[:,:-1,:-2]\n",
    "                reconstructed = tf.math.argmax(reconstructed,-1)\n",
    "                reconstructed = tf.one_hot(reconstructed, args.vocab_size)\n",
    "                vqvae_decode = vq_vae.decoder(reconstructed).numpy()\n",
    "                \n",
    "                # Compute reconstruction loss\n",
    "                loss = ce_loss_object(decode_label, prediction)\n",
    "                loss += sum(transformer.losses)  # Add KL loss\n",
    "\n",
    "            grads = tape.gradient(loss, transformer.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, transformer.trainable_weights))\n",
    "\n",
    "            #loss_metric(loss)\n",
    "#             sisdr_Metric.update_state(x_batch_train[0], x_batch_train[0])\n",
    "\n",
    "            if step % 100 == 1:\n",
    "                print(\"step %d: mean loss = %.4f, Si-sdr = %.4f\" % (step, loss_metric.result(), sisdr_Metric()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_real_T5(input_size, output_size, args):\n",
    "    inputs = (tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Input(shape=(None, 1)),\n",
    "    tf.keras.layers.Input(shape=(1)) )\n",
    "    # targets, length\n",
    "    transformer = T5ModelNoMaskCreationModel(vocab_size = args.vocab_size, num_layers=args.num_layers, d_model=args.d_model, num_heads=args.num_heads, d_ff=args.d_ff, d_kv = args.d_kv, feed_forward_proj = args.feed_forward_proj, \n",
    "            relative_attention_num_buckets=args.relative_attention_num_buckets, eps=args.layer_norm_epsilon, dropout=args.dropout, factor=args.init_factor,\n",
    "            embed_or_dense=\"embed\", target_size= args.output_size)\n",
    "\n",
    "    inp, tar, length = inputs\n",
    "    enc_padding_mask = create_masks(inp, tar, length)\n",
    "    #dec_padding_mask = tf.squeeze(dec_padding_mask)\n",
    "    outputs = transformer(input_ids=inp, attention_mask=enc_padding_mask, \n",
    "            decoder_input_ids=tar, \n",
    "             training=False) # (batch_size, tar_seq_len, target_vocab_size)\n",
    "    \n",
    "    model = T5VQ_VAE(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "    learning_rate = CustomSchedule(args.d_model)\n",
    "    #optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-8)\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999,epsilon=1e-8, weight_decay = 0.01)\n",
    "    #model.add_metric(tf.keras.metrics.Mean(name='train_loss')(outputs))\n",
    "    #model.compile(loss=mse_with_proper_loss(output_size), optimizer=optimizer)\n",
    "    model.compile(loss=pit_with_stft_trace(output_size), optimizer=optimizer)\n",
    "#     model.compile(loss=keras.losses.mean_squared_error, optimizer=adam)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f041689",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: The shape of labels (received (2, 3502)) should equal the shape of logits except for the last dimension (received (2, 3501, 514)).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-bfe5572a01e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Custom mse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mce_loss_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mce_loss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    139\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m    143\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[0mag_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mag_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[0;32m   1737\u001b[0m   \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m   return backend.sparse_categorical_crossentropy(\n\u001b[1;32m-> 1739\u001b[1;33m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[0;32m   1740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   4958\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4959\u001b[0m     res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m-> 4960\u001b[1;33m         labels=target, logits=output)\n\u001b[0m\u001b[0;32m   4961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4962\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mupdate_shape\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0moutput_rank\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits_v2\u001b[1;34m(labels, logits, name)\u001b[0m\n\u001b[0;32m   4350\u001b[0m   \"\"\"\n\u001b[0;32m   4351\u001b[0m   return sparse_softmax_cross_entropy_with_logits(\n\u001b[1;32m-> 4352\u001b[1;33m       labels=labels, logits=logits, name=name)\n\u001b[0m\u001b[0;32m   4353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m   4257\u001b[0m                        \u001b[1;34m\"should equal the shape of logits except for the last \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4258\u001b[0m                        \"dimension (received %s).\" % (labels_static_shape,\n\u001b[1;32m-> 4259\u001b[1;33m                                                      logits.get_shape()))\n\u001b[0m\u001b[0;32m   4260\u001b[0m     \u001b[1;31m# Check if no reshapes are required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape mismatch: The shape of labels (received (2, 3502)) should equal the shape of logits except for the last dimension (received (2, 3501, 514))."
     ]
    }
   ],
   "source": [
    "# Custom mse\n",
    "ce_loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "ce_loss_object(decode_label, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7be692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        vqvae_encode = vq_vae.encoder(x)\n",
    "        #encode_onehot = tf.cast(tf.equal(vqvae_encode, tf.math.reduce_max(vqvae_encode, 2, keepdims=True)), vqvae_encode.dtype)\n",
    "        encode_inp = tf.math.argmax(vqvae_encode, -1)\n",
    "\n",
    "        #zeros = tf.zeros([tf.shape(encode_onehot)[0].numpy(), tf.shape(encode_onehot)[1].numpy(), 2], encode_onehot.dtype)\n",
    "        #decode_onehot = tf.concat([encode_onehot, zeros],-1)\n",
    "        start = tf.cast(tf.repeat(tf.constant([[args.vocab_size]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "        end = tf.cast(tf.repeat(tf.constant([[args.vocab_size+1]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "        decode_inp = tf.concat([start, encode_inp],1)\n",
    "        decode_label = tf.concat([encode_inp, end],1)\n",
    "\n",
    "        enc_mask = create_masks(encode_inp, _)\n",
    "\n",
    "        prediction = transformer(\n",
    "                        input_ids=encode_inp, #\n",
    "                        attention_mask=enc_mask, #\n",
    "                        decoder_input_ids=decode_inp\n",
    "                    )\n",
    "        reconstructed = prediction[:,:-1,:-2]\n",
    "        reconstructed = tf.math.argmax(reconstructed,-1)\n",
    "        reconstructed = tf.one_hot(reconstructed, args.vocab_size)\n",
    "        vqvae_decode = vq_vae.decoder(reconstructed)\n",
    "\n",
    "        # Compute reconstruction loss\n",
    "        loss = ce_loss_object(decode_label, prediction)\n",
    "        loss += sum(transformer.losses)  # Add KL loss\n",
    "    \n",
    "    # Update weights\n",
    "    grads = tape.gradient(loss, transformer.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, transformer.trainable_weights))\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "    train_loss.update_state(loss)\n",
    "    sisdr_Metric.update_state(y, vqvae_decode)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    # Call model\n",
    "    vqvae_encode = vq_vae.encoder(x)\n",
    "    # 원핫따위 쓰지 않고 바로 (batch, sequence) 로 맞춰줌\n",
    "    encode_inp = tf.math.argmax(vqvae_encode, -1)\n",
    "\n",
    "    # decoder input과 label을 만들어주자.\n",
    "    start = tf.cast(tf.repeat(tf.constant([[args.vocab_size]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "    end = tf.cast(tf.repeat(tf.constant([[args.vocab_size+1]]), tf.shape(encode_inp)[0], 0), encode_inp.dtype)\n",
    "    decode_inp = tf.concat([start, encode_inp],1)\n",
    "    decode_label = tf.concat([encode_inp, end],1)\n",
    "\n",
    "    # attention mask 만들어주자.\n",
    "    enc_mask = create_masks(encode_inp, _)\n",
    "\n",
    "    prediction = transformer(\n",
    "                    input_ids=encode_inp, \n",
    "                    attention_mask=enc_mask, \n",
    "                    decoder_input_ids=decode_inp\n",
    "                )\n",
    "    # decoder = (batch, seq_len + 1, 514) 를 (batch, seq_len, 512)로 되돌림\n",
    "    reconstructed = prediction[:,:-1,:-2]\n",
    "    reconstructed = tf.math.argmax(reconstructed,-1)\n",
    "    reconstructed = tf.one_hot(reconstructed, args.vocab_size)\n",
    "    # decoder로 복원하자.\n",
    "    vqvae_decode = vq_vae.decoder(reconstructed)\n",
    "\n",
    "    # Calculate losses\n",
    "    val_loss_value = ce_loss_object(decode_label, prediction)\n",
    "    val_loss_value += sum(transformer.losses) # Add KL loss\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "    valid_loss.update_state(val_loss_value)\n",
    "    val_sisdr_Metric.update_state(y, vqvae_decode)\n",
    "    \n",
    "    return val_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bf2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_208 (Softmax)        (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"t5_model_no_mask_creation_model_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "t5_model_31 (T5Model)        multiple                  6824448   \n",
      "_________________________________________________________________\n",
      "dense_1125 (Dense)           multiple                  263682    \n",
      "=================================================================\n",
      "Total params: 7,088,130\n",
      "Trainable params: 7,088,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d889748b6a6943abaa4078d7a6190f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "latent_size = 512\n",
    "epochs = 2\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    model_path = './CKPT/vqvae_same/CKP_ep_291__loss_89.49190_.h5'\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    ce_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_loss = tf.keras.metrics.Mean()\n",
    "    valid_loss = tf.keras.metrics.Mean()\n",
    "    sisdr_Metric = SiSdr()\n",
    "    val_sisdr_Metric = SiSdr()\n",
    "    \n",
    "    \n",
    "    # load vq_vae\n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # load_transformer\n",
    "    transformer = T5ModelNoMaskCreationModel(vocab_size = args.vocab_size, num_layers=args.num_layers, d_model=args.d_model, num_heads=args.num_heads, d_ff=args.d_ff, d_kv = args.d_kv, feed_forward_proj = args.feed_forward_proj, \n",
    "            relative_attention_num_buckets=args.relative_attention_num_buckets, eps=args.layer_norm_epsilon, dropout=args.dropout, factor=args.init_factor,\n",
    "            embed_or_dense=\"embed\", target_size= args.output_size)\n",
    "    inputs = tf.keras.layers.Input(shape=(None, 1))\n",
    "    dec_inputs = tf.keras.layers.Input(shape=(None, 1))\n",
    "    transformer(input_ids=inputs, decoder_input_ids = dec_inputs)\n",
    "    transformer.summary()\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset\n",
    "        progress_bar = tqdm(range(len(train_dataset)))\n",
    "        for step, (x_batch_train, length) in enumerate(train_dataset):\n",
    "            x_batch_train = tf.cast(x_batch_train, dtype=tf.float32)\n",
    "\n",
    "            loss_value = train_step(x_batch_train, x_batch_train)\n",
    "\n",
    "            # Log every 1 batches\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(\"step : %d loss : %.4f Si-sdr : %.4f\" % (step, train_loss.result(), sisdr_Metric.result()))\n",
    "\n",
    "        # Run a validation loop at the end of each epoch\n",
    "        valid_progress_bar = tqdm(range(len(train_dataset)))\n",
    "        for x_batch_val, length in valid_dataset:\n",
    "            x_batch_val = tf.cast(x_batch_val, dtype=tf.float32)\n",
    "\n",
    "            val_loss_value = test_step(x_batch_val, x_batch_val)\n",
    "            valid_progress_bar.update(1)\n",
    "            valid_progress_bar.set_description(\"valid loss : %.4f valid Si-sdr : %.4f\" % (valid_loss.result(), val_sisdr_Metric.result()))\n",
    "\n",
    "\n",
    "        print()\n",
    "        print('----------------------------------------------------------------------------------')\n",
    "        print(\"Time taken >>> %.2fs <<<\" % (time.time() - start_time))\n",
    "        print('epoch: {}, Train_loss: {}, Train_Si-sdr: {} \\n\\\n",
    "        Valid_loss: {}, Valid_Si-sdr: {}'.format(\n",
    "            epoch + 1,\n",
    "            train_loss.result(),\n",
    "            sisdr_Metric.result(),\n",
    "            valid_loss.result(),\n",
    "            val_sisdr_Metric.result()))\n",
    "        print('----------------------------------------------------------------------------------')\n",
    "        print()\n",
    "\n",
    "        # Reset metrics at the end of each epoch\n",
    "        train_loss.reset_states()\n",
    "        sisdr_Metric.reset_states()\n",
    "        valid_loss.reset_states()\n",
    "        val_sisdr_Metric.reset_states()\n",
    "\n",
    "        # Data shuffle at the end of each epoch\n",
    "        train_dataset.on_epoch_end()\n",
    "        valid_dataset.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a00acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (x_batch_train, length) in enumerate(train_dataset):\n",
    "    x_batch_train = tf.cast(x_batch_train, dtype=tf.float32)\n",
    "    vq_vae.encoder(x_batch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1939b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vq_vae.encoder(x_batch_train).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2b0b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
