{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {
    "id": "frank-northwest"
   },
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {
    "executionInfo": {
     "elapsed": 2721,
     "status": "ok",
     "timestamp": 1641547283433,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "requested-installation"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import librosa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import scipy.signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5502cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\lynn1\\AppData\\Local\\Temp/ipykernel_7820/994479228.py:74: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if self.files is not 'tt':\n"
     ]
    }
   ],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "#         signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "        \n",
    "#         return signal[0]\n",
    "    \n",
    "        signal_rate, signal = wavfile.read(path)\n",
    "        number_of_samples = round(len(signal) * float(self.sample_rate) / signal_rate)\n",
    "        signal = scipy.signal.resample(signal, number_of_samples)\n",
    "        signal /= np.max(np.abs(signal),axis=0)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "#             return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "            return sour_pad, label_pad\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-safety",
   "metadata": {
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1641547287252,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "architectural-safety"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\lynn1\\AppData\\Local\\Temp/ipykernel_7820/894115812.py:84: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if self.files is not 'tt':\n"
     ]
    }
   ],
   "source": [
    "class RawStackForVAEGenerator(Sequence):   ## sequence 상속\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=16, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):   ## 하나의 epoch이 끝나면 자동으로 실행\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:  ## 다른 샘플 shuffling\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):   ## 다른 폴더에 있는 오디어 파일 데이터를 읽어옴\n",
    "#         signal = librosa.load(path, sr=sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "#         return signal[0]\n",
    "    \n",
    "        signal_rate, signal = wavfile.read(path)\n",
    "        number_of_samples = round(len(signal) * float(self.sample_rate) / signal_rate)\n",
    "        signal = scipy.signal.resample(signal, number_of_samples)\n",
    "        signal /= np.max(np.abs(signal),axis=0)\n",
    "        \n",
    "        return signal\n",
    "    \n",
    "    def __padding__(self, data):   ## 짧은 오디오 데이터에 0을 붙여서 길이를 맞춰줌\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        pad = np.zeros((n_batch, max_len, data[0].shape[1]))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return pad\n",
    "        \n",
    "    def __data_generation__(self, source_list):   ## audio-read해서 읽어온 데이터를 전처리\n",
    "        L = 40\n",
    "        \n",
    "        wav_list = []\n",
    "        label_wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "#             print(s_wav.shape)\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- TIME AXIS CALCULATE -------\n",
    "            K = int(np.ceil(len(s_wav) / L))\n",
    "            \n",
    "            if K//4 != 0:          # stride=2일 때, K를 4의 배수로 만들어줌\n",
    "                K = ((K//4)+1)*4\n",
    "            # -----------------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "            pad_len = K * L\n",
    "            pad_s = np.concatenate([s_wav, np.zeros([pad_len - len(s_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            # ------- RESHAPE -------\n",
    "            s = np.reshape(pad_s, [K, L])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s)\n",
    "            label_wav_list.append(np.expand_dims(s_wav, 1))\n",
    "        \n",
    "        return wav_list, label_wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)   ## 데이터 받아오기\n",
    "            \n",
    "            # Get Lengths(label length)\n",
    "            lengths = np.array([m.shape[0] for m in labels])\n",
    "            tiled = np.tile(np.expand_dims(lengths, 1), [1, labels[0].shape[1]])\n",
    "            tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step_for_stack, Dimension(=40)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, sour_pad\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "#             lengths = np.array([m.shape[0] for m in mix])\n",
    "#             tiled = np.tile(np.expand_dims(l bvengths, 1), [1, labels[0].shape[1]])\n",
    "#             tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension]\n",
    "            \n",
    "            return sour_pad, tiled, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {
    "id": "taken-league"
   },
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "martial-allocation",
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1641547292579,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "martial-allocation"
   },
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attached-debate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17855,
     "status": "ok",
     "timestamp": 1641547313592,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "attached-debate",
    "outputId": "94bfe083-c2a7-4971-fa74-e89a48a8a1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comparable-tiger",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547316527,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "comparable-tiger"
   },
   "outputs": [],
   "source": [
    "batch_size_for_generator = 10\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawStackForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af27c272",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4969,
     "status": "ok",
     "timestamp": 1641547324696,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "af27c272",
    "outputId": "6b3ddea0-efad-4668-ad05-33090b5d2fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1568, 40)\n",
      "(10, 1568, 40)\n"
     ]
    }
   ],
   "source": [
    "a, b = next(iter(train_dataset))\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {
    "id": "human-russian"
   },
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "environmental-revision",
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1641547328656,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "environmental-revision"
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sexual-ordering",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547329495,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "sexual-ordering"
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-height",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1641547329496,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "piano-height"
   },
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "printable-tennis",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547329930,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "printable-tennis"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb3284d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547332674,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "fb3284d8"
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(layers.Layer):  \n",
    "    def __init__(self, num_embeddings, embedding_dim, name='embedding_vqvae', beta=0.25, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = (beta)\n",
    "\n",
    "        # Initialize the embeddings which we will quantize\n",
    "        w_init = tf.random_uniform_initializer()\n",
    "        self.embeddings = tf.Variable(\n",
    "            initial_value=w_init(\n",
    "                shape=(self.embedding_dim, self.num_embeddings), dtype=\"float32\"\n",
    "            ),\n",
    "            trainable=True,\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        flattened = tf.reshape(x, [-1, self.embedding_dim])\n",
    "        \n",
    "        # Quantization\n",
    "        encoding_indices = self.get_code_indices(flattened)\n",
    "        encodings = tf.one_hot(encoding_indices, self.num_embeddings)\n",
    "        quantized = tf.matmul(encodings, self.embeddings, transpose_b=True)\n",
    "        quantized = tf.reshape(quantized, input_shape)\n",
    "\n",
    "        # Calculate vector quantization loss and add that to the layer\n",
    "        commitment_loss = self.beta * tf.reduce_mean(\n",
    "            (tf.stop_gradient(quantized) - x) ** 2\n",
    "        )\n",
    "        codebook_loss = tf.reduce_mean((quantized - tf.stop_gradient(x)) ** 2)\n",
    "        quantization_loss = commitment_loss + codebook_loss\n",
    "        \n",
    "        self.add_loss(0.5 * quantization_loss)\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = x + tf.stop_gradient(quantized - x)\n",
    "        \n",
    "        return quantized\n",
    "\n",
    "    def get_code_indices(self, flattened_inputs):\n",
    "        # Calculate L2-normalized distance between the inputs and the codes\n",
    "        similarity = tf.matmul(flattened_inputs, self.embeddings)\n",
    "        distances = (\n",
    "            tf.reduce_sum(flattened_inputs ** 2, axis=1, keepdims=True)\n",
    "            + tf.reduce_sum(self.embeddings ** 2, axis=0)\n",
    "            - 2 * similarity\n",
    "        )\n",
    "\n",
    "        # Derive the indices for minimum distances\n",
    "        encoding_indices = tf.argmin(distances, axis=1)\n",
    "        \n",
    "        return encoding_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e63776",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547333677,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "d0e63776"
   },
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "class Encoder(layers.Layer):  \n",
    "    def __init__(self, kernel, latent_dim, strides, name = 'encoder',**kwargs):  ## ak==latendt dim : 40 -> latent dim\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=2*latent_dim, kernel_size=kernel, strides=strides, activation='tanh', padding='same')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=latent_dim, kernel_size=kernel, strides=strides, activation='tanh', padding='same')\n",
    "        self.dropout_1 = layers.Dropout(0.2)\n",
    "        self.resblock_1 = ResBlock(filters=latent_dim)\n",
    "#         self.maxpooling1d_1 = layers.MaxPooling1D()\n",
    "#         self.upsampling1d_1 = layers.UpSampling1D()\n",
    "#         self.batchnorm1d_1 = layers.BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.conv1d_2(inputs)\n",
    "        print('conv1d ', logit1.shape)\n",
    "#         logit2 = self.conv1d_2(logit1)\n",
    "#         print('Encoder_conv1d ', logit2.shape)\n",
    "#         logit2 = self.resblock_1(logit1)  \n",
    "        \n",
    "#         logit2 = self.dropout_1(logit1)\n",
    "#         print('Encoder_dropout1d ', logit2.shape)\n",
    "#         logit3 = self.conv1d_2(logit2)\n",
    "#         print('Encoder_conv1d ', logit3.shape)  \n",
    "        \n",
    "        return logit1\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):  \n",
    "    def __init__(self, kernel, latent_dim, strides, activation, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=2*latent_dim, kernel_size=kernel, strides=strides, activation=activation, padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=latent_dim, kernel_size=kernel, strides=strides, activation=activation, padding='same')\n",
    "        self.resblock_1 = ResBlock(filters=latent_dim)\n",
    "#         self.upsampling_1 = layers.UpSampling1D()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.trans_conv1d_2(inputs)\n",
    "        print('transconv1d ', logit1.shape)\n",
    "#         logit2 = self.resblock_1(logit1)\n",
    "#         logit3 = self.trans_conv1d_2(logit)\n",
    "#         print('Decoder_transconv1d ', x.shape)\n",
    "        \n",
    "        return logit1\n",
    "    \n",
    "    \n",
    "class ResBlock(layers.Layer):  \n",
    "    def __init__(self, filters, name = 'resblock', **kwargs):\n",
    "        super(ResBlock, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=filters, kernel_size=1, strides=1, activation='tanh')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        logit1 = self.conv1d_1(inputs)\n",
    "        logit2 = inputs + logit1\n",
    "        \n",
    "        return logit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30fcfafc",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547334774,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "30fcfafc"
   },
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        y_true_flatten = self.overlab_add(y_true)\n",
    "        y_pred_flatten = self.overlab_add(y_pred)\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        labels = y_true_flatten\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred_flatten)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred_flatten = tf.slice(y_pred_flatten, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred_flatten, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred_flatten - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "    \n",
    "    def overlab_add(self, y_pred):\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "\n",
    "        return tf.reshape(y_pred, [batch_size, -1, 1])\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd48a2a7",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547336653,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "fd48a2a7"
   },
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - y_true, 2), axis=[1, 2])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91a366d5",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547339090,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "91a366d5"
   },
   "outputs": [],
   "source": [
    "# Here vnsmsrj\n",
    "\n",
    "def overlab_add(y_pred):\n",
    "    batch_size = tf.shape(y_pred)[0]\n",
    "    \n",
    "    return tf.reshape(y_pred, [batch_size, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "renewable-discharge",
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1641547340148,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "renewable-discharge"
   },
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):   ####\n",
    "    def __init__(self, embedding_dim, num_embeddings, gumbel_hard=False, for_predict=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.for_predict = for_predict\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        self.encoder1 = Encoder(4, 128, 2, name='encoder1')\n",
    "        self.encoder2 = Encoder(4, embedding_dim, 2, name='encoder2')\n",
    "#         self.encoder2_2 = Encoder(4, embedding_dim, 1, name='encoder2_2')\n",
    "        self.quantize1 = VectorQuantizer(num_embeddings, embedding_dim, name='embedding_vqvae1')  \n",
    "        \n",
    "        self.decoder1 = Decoder(4, 128, 2, 'relu', name='decoder1')  \n",
    "#         self.encoder3 = Encoder(1, embedding_dim, 1, name='encoder3')\n",
    "#         self.quantize2 = VectorQuantizer(num_embeddings, embedding_dim, name='embedding_vqvae2')\n",
    "        \n",
    "#         self.decoder2 = Decoder(4, 128, 1, 'tanh', name='decoder2')\n",
    "        self.decoder3 = Decoder(4, 40, 2, None, name='decoder3')\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 40))\n",
    "               \n",
    "        if self.for_predict:\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            one_hot_enc = tf.cast(tf.equal(encode2, tf.math.reduce_max(encode2, 2, keepdims=True)), encode2.dtype)\n",
    "            sample = self.sampled(one_hot_enc)\n",
    "            decode = self.decoder(sample)\n",
    "            \n",
    "        else:  \n",
    "#             print('# Encoder1')\n",
    "#             encode1 = self.encoder1(inputs)\n",
    "#             print('\\n# Encoder2')\n",
    "#             encode2 = self.encoder2(encode1)\n",
    "# #             print('# Encoder2-2')  \n",
    "# #             encode2_2 = self.encoder2_2(encode2)   # top level 추가\n",
    "#             print('\\n# Quantization1')\n",
    "#             quantization1 = self.quantize1(encode2)  \n",
    "#             print('encoder2 ', quantization1.shape)\n",
    "#             print('\\n# Decoder1')\n",
    "#             decode1 = self.decoder1(quantization1)  \n",
    "# #             print('# Decoder1_2')\n",
    "# #             decode1_2 = self.decoder1(decode1)   # new bottom level\n",
    "            \n",
    "#             print('\\n# Enc')\n",
    "# #             enc = layers.Concatenate()([encode1, decode1])\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "#             print('encoder1+decoder1 ', enc.shape)\n",
    "#             print('\\n# Encoder3')\n",
    "#             encode3 = self.encoder3(enc)\n",
    "#             print('\\n# Quantization2')\n",
    "#             quantization2 = self.quantize2(encode3)\n",
    "#             print('encoder3 ', quantization2.shape)\n",
    "#             print('\\n# Decoder2')\n",
    "#             decode2 = self.decoder2(quantization2)\n",
    "            \n",
    "#             print('\\n# Quant')\n",
    "#             quant = layers.Concatenate()([decode2, quantization2])\n",
    "#             print('decoder2+quantization2 ', quant.shape)\n",
    "#             print('\\n# Decoder3')\n",
    "#             decode3 = self.decoder3(quant)\n",
    "            \n",
    "            print(inputs.shape)\n",
    "            print('# Encoder1')\n",
    "            encode1 = self.encoder1(inputs)\n",
    "            print('\\n# Encoder2')\n",
    "            encode2 = self.encoder2(encode1)\n",
    "            print('\\n# Quantization1')\n",
    "            quantization1 = self.quantize1(encode2)  \n",
    "            print('encoder2 ', quantization1.shape)            \n",
    "            print('\\n# Decoder1')\n",
    "            decode1 = self.decoder1(quantization1)\n",
    "#             print('\\n# Quantization2')  #\n",
    "#             quantization2 = self.quzntize2(decode1)  #\n",
    "            print('\\n# Enc')\n",
    "            enc = layers.Concatenate()([encode1, decode1])\n",
    "#             enc = tf.math.multiply(encode1, decode1)\n",
    "            print('encoder1+decoder1 ', enc.shape)\n",
    "            print('\\n# Decoder2')\n",
    "            decode3 = self.decoder3(enc)\n",
    "            \n",
    "        return decode3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adaptive-barbados",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1641547342306,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "adaptive-barbados"
   },
   "outputs": [],
   "source": [
    "def gen_train_data_generator():\n",
    "    for i in range(train_dataset.__len__()):\n",
    "        data = np.squeeze(train_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(train_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)\n",
    "\n",
    "def gen_valid_data_generator():\n",
    "    for i in range(valid_dataset.__len__()):\n",
    "        data = np.squeeze(valid_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(valid_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-frequency",
   "metadata": {
    "id": "vocal-frequency"
   },
   "source": [
    "# 여기는 기존의 .fit() 함수를 사용해서 학습하는 부분임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "referenced-bangkok",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1367135,
     "status": "ok",
     "timestamp": 1641548715874,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "referenced-bangkok",
    "outputId": "86f0bdd5-75fb-4c6a-9bce-d47078edd458",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n",
      "(10, 1848, 40)\n",
      "# Encoder1\n",
      "conv1d  (10, 924, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (10, 462, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder2  (10, 462, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (10, 924, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder1  (10, 924, 256)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (10, 1848, 40)\n",
      "Epoch 1/20\n",
      "(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder2  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder1  (None, None, 256)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 40)\n",
      "(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder2  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder1  (None, None, 256)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 40)\n",
      "192/192 [==============================] - ETA: 0s - loss: 101.6929 - Si-sdr: 8.44 - ETA: 0s - loss: 100.8517 - Si-sdr: 8.5262(None, None, None)\n",
      "# Encoder1\n",
      "conv1d  (None, None, 128)\n",
      "\n",
      "# Encoder2\n",
      "conv1d  (None, None, 64)\n",
      "\n",
      "# Quantization1\n",
      "encoder2  (None, None, 64)\n",
      "\n",
      "# Decoder1\n",
      "transconv1d  (None, None, 128)\n",
      "\n",
      "# Enc\n",
      "encoder1+decoder1  (None, None, 256)\n",
      "\n",
      "# Decoder2\n",
      "transconv1d  (None, None, 40)\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 16s 76ms/step - loss: 100.8517 - Si-sdr: 8.5262 - val_loss: 40.0267 - val_Si-sdr: 11.2415\n",
      "Epoch 2/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 12.2247 - Si-sdr: 17.4475\n",
      "Epoch 00002: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 12.1956 - Si-sdr: 17.4653 - val_loss: 16.1470 - val_Si-sdr: 16.2738\n",
      "Epoch 3/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 4.6744 - Si-sdr: 21.7421 - ETA: 2 - ETA: 0s - loss: 4.7305 - Si-sdr: 2\n",
      "Epoch 00003: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 73ms/step - loss: 4.6744 - Si-sdr: 21.7421 - val_loss: 6.8797 - val_Si-sdr: 20.1180\n",
      "Epoch 4/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 2.0731 - Si-sdr: 25.3145\n",
      "Epoch 00004: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 2.0731 - Si-sdr: 25.3145 - val_loss: 2.6560 - val_Si-sdr: 24.1326\n",
      "Epoch 5/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 1.0721 - Si-sdr: 28.1030\n",
      "Epoch 00005: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 1.0721 - Si-sdr: 28.1030 - val_loss: 1.1723 - val_Si-sdr: 26.9447\n",
      "Epoch 6/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.7150 - Si-sdr: 30.0214\n",
      "Epoch 00006: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.7141 - Si-sdr: 30.0307 - val_loss: 0.6402 - val_Si-sdr: 29.2756\n",
      "Epoch 7/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.5684 - Si-sdr: 31.0774\n",
      "Epoch 00007: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.5694 - Si-sdr: 31.0791 - val_loss: 0.6184 - val_Si-sdr: 29.2985\n",
      "Epoch 8/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.5129 - Si-sdr: 32.0106\n",
      "Epoch 00008: val_loss did not improve from 0.34628\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.5233 - Si-sdr: 31.9726 - val_loss: 0.5144 - val_Si-sdr: 29.8482\n",
      "Epoch 9/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 1.0643 - Si-sdr: 31.2893\n",
      "Epoch 00009: val_loss improved from 0.34628 to 0.34036, saving model to ./CKPT\\CKP_ep_9__loss_0.34036_.h5\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 1.0643 - Si-sdr: 31.2893 - val_loss: 0.3404 - val_Si-sdr: 31.9820\n",
      "Epoch 10/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.3653 - Si-sdr: 33.3771\n",
      "Epoch 00010: val_loss improved from 0.34036 to 0.30364, saving model to ./CKPT\\CKP_ep_10__loss_0.30364_.h5\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.3653 - Si-sdr: 33.3771 - val_loss: 0.3036 - val_Si-sdr: 32.5341\n",
      "Epoch 11/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.6746 - Si-sdr: 33.1120\n",
      "Epoch 00011: val_loss improved from 0.30364 to 0.28216, saving model to ./CKPT\\CKP_ep_11__loss_0.28216_.h5\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.6725 - Si-sdr: 33.1208 - val_loss: 0.2822 - val_Si-sdr: 32.8029\n",
      "Epoch 12/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.3482 - Si-sdr: 33.6146\n",
      "Epoch 00012: val_loss did not improve from 0.28216\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.3502 - Si-sdr: 33.6068 - val_loss: 1.1270 - val_Si-sdr: 27.3847\n",
      "Epoch 13/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.8336 - Si-sdr: 33.1073\n",
      "Epoch 00013: val_loss did not improve from 0.28216\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.8336 - Si-sdr: 33.1073 - val_loss: 1.3533 - val_Si-sdr: 26.4321\n",
      "Epoch 14/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.3579 - Si-sdr: 33.9300\n",
      "Epoch 00014: val_loss improved from 0.28216 to 0.22705, saving model to ./CKPT\\CKP_ep_14__loss_0.22705_.h5\n",
      "192/192 [==============================] - 14s 75ms/step - loss: 0.3579 - Si-sdr: 33.9300 - val_loss: 0.2270 - val_Si-sdr: 33.9376\n",
      "Epoch 15/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.3764 - Si-sdr: 33.8850\n",
      "Epoch 00015: val_loss did not improve from 0.22705\n",
      "192/192 [==============================] - 14s 75ms/step - loss: 0.3762 - Si-sdr: 33.8896 - val_loss: 0.2474 - val_Si-sdr: 33.6159\n",
      "Epoch 16/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.3799 - Si-sdr: 34.1000\n",
      "Epoch 00016: val_loss did not improve from 0.22705\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.3799 - Si-sdr: 34.1000 - val_loss: 0.2965 - val_Si-sdr: 32.4256\n",
      "Epoch 17/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 1.2953 - Si-sdr: 33.1424\n",
      "Epoch 00017: val_loss did not improve from 0.22705\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 1.2953 - Si-sdr: 33.1424 - val_loss: 0.2430 - val_Si-sdr: 33.4283\n",
      "Epoch 18/20\n",
      "191/192 [============================>.] - ETA: 0s - loss: 0.2563 - Si-sdr: 34.8256\n",
      "Epoch 00018: val_loss improved from 0.22705 to 0.22533, saving model to ./CKPT\\CKP_ep_18__loss_0.22533_.h5\n",
      "192/192 [==============================] - 14s 74ms/step - loss: 0.2563 - Si-sdr: 34.8311 - val_loss: 0.2253 - val_Si-sdr: 33.9510\n",
      "Epoch 19/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.2749 - Si-sdr: 34.8507\n",
      "Epoch 00019: val_loss did not improve from 0.22533\n",
      "192/192 [==============================] - 15s 76ms/step - loss: 0.2749 - Si-sdr: 34.8507 - val_loss: 0.2451 - val_Si-sdr: 33.0738\n",
      "Epoch 20/20\n",
      "192/192 [==============================] - ETA: 0s - loss: 0.6805 - Si-sdr: 34.1937\n",
      "Epoch 00020: val_loss improved from 0.22533 to 0.19617, saving model to ./CKPT\\CKP_ep_20__loss_0.19617_.h5\n",
      "192/192 [==============================] - 15s 76ms/step - loss: 0.6805 - Si-sdr: 34.1937 - val_loss: 0.1962 - val_Si-sdr: 34.6436\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "epoch = 20\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['/cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_594__loss_229.89435_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(embedding_dim, num_embeddings, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "#     vq_vae(0, True)\n",
    "#     vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "    \n",
    "    \n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7461738a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sisdr = history.history['Si-sdr'] # Training loss.\n",
    "val_sisdr = history.history['val_Si-sdr'] # Validation loss.\n",
    "num_epochs = range(1, 1 + len(history.history['Si-sdr'])) # Number of training epochs.\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.plot(num_epochs, sisdr, label='Training Si-sdr') # Plot training loss.\n",
    "plt.plot(num_epochs, val_sisdr, label='Validation Si-sdr') # Plot validation loss.\n",
    "\n",
    "plt.title('Training and validation Si-sdr')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "b726db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sisdr32 = history.history['Si-sdr']\n",
    "val_sisdr32= history.history['val_Si-sdr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {
    "id": "amber-overhead"
   },
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-architect",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 725,
     "status": "error",
     "timestamp": 1641453129862,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "annoying-architect",
    "outputId": "2f6b0d49-da97-48b5-8d53-e9fb91969713"
   },
   "outputs": [],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, num_embeddings, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {
    "id": "19cf7a69"
   },
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c06eb91",
   "metadata": {
    "id": "3c06eb91"
   },
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create .//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73b6a2",
   "metadata": {
    "id": "2e73b6a2"
   },
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be7f5d",
   "metadata": {
    "id": "64be7f5d"
   },
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc186e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "executionInfo": {
     "elapsed": 256,
     "status": "error",
     "timestamp": 1641453176295,
     "user": {
      "displayName": "권세린",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5MKjbRw1u6vei2YvVggGQrVj8p1crrjGM4ZQGFA=s64",
      "userId": "18353460218660294941"
     },
     "user_tz": -540
    },
    "id": "a7cc186e",
    "outputId": "eca7491b-132c-4df3-dd9e-0263dae19bdc"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3e58df71e698>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvq_vae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVq_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor_predict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvq_vae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvq_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-aa1699be7fe5>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, load)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mencode2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mone_hot_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencode2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Exception encountered when calling layer \"vqvae\" (type Vq_vae).\n\n'Vq_vae' object has no attribute 'sampled'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(), dtype=int32)\n  • load=True"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 1024\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_293__loss_49.28763_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, num_embeddings, for_predict=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {
    "id": "7745f214"
   },
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-notebook",
   "metadata": {
    "id": "temporal-notebook"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=20, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-highlight",
   "metadata": {
    "id": "baking-highlight",
    "outputId": "58f8e3dc-7afd-443f-d7cf-a54911883f35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 9.221684, 10.636035], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(output_array, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-circular",
   "metadata": {
    "id": "separate-circular",
    "outputId": "1c43ebfa-8049-4f80-bc60-923564a4f038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.05127785,  0.04667316,  0.02635628, ...,  0.04573939,\n",
       "          0.01375185, -0.01533533],\n",
       "        [ 0.01121127,  0.06675138,  0.0021627 , ...,  0.07490488,\n",
       "          0.07613068,  0.02180689]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.08263874,  0.03258345,  0.01568549, ...,  0.03589562,\n",
       "         -0.01625197, -0.03177397],\n",
       "        [ 0.13691738,  0.09397386, -0.01036285, ..., -0.00614451,\n",
       "          0.05317175, -0.01284541]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07028106,  0.00527185,  0.01043324, ...,  0.0617263 ,\n",
       "         -0.03244079, -0.01786789],\n",
       "        [ 0.10520705,  0.042405  ,  0.03726472, ...,  0.07816655,\n",
       "          0.03663039, -0.03122897]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07235897,  0.00636261, -0.03309729, ...,  0.05743181,\n",
       "         -0.01674554, -0.01104611],\n",
       "        [ 0.11989237,  0.04727669,  0.04978402, ...,  0.10254725,\n",
       "         -0.0322747 , -0.04379546]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.06034716,  0.02196031, -0.03286755, ...,  0.04720719,\n",
       "          0.02011653,  0.01457734],\n",
       "        [ 0.08640987,  0.05295723, -0.05331329, ...,  0.05878888,\n",
       "         -0.11385743, -0.11239739]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.03044085,  0.01922614, -0.01506157, ...,  0.0429484 ,\n",
       "          0.03525994,  0.01746666],\n",
       "        [ 0.1105442 ,  0.06415009, -0.11616933, ...,  0.12055975,\n",
       "         -0.0501071 , -0.02813609]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 5.68038262e-02,  5.34826368e-02,  1.02059479e-04, ...,\n",
       "          2.20218338e-02,  5.25824353e-02,  1.43096512e-02],\n",
       "        [ 1.03822105e-01,  5.07594347e-02, -1.07106149e-01, ...,\n",
       "          1.95540398e-01,  1.40878245e-01,  3.50491852e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[-0.01148595,  0.03309207, -0.02289026, ...,  0.02078854,\n",
       "         -0.06813245, -0.09970599],\n",
       "        [ 0.04621019,  0.06871783,  0.00798892, ...,  0.01603857,\n",
       "          0.04825678,  0.01381727]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07415138,  0.0991153 , -0.03032039, ...,  0.01240078,\n",
       "         -0.04231954, -0.04340101],\n",
       "        [ 0.0583101 ,  0.0205835 , -0.01229584, ..., -0.01952541,\n",
       "          0.00030562, -0.05487346]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.02075784,  0.00618587, -0.07708566, ...,  0.10809869,\n",
       "          0.01363952,  0.02574195],\n",
       "        [-0.00304264, -0.00650905, -0.00512165, ...,  0.01797911,\n",
       "         -0.00719647, -0.02386911]], dtype=float32)>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(output_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba32a5d",
   "metadata": {
    "id": "4ba32a5d",
    "outputId": "5ef9064e-4583-4776-f190-a3e7f0d15979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.05127785  0.04667316  0.02635628 ...  0.04573939  0.01375185\n",
      "   -0.01533533]\n",
      "  [ 0.08263874  0.03258345  0.01568549 ...  0.03589562 -0.01625197\n",
      "   -0.03177397]\n",
      "  [ 0.07028106  0.00527185  0.01043324 ...  0.0617263  -0.03244079\n",
      "   -0.01786789]\n",
      "  ...\n",
      "  [-0.01148595  0.03309207 -0.02289026 ...  0.02078854 -0.06813245\n",
      "   -0.09970599]\n",
      "  [ 0.07415138  0.0991153  -0.03032039 ...  0.01240078 -0.04231954\n",
      "   -0.04340101]\n",
      "  [ 0.02075784  0.00618587 -0.07708566 ...  0.10809869  0.01363952\n",
      "    0.02574195]]\n",
      "\n",
      " [[ 0.01121127  0.06675138  0.0021627  ...  0.07490488  0.07613068\n",
      "    0.02180689]\n",
      "  [ 0.13691738  0.09397386 -0.01036285 ... -0.00614451  0.05317175\n",
      "   -0.01284541]\n",
      "  [ 0.10520705  0.042405    0.03726472 ...  0.07816655  0.03663039\n",
      "   -0.03122897]\n",
      "  ...\n",
      "  [ 0.04621019  0.06871783  0.00798892 ...  0.01603857  0.04825678\n",
      "    0.01381727]\n",
      "  [ 0.0583101   0.0205835  -0.01229584 ... -0.01952541  0.00030562\n",
      "   -0.05487346]\n",
      "  [-0.00304264 -0.00650905 -0.00512165 ...  0.01797911 -0.00719647\n",
      "   -0.02386911]]]\n",
      "(2, 10, 512)\n",
      "(2, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-dispatch",
   "metadata": {
    "id": "backed-dispatch",
    "outputId": "903333a9-a2c2-4a47-fded-0e3941b1a314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[259 259  34  50   0 110   0 305 200 287]\n",
      " [257   0 200 259  34 200 509 110 200 287]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype), axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)\n",
    "# layers.Embedding(512, 512)(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-coupon",
   "metadata": {
    "id": "liable-coupon",
    "outputId": "504f0d56-afee-4014-d8e6-b85117827ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[228 249 283 206  20 206 435  32 270  30]\n",
      " [428 206  20 244 357 289 324 249 498 134]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(output_array, axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-trading",
   "metadata": {
    "id": "bacterial-trading",
    "outputId": "3de03c4a-928a-4b5a-eba2-50fe561b5321"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.012074914, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "qy = tf.nn.softmax(output_array)\n",
    "log_qy = tf.math.log(qy + 1e-10)\n",
    "log_uniform = qy * (log_qy - tf.math.log(1.0 / 512))\n",
    "kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8e699",
   "metadata": {
    "id": "fbc8e699",
    "outputId": "238a3c2b-5691-4eca-e8ed-7acd245d470e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n",
      "tf.Tensor(2.8309882, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "noise = output_array2 - target\n",
    "si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "si_sdr = tf.reduce_mean(si_sdr)\n",
    "print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-drink",
   "metadata": {
    "id": "clean-drink",
    "outputId": "1c57884c-ed2f-4cd9-eb7f-791ceaee09a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153f9e7c",
   "metadata": {
    "id": "153f9e7c",
    "outputId": "814970f9-a907-474c-f4b1-6ec3445d138c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[-0.03009652, -0.03612775, -0.06680483, -0.03670201],\n",
       "        [-0.04768711, -0.12344762, -0.03924457, -0.11762322],\n",
       "        [ 0.01808495, -0.16106637, -0.19467078, -0.15282159],\n",
       "        [-0.0986427 , -0.08625205, -0.12661007, -0.16366175],\n",
       "        [-0.09758376, -0.08886974, -0.0433558 , -0.19985165],\n",
       "        [-0.06933096, -0.03154394, -0.13725929, -0.20143284],\n",
       "        [ 0.03375649,  0.00182091, -0.01022564, -0.35924646],\n",
       "        [-0.01645333, -0.10466891, -0.13975918, -0.12066491],\n",
       "        [-0.13588801, -0.08173112, -0.00253745, -0.28615874]],\n",
       "\n",
       "       [[ 0.04865369, -0.02880372, -0.06414615, -0.07730438],\n",
       "        [-0.08225074, -0.03192509, -0.06216412, -0.08035193],\n",
       "        [-0.09515338,  0.04221668,  0.14230826, -0.23082384],\n",
       "        [-0.00094383,  0.05597762, -0.09290768, -0.08630683],\n",
       "        [-0.09894791, -0.04727853, -0.01004983, -0.30325216],\n",
       "        [ 0.01705559, -0.16948727, -0.08829505, -0.16453639],\n",
       "        [-0.07230186, -0.15348263, -0.06832955, -0.09588489],\n",
       "        [-0.1258373 ,  0.02068143,  0.07559443, -0.19079593],\n",
       "        [-0.08558049, -0.07712768, -0.07924994, -0.07352428]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044d110",
   "metadata": {
    "id": "5044d110",
    "outputId": "45895281-ca9c-4fb8-c823-d0dac1afe7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4122a51",
   "metadata": {
    "id": "c4122a51",
    "outputId": "d9fedcb5-b498-4c12-ada8-f3f5ab813904"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31fcf6",
   "metadata": {
    "id": "1d31fcf6",
    "outputId": "f19a2c99-c184-4df2-cf6e-95078bf110ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c293c1",
   "metadata": {
    "id": "01c293c1",
    "outputId": "0aaa46e4-44e8-40d1-8d8e-7eef008a5664"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703002f",
   "metadata": {
    "id": "7703002f",
    "outputId": "2b2ee4ed-0bc6-49f7-c43e-e109c44d3c1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-coral",
   "metadata": {
    "id": "constitutional-coral",
    "outputId": "18feced3-6818-4fa7-badb-06330d71e01e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-distinction",
   "metadata": {
    "id": "prompt-distinction",
    "outputId": "b52d84f6-5c22-4780-9fca-a37770195961"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-flash",
   "metadata": {
    "id": "treated-flash",
    "outputId": "3d37a8f1-53f9-4a9f-cf62-a15e4b1861e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-citizen",
   "metadata": {
    "id": "fifth-citizen",
    "outputId": "7efecbef-fb5e-4f4a-8be6-66413cf6d84a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {
    "id": "dac75a02"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7745f214"
   ],
   "name": "Engineering_vq-vae_for_1d_data_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "'tensorflow_1'",
   "language": "python",
   "name": "tensorlow_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
