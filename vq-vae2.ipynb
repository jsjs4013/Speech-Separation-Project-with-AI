{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=1.0, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=256, kernel_size=3, strides=2, padding='same', activation='relu')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=256, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=3, strides=2, padding='same', activation=None)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_mean(tf.pow(y_pred - labels, 2))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim) # 512\n",
    "        self.decoder = Decoder(latent_dim) # 512\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        encode = self.encoder(inputs) # 512\n",
    "        gumbel = self.gumbel(encode) # 512\n",
    "        decode = self.decoder(gumbel) \n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-20)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_mean(log_uniform)\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_2 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.0044 - Si-sdr: -53.9975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\qkrwo\\anaconda3\\envs\\nlp_task\\lib\\site-packages\\keras\\metrics.py:257: UserWarning: Metric SiSdr implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  'consistency.' % (self.__class__.__name__,))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 2s/step - loss: 0.0044 - Si-sdr: -53.9975 - val_loss: 0.0047 - val_Si-sdr: -60.1068\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.00472, saving model to ./CKPT\\CKP_ep_1__loss_0.00472_.h5\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 1s 590ms/step - loss: 0.0047 - Si-sdr: -49.5316 - val_loss: 0.0047 - val_Si-sdr: -61.1810\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00472 to 0.00469, saving model to ./CKPT\\CKP_ep_2__loss_0.00469_.h5\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 0.0043 - Si-sdr: -55.7993 - val_loss: 0.0043 - val_Si-sdr: -57.9440\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00469 to 0.00434, saving model to ./CKPT\\CKP_ep_3__loss_0.00434_.h5\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 0.0047 - Si-sdr: -59.2706 - val_loss: 0.0043 - val_Si-sdr: -62.7593\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00434 to 0.00434, saving model to ./CKPT\\CKP_ep_4__loss_0.00434_.h5\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 1s 619ms/step - loss: 0.0045 - Si-sdr: -56.3328 - val_loss: 0.0047 - val_Si-sdr: -57.0345\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00434\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 1s 691ms/step - loss: 0.0043 - Si-sdr: -47.8721 - val_loss: 0.0047 - val_Si-sdr: -60.4655\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00434\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 1s 621ms/step - loss: 0.0045 - Si-sdr: -57.8176 - val_loss: 0.0047 - val_Si-sdr: -56.5186\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00434\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 1s 622ms/step - loss: 0.0045 - Si-sdr: -55.2893 - val_loss: 0.0047 - val_Si-sdr: -52.0559\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00434\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 0.0045 - Si-sdr: -58.0495 - val_loss: 0.0047 - val_Si-sdr: -53.1563\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00434\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 1s 739ms/step - loss: 0.0043 - Si-sdr: -53.0187 - val_loss: 0.0045 - val_Si-sdr: -44.0555\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00434\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 1s 616ms/step - loss: 0.0047 - Si-sdr: -50.0434 - val_loss: 0.0045 - val_Si-sdr: -49.9693\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00434\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 0.0043 - Si-sdr: -47.7220 - val_loss: 0.0047 - val_Si-sdr: -54.0509\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.00434\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 0.0043 - Si-sdr: -54.7650 - val_loss: 0.0043 - val_Si-sdr: -54.3792\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00434 to 0.00432, saving model to ./CKPT\\CKP_ep_13__loss_0.00432_.h5\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 1s 701ms/step - loss: 0.0047 - Si-sdr: -47.0335 - val_loss: 0.0043 - val_Si-sdr: -51.3802\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_14__loss_0.00432_.h5\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 0.0047 - Si-sdr: -66.2373 - val_loss: 0.0047 - val_Si-sdr: -52.8538\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00432\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 1s 642ms/step - loss: 0.0043 - Si-sdr: -49.8207 - val_loss: 0.0045 - val_Si-sdr: -55.7958\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00432\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 0.0043 - Si-sdr: -48.0582 - val_loss: 0.0045 - val_Si-sdr: -48.1866\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00432\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 0.0043 - Si-sdr: -52.4749 - val_loss: 0.0043 - val_Si-sdr: -54.3839\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_18__loss_0.00432_.h5\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 0.0043 - Si-sdr: -57.3314 - val_loss: 0.0043 - val_Si-sdr: -55.7164\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00432\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 0.0043 - Si-sdr: -48.6193 - val_loss: 0.0045 - val_Si-sdr: -54.1792\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00432\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 0.0047 - Si-sdr: -50.2826 - val_loss: 0.0045 - val_Si-sdr: -49.0075\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00432\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 1s 607ms/step - loss: 0.0047 - Si-sdr: -55.9988 - val_loss: 0.0047 - val_Si-sdr: -48.8588\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00432\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 0.0047 - Si-sdr: -50.0555 - val_loss: 0.0043 - val_Si-sdr: -51.5363\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_23__loss_0.00432_.h5\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.0047 - Si-sdr: -59.4402 - val_loss: 0.0047 - val_Si-sdr: -51.2680\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00432\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 1s 641ms/step - loss: 0.0045 - Si-sdr: -67.3299 - val_loss: 0.0045 - val_Si-sdr: -51.2734\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00432\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 1s 629ms/step - loss: 0.0043 - Si-sdr: -46.5156 - val_loss: 0.0045 - val_Si-sdr: -53.9383\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00432\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 0.0043 - Si-sdr: -52.2544 - val_loss: 0.0043 - val_Si-sdr: -54.0655\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00432\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 1s 632ms/step - loss: 0.0047 - Si-sdr: -46.2948 - val_loss: 0.0043 - val_Si-sdr: -50.9733\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00432\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 0.0045 - Si-sdr: -52.9681 - val_loss: 0.0045 - val_Si-sdr: -45.5641\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.00432\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 1s 637ms/step - loss: 0.0045 - Si-sdr: -44.7049 - val_loss: 0.0045 - val_Si-sdr: -54.0807\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00432\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 0.0043 - Si-sdr: -48.5713 - val_loss: 0.0047 - val_Si-sdr: -51.4147\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00432\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.0047 - Si-sdr: -53.8338 - val_loss: 0.0045 - val_Si-sdr: -48.0090\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00432\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 0.0043 - Si-sdr: -55.6274 - val_loss: 0.0047 - val_Si-sdr: -53.7773\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.00432\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 0.0045 - Si-sdr: -63.4019 - val_loss: 0.0045 - val_Si-sdr: -57.5175\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00432\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 1s 705ms/step - loss: 0.0045 - Si-sdr: -55.0910 - val_loss: 0.0045 - val_Si-sdr: -49.0577\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00432\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 0.0043 - Si-sdr: -57.0110 - val_loss: 0.0045 - val_Si-sdr: -56.8513\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00432\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 0.0045 - Si-sdr: -48.1503 - val_loss: 0.0045 - val_Si-sdr: -54.8743\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00432\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 0.0043 - Si-sdr: -61.2202 - val_loss: 0.0047 - val_Si-sdr: -53.8794\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00432\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 0.0047 - Si-sdr: -57.8902 - val_loss: 0.0043 - val_Si-sdr: -47.7493\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_39__loss_0.00432_.h5\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 1s 644ms/step - loss: 0.0045 - Si-sdr: -56.6674 - val_loss: 0.0043 - val_Si-sdr: -51.4168\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_40__loss_0.00432_.h5\n",
      "Epoch 41/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 672ms/step - loss: 0.0045 - Si-sdr: -65.1194 - val_loss: 0.0045 - val_Si-sdr: -53.9765\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.00432\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 0.0045 - Si-sdr: -46.8984 - val_loss: 0.0045 - val_Si-sdr: -50.9471\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.00432\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 1s 618ms/step - loss: 0.0043 - Si-sdr: -52.6457 - val_loss: 0.0047 - val_Si-sdr: -48.3479\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.00432\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.0043 - Si-sdr: -47.3449 - val_loss: 0.0047 - val_Si-sdr: -47.1678\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.00432\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 0.0047 - Si-sdr: -49.9312 - val_loss: 0.0045 - val_Si-sdr: -48.5689\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.00432\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 1s 626ms/step - loss: 0.0045 - Si-sdr: -52.2713 - val_loss: 0.0047 - val_Si-sdr: -55.5899\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.00432\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 1s 643ms/step - loss: 0.0043 - Si-sdr: -51.7112 - val_loss: 0.0045 - val_Si-sdr: -55.8036\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.00432\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 1s 585ms/step - loss: 0.0047 - Si-sdr: -54.1849 - val_loss: 0.0045 - val_Si-sdr: -52.7579\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.00432\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 0.0045 - Si-sdr: -50.5564 - val_loss: 0.0043 - val_Si-sdr: -48.2584\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.00432\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 0.0043 - Si-sdr: -50.3320 - val_loss: 0.0043 - val_Si-sdr: -51.7056\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.00432\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 1s 577ms/step - loss: 0.0047 - Si-sdr: -51.6559 - val_loss: 0.0047 - val_Si-sdr: -68.2158\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.00432\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 0.0045 - Si-sdr: -57.1078 - val_loss: 0.0043 - val_Si-sdr: -44.3362\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_52__loss_0.00432_.h5\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.0043 - Si-sdr: -48.6225 - val_loss: 0.0043 - val_Si-sdr: -47.6636\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.00432\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 0.0043 - Si-sdr: -52.8459 - val_loss: 0.0047 - val_Si-sdr: -51.5559\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.00432\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 0.0043 - Si-sdr: -55.3033 - val_loss: 0.0047 - val_Si-sdr: -56.1830\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.00432\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 0.0045 - Si-sdr: -52.5177 - val_loss: 0.0043 - val_Si-sdr: -61.8400\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.00432\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 0.0045 - Si-sdr: -48.0077 - val_loss: 0.0045 - val_Si-sdr: -53.6575\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.00432\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 1s 739ms/step - loss: 0.0047 - Si-sdr: -50.1280 - val_loss: 0.0045 - val_Si-sdr: -58.0609\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.00432\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 1s 662ms/step - loss: 0.0043 - Si-sdr: -48.9276 - val_loss: 0.0047 - val_Si-sdr: -52.1076\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.00432\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 1s 726ms/step - loss: 0.0043 - Si-sdr: -48.6563 - val_loss: 0.0047 - val_Si-sdr: -53.4749\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.00432\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 1s 745ms/step - loss: 0.0045 - Si-sdr: -48.8496 - val_loss: 0.0043 - val_Si-sdr: -45.5189\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.00432\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 0.0043 - Si-sdr: -54.4638 - val_loss: 0.0047 - val_Si-sdr: -46.7676\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.00432\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 1s 652ms/step - loss: 0.0045 - Si-sdr: -51.4217 - val_loss: 0.0047 - val_Si-sdr: -49.0193\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.00432\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 1s 606ms/step - loss: 0.0047 - Si-sdr: -51.6647 - val_loss: 0.0043 - val_Si-sdr: -45.9830\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.00432\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 1s 645ms/step - loss: 0.0045 - Si-sdr: -47.2416 - val_loss: 0.0045 - val_Si-sdr: -48.0637\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.00432\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 1s 605ms/step - loss: 0.0047 - Si-sdr: -49.2009 - val_loss: 0.0047 - val_Si-sdr: -54.6097\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.00432\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 0.0047 - Si-sdr: -57.1639 - val_loss: 0.0043 - val_Si-sdr: -51.1206\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.00432\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 0.0043 - Si-sdr: -52.4274 - val_loss: 0.0043 - val_Si-sdr: -45.8523\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.00432 to 0.00432, saving model to ./CKPT\\CKP_ep_68__loss_0.00432_.h5\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 0.0043 - Si-sdr: -53.5706 - val_loss: 0.0047 - val_Si-sdr: -56.9464\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.00432\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 1s 611ms/step - loss: 0.0047 - Si-sdr: -58.4785 - val_loss: 0.0043 - val_Si-sdr: -53.1416\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.00432\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 1s 625ms/step - loss: 0.0043 - Si-sdr: -54.1529 - val_loss: 0.0047 - val_Si-sdr: -54.2974\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.00432\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 0.0045 - Si-sdr: -46.9650 - val_loss: 0.0045 - val_Si-sdr: -52.7082\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.00432\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 0.0045 - Si-sdr: -49.5257 - val_loss: 0.0043 - val_Si-sdr: -56.0628\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.00432\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 0.0043 - Si-sdr: -54.6326 - val_loss: 0.0047 - val_Si-sdr: -48.4974\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.00432\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 0.0045 - Si-sdr: -54.3608 - val_loss: 0.0043 - val_Si-sdr: -46.8274\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.00432\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 1s 689ms/step - loss: 0.0047 - Si-sdr: -55.6993 - val_loss: 0.0047 - val_Si-sdr: -46.7215\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.00432\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 1s 727ms/step - loss: 0.0043 - Si-sdr: -64.6544 - val_loss: 0.0043 - val_Si-sdr: -55.1287\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.00432\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 1s 635ms/step - loss: 0.0045 - Si-sdr: -62.5552 - val_loss: 0.0045 - val_Si-sdr: -48.0219\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.00432\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.0043 - Si-sdr: -47.8649 - val_loss: 0.0043 - val_Si-sdr: -53.5533\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.00432\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 1s 638ms/step - loss: 0.0043 - Si-sdr: -53.6872 - val_loss: 0.0043 - val_Si-sdr: -45.4774\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.00432\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 1s 636ms/step - loss: 0.0045 - Si-sdr: -68.4131 - val_loss: 0.0043 - val_Si-sdr: -52.9889\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.00432\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 1s 631ms/step - loss: 0.0043 - Si-sdr: -44.6486 - val_loss: 0.0045 - val_Si-sdr: -45.3510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00082: val_loss did not improve from 0.00432\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 1s 700ms/step - loss: 0.0047 - Si-sdr: -49.4807 - val_loss: 0.0047 - val_Si-sdr: -46.9932\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.00432\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 0.0043 - Si-sdr: -44.9780 - val_loss: 0.0045 - val_Si-sdr: -54.0653\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.00432\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 0.0043 - Si-sdr: -48.8215 - val_loss: 0.0043 - val_Si-sdr: -53.8353\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.00432\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 1s 647ms/step - loss: 0.0045 - Si-sdr: -54.7647 - val_loss: 0.0045 - val_Si-sdr: -52.7919\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.00432\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 1s 684ms/step - loss: 0.0043 - Si-sdr: -47.1041 - val_loss: 0.0047 - val_Si-sdr: -49.6886\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.00432\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 0.0043 - Si-sdr: -55.7255 - val_loss: 0.0047 - val_Si-sdr: -53.7051\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.00432\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 0.0045 - Si-sdr: -56.2653 - val_loss: 0.0047 - val_Si-sdr: -48.4837\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.00432\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 1s 603ms/step - loss: 0.0047 - Si-sdr: -46.0115 - val_loss: 0.0043 - val_Si-sdr: -46.9391\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.00432\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 0.0043 - Si-sdr: -49.2094 - val_loss: 0.0045 - val_Si-sdr: -50.2529\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.00432\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 1s 614ms/step - loss: 0.0047 - Si-sdr: -61.1046 - val_loss: 0.0043 - val_Si-sdr: -60.5761\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.00432\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 1s 690ms/step - loss: 0.0043 - Si-sdr: -51.3792 - val_loss: 0.0047 - val_Si-sdr: -56.2698\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.00432\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 1s 719ms/step - loss: 0.0043 - Si-sdr: -47.1717 - val_loss: 0.0043 - val_Si-sdr: -50.8060\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.00432\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 1s 656ms/step - loss: 0.0045 - Si-sdr: -55.5025 - val_loss: 0.0043 - val_Si-sdr: -49.2684\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.00432\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 0.0043 - Si-sdr: -46.2990 - val_loss: 0.0045 - val_Si-sdr: -50.1574\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.00432\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 1s 600ms/step - loss: 0.0047 - Si-sdr: -59.2774 - val_loss: 0.0047 - val_Si-sdr: -43.3020\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.00432\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 1s 639ms/step - loss: 0.0043 - Si-sdr: -48.8377 - val_loss: 0.0045 - val_Si-sdr: -61.8041\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.00432\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 1s 592ms/step - loss: 0.0047 - Si-sdr: -54.8397 - val_loss: 0.0043 - val_Si-sdr: -49.7574\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.00432\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 1s 624ms/step - loss: 0.0047 - Si-sdr: -55.2844 - val_loss: 0.0047 - val_Si-sdr: -48.0760\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.00432\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 1s 633ms/step - loss: 0.0043 - Si-sdr: -47.6497 - val_loss: 0.0047 - val_Si-sdr: -47.4169\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.00432\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 0.0043 - Si-sdr: -53.7872 - val_loss: 0.0047 - val_Si-sdr: -50.7442\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.00432\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 1s 630ms/step - loss: 0.0043 - Si-sdr: -55.0763 - val_loss: 0.0043 - val_Si-sdr: -47.7069\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.00432\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 0.0045 - Si-sdr: -55.5759 - val_loss: 0.0047 - val_Si-sdr: -50.7433\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.00432\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 1s 620ms/step - loss: 0.0043 - Si-sdr: -55.6027 - val_loss: 0.0047 - val_Si-sdr: -45.0219\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.00432\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 0.0047 - Si-sdr: -53.5613 - val_loss: 0.0047 - val_Si-sdr: -44.4928\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.00432\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 1s 646ms/step - loss: 0.0043 - Si-sdr: -48.9759 - val_loss: 0.0047 - val_Si-sdr: -49.3879\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.00432\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 0.0047 - Si-sdr: -55.2316 - val_loss: 0.0045 - val_Si-sdr: -57.8788\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.00432\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 0.0045 - Si-sdr: -50.9911 - val_loss: 0.0047 - val_Si-sdr: -52.5361\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.00432\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 0.0047 - Si-sdr: -48.9799 - val_loss: 0.0047 - val_Si-sdr: -48.6587\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.00432\n",
      "Epoch 111/300\n",
      "2/2 [==============================] - 1s 653ms/step - loss: 0.0045 - Si-sdr: -45.7643 - val_loss: 0.0047 - val_Si-sdr: -45.0151\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.00432\n",
      "Epoch 112/300\n",
      "2/2 [==============================] - 1s 650ms/step - loss: 0.0043 - Si-sdr: -50.5082 - val_loss: 0.0047 - val_Si-sdr: -49.1734\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.00432\n",
      "Epoch 113/300\n",
      "2/2 [==============================] - 1s 765ms/step - loss: 0.0043 - Si-sdr: -50.5676 - val_loss: 0.0047 - val_Si-sdr: -52.5379\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.00432\n",
      "Epoch 114/300\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 0.0043 - Si-sdr: -59.0069 - val_loss: 0.0045 - val_Si-sdr: -45.9051\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.00432\n",
      "Epoch 115/300\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 0.0045 - Si-sdr: -53.1295 - val_loss: 0.0047 - val_Si-sdr: -49.9009\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.00432\n",
      "Epoch 116/300\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 0.0047 - Si-sdr: -54.4402 - val_loss: 0.0045 - val_Si-sdr: -53.3705\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.00432\n",
      "Epoch 117/300\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 0.0043 - Si-sdr: -50.3672 - val_loss: 0.0045 - val_Si-sdr: -63.1068\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.00432\n",
      "Epoch 118/300\n",
      "2/2 [==============================] - 1s 762ms/step - loss: 0.0043 - Si-sdr: -50.4672 - val_loss: 0.0043 - val_Si-sdr: -49.6726\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.00432\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00118: early stopping\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 300\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_30__loss_158.95885_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = vq_vae.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-gregory",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "clear-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_2 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_3 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 512\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_262__loss_142.70529_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 연습장 INAVSION~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8314750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.util.math_function import create_padding_mask, create_look_ahead_mask\n",
    "def create_masks(inp, tar, length=None):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp, length) # (batch, 1, 1, seq_len)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp, length)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by\n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]) # (seq_len, seq_len)\n",
    "    dec_target_padding_mask = create_padding_mask(tar, length) # (batch, 1, 1, seq_len)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch, 1, seq_len, seq_len)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "33fbe1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_1 (Softmax)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "\n",
    "vq_vae = Vq_vae(latent_size, gumbel_hard=True)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "\n",
    "vq_vae(0, True)\n",
    "vq_vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 80000, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = vq_vae(next(iter(train_dataset))[0])\n",
    "encode_onehot = tf.cast(tf.equal(outputs, tf.math.reduce_max(outputs, 2, keepdims=True)), outputs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 80000, 1), dtype=float32, numpy=\n",
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.14930329, -0.05488385, -0.03165275,  0.02511186],\n",
       "        [-0.04538564, -0.02340828, -0.13736942,  0.03185076],\n",
       "        [-0.18286747, -0.01420305,  0.02583414,  0.20739384],\n",
       "        [-0.15128048, -0.15295134,  0.00826486, -0.03651741],\n",
       "        [-0.19406578, -0.14820886, -0.24136184,  0.01954714],\n",
       "        [-0.11608941,  0.05977409, -0.05805521,  0.08262399],\n",
       "        [-0.10437049,  0.00281959,  0.00744648,  0.06939161],\n",
       "        [-0.32889026, -0.11109954, -0.06513101,  0.02347144],\n",
       "        [-0.10914627,  0.04005037, -0.11194003,  0.34588984],\n",
       "        [-0.55321944, -0.24241613, -0.17234027,  0.2255739 ]],\n",
       "\n",
       "       [[-0.12665638, -0.02560038, -0.07357763, -0.03386865],\n",
       "        [-0.00296582,  0.05382628, -0.02431939, -0.00124017],\n",
       "        [-0.152773  ,  0.01729362, -0.02049777,  0.00555693],\n",
       "        [-0.01630396, -0.0508009 ,  0.00338947, -0.00536075],\n",
       "        [-0.11710438, -0.0411608 ,  0.01888775,  0.10048383],\n",
       "        [-0.1000828 ,  0.06160894, -0.04584875,  0.14583868],\n",
       "        [-0.29426795, -0.04820506, -0.1607864 ,  0.03541403],\n",
       "        [-0.14156486,  0.07812893, -0.13114211,  0.12421213],\n",
       "        [-0.20461929,  0.08133916, -0.08211927,  0.03100684],\n",
       "        [-0.00948744, -0.11452891,  0.03942408, -0.1281477 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
