{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "frank-northwest",
   "metadata": {},
   "source": [
    "# 1. Data Generator\n",
    "- Raw Data를 읽어옴\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "requested-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import librosa\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import scipy.signal\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "architectural-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawForVAEGenerator(Sequence):\n",
    "    def __init__(self, source, wav_dir, files, sourNum='s1', batch_size=10, shuffle=True):\n",
    "        self.source = source\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.sourNum = sourNum\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "        self.sample_rate = 8000\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.source))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "#         signal = librosa.load(path, sr=self.sample_rate, mono=False, offset=offset, duration=duration)\n",
    "        \n",
    "#         return signal[0]\n",
    "    \n",
    "        signal_rate, signal = wavfile.read(path)\n",
    "        number_of_samples = round(len(signal) * float(self.sample_rate) / signal_rate)\n",
    "        signal = scipy.signal.resample(signal, number_of_samples)\n",
    "        signal /= np.max(np.abs(signal),axis=0)\n",
    "\n",
    "        return signal\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        extrapadding = int(np.ceil(max_len / self.sample_rate) * self.sample_rate)\n",
    "        pad = np.zeros((n_batch, extrapadding))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return np.expand_dims(pad, -1)\n",
    "        \n",
    "    def __data_generation__(self, source_list):\n",
    "        wav_list = []\n",
    "        for name in source_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            s_wav_name = self.wav_dir + self.files + '/' + self.sourNum + '/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            s_wav = (self.__audioread__(s_wav_name,  offset=0.0, duration=None, sample_rate=self.sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "#             pad_len = max(len(samples1),len(samples2))\n",
    "#             pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            \n",
    "#             extrapadding = ceil(len(pad_s1) / sample_rate) * sample_rate - len(pad_s1)\n",
    "#             pad_s1 = np.concatenate([pad_s1, np.zeros([extrapadding - len(pad_s1)])])\n",
    "#             pad_s2 = np.concatenate([s2_wav, np.zeros([extrapadding - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            wav_list.append(s_wav)\n",
    "        \n",
    "        return wav_list, wav_list, source_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.source) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        source_list = [self.source[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            sour, labels, _ = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "#             return sour_pad, np.concatenate([label_pad, exp], axis=1)\n",
    "            return sour_pad, label_pad\n",
    "        else:\n",
    "            sour, labels, name = self.__data_generation__(source_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in sour])\n",
    "            exp = np.expand_dims(lengths, 1)\n",
    "            exp = np.expand_dims(exp, -1) # [Batch, 1, 1] (length)\n",
    "            \n",
    "            # Padding\n",
    "            sour_pad = self.__padding__(sour) # [Batch, Time_step, Dimension(=1)]\n",
    "            \n",
    "            return sour_pad, exp, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-league",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "martial-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_for_generator = 1\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\"\"\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', batch_size_for_generator)\n",
    "    else:\n",
    "        test_batch = 1\n",
    "        test_dataset = RawForVAEGenerator(lines, WAV_DIR, files, 's1', test_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-russian",
   "metadata": {},
   "source": [
    "# 2. Building VQ-VAE model with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "environmental-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tensorflow.keras import backend as Kb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import time\n",
    "from tensorflow.keras.models import Model, Sequential, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sexual-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "piano-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e63776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GumbelSoftmax(layers.Layer):\n",
    "    def __init__(self, temperature=0.5, hard=False, name = 'gumbel_softmax',**kwargs):\n",
    "        super(GumbelSoftmax, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "    \n",
    "    def sample_gumbel(self, shape, eps=1e-20): \n",
    "        \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "        U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "        \n",
    "        return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "    def gumbel_softmax_sample(self, logits, temperature): \n",
    "        \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "        y = logits + self.sample_gumbel(tf.shape(logits))\n",
    "        \n",
    "        return tf.nn.softmax(y / temperature)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        y = self.gumbel_softmax_sample(inputs, self.temperature)\n",
    "        \n",
    "        if self.hard:\n",
    "            y_hard = tf.cast(tf.equal(y, tf.math.reduce_max(y, 2, keepdims=True)), y.dtype)\n",
    "            y = tf.stop_gradient(y_hard - y) + y\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'encoder',**kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.conv1d_1 = layers.Conv1D(filters=32, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.conv1d_2 = layers.Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.conv1d_3 = layers.Conv1D(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.conv1d_4 = layers.Conv1D(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.conv1d_5 = layers.Conv1D(filters=512, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1D(filters=latent_dim, kernel_size=1, strides=1, activation=None, padding='valid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.conv1d_1(inputs)\n",
    "        x = self.conv1d_2(x)\n",
    "        x = self.conv1d_3(x)\n",
    "        x = self.conv1d_4(x)\n",
    "        x = self.conv1d_5(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, latent_dim, name = 'decoder',**kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.trans_conv1d_1 = layers.Conv1DTranspose(filters=512, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_2 = layers.Conv1DTranspose(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_3 = layers.Conv1DTranspose(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_4 = layers.Conv1DTranspose(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.trans_conv1d_5 = layers.Conv1DTranspose(filters=32, kernel_size=4, strides=2, activation='relu', padding='same')\n",
    "        self.logit = layers.Conv1DTranspose(filters=1, kernel_size=1, strides=1, activation=None, padding='valid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.trans_conv1d_1(inputs)\n",
    "        x = self.trans_conv1d_2(x)\n",
    "        x = self.trans_conv1d_3(x)\n",
    "        x = self.trans_conv1d_4(x)\n",
    "        x = self.trans_conv1d_5(x)\n",
    "        logit = self.logit(x)\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30fcfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric Si-sdr\n",
    "\n",
    "class SiSdr(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"Si-sdr\", **kwargs):\n",
    "        super(SiSdr, self).__init__(name=name, **kwargs)\n",
    "        self.sdr = self.add_weight(name=\"sdr\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"cnt\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Check sequence length\n",
    "        batch_size = tf.shape(labels)[0]\n",
    "        label_size = tf.shape(labels)[1]\n",
    "        pred_size = tf.shape(y_pred)[1]\n",
    "        feature_size = tf.shape(labels)[-1]\n",
    "        \n",
    "        # Change sequence length\n",
    "        if label_size < pred_size:\n",
    "            y_pred = tf.slice(y_pred, [0, 0, 0], [-1, label_size, -1])\n",
    "        elif label_size > pred_size:\n",
    "            labels = tf.slice(labels, [0, 0, 0], [-1, pred_size, -1])\n",
    "\n",
    "        # SI-SDR\n",
    "        target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "        noise = y_pred - target\n",
    "        values = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "        \n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, \"float32\")\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sdr.assign_add(tf.reduce_sum(values))\n",
    "        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sdr / self.count\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.sdr.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "\n",
    "# Custom mse\n",
    "def custom_mse(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    loss = tf.reduce_sum(tf.pow(y_pred - y_true, 2), axis=[1, 2])\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Custom si-sdr loss\n",
    "def custom_sisdr_loss(y_true, y_pred):\n",
    "    ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "    # Label & Length divide\n",
    "    labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 1]\n",
    "    lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "    target = tf.linalg.matmul(y_pred, labels, transpose_a=True) * labels / tf.expand_dims(tf.experimental.numpy.square(tf.norm(labels, axis=1)), axis=-1)\n",
    "    noise = y_pred - target\n",
    "    si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "    si_sdr = tf.reduce_mean(si_sdr) * -1\n",
    "\n",
    "    return si_sdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "renewable-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vq_vae(keras.Model):\n",
    "    def __init__(self, latent_dim, gumbel_hard=False, for_predict=False, name='vqvae', **kwargs):\n",
    "        super(Vq_vae, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.for_predict = for_predict\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.softmax = layers.Softmax(-1)\n",
    "        \n",
    "        self.encoder = Encoder(latent_dim)\n",
    "        self.gumbel = GumbelSoftmax(hard=gumbel_hard)\n",
    "        self.sampled = layers.experimental.EinsumDense('bsc,cd->bsd',\n",
    "                                                       output_shape=(None, latent_dim),\n",
    "                                                       bias_axes='d')\n",
    "        self.decoder = Decoder(latent_dim)\n",
    "        \n",
    "    def call(self, inputs, load=False):\n",
    "        if load:\n",
    "            inputs = layers.Input(shape=(None, 1))\n",
    "        \n",
    "        \n",
    "        if self.for_predict:\n",
    "            encode = self.encoder(inputs)\n",
    "            one_hot_enc = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "            sample = self.sampled(one_hot_enc)\n",
    "            decode = self.decoder(sample)\n",
    "        else:\n",
    "            encode = self.encoder(inputs)\n",
    "            gumbel = self.gumbel(encode)\n",
    "            sample = self.sampled(gumbel)\n",
    "            decode = self.decoder(sample)\n",
    "        \n",
    "        # ------------------ KL loss ------------------\n",
    "        qy = self.softmax(encode)\n",
    "        log_qy = tf.math.log(qy + 1e-10)\n",
    "        log_uniform = qy * (log_qy - tf.math.log(1.0 / self.latent_dim))\n",
    "        kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "        kl_loss = tf.reduce_mean(kl_loss) * 0.2\n",
    "        # ---------------------------------------------\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        \n",
    "        return decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-azerbaijan",
   "metadata": {},
   "source": [
    "# 이렇게 GradientTape 를 사용해서 프로그램 해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hawaiian-alloy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, None, 1) dtype=float32 (created by layer 'decoder')>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "latent_size = 1024\n",
    "epochs = 3\n",
    "\n",
    "filePath = \"./CKPT/CKP_ep_{0}__loss_{1:.5f}_.h5\"\n",
    "model_path = './CKPT/CKP_ep_576__loss_168.75122_.h5'\n",
    "\n",
    "loss_fun = custom_mse\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean()\n",
    "train_kl_loss = tf.keras.metrics.Mean()\n",
    "valid_loss = tf.keras.metrics.Mean()\n",
    "sisdr_Metric = SiSdr()\n",
    "val_sisdr_Metric = SiSdr()\n",
    "\n",
    "# Model 불러오는 부분이다\n",
    "vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "vq_vae(0, True)\n",
    "# vq_vae.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bored-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Call model\n",
    "        results = vq_vae(x)\n",
    "        \n",
    "        loss_value = loss_fun(y, results)\n",
    "        loss_value += sum(vq_vae.losses) # Add KL loss\n",
    "    \n",
    "    # Update weights\n",
    "    grads = tape.gradient(loss_value, vq_vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vq_vae.trainable_weights))\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "    train_loss.update_state(loss_value)\n",
    "    sisdr_Metric.update_state(y, results)\n",
    "    \n",
    "    train_kl_loss.update_state(sum(vq_vae.losses))\n",
    "    \n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    # Call model\n",
    "    val_results = vq_vae(x)\n",
    "    \n",
    "    val_loss_value = loss_fun(y, val_results)\n",
    "    val_loss_value += sum(vq_vae.losses) # Add KL loss\n",
    "    \n",
    "    # Update loss and si-sdr\n",
    "    valid_loss.update_state(val_loss_value)\n",
    "    val_sisdr_Metric.update_state(y, val_results)\n",
    "    \n",
    "    return val_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "descending-improvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "(2, 80000, 1)\n",
      "(2, 64000, 1)\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "Time taken >>> 1.70s <<<\n",
      "epoch: 1, Train_loss: 468.0998840332031, Train_Si-sdr: -38.02525329589844, Train_KL_loss: 0.0 \n",
      "    Valid_loss: 468.36773681640625, Valid_Si-sdr: -40.73042678833008\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 1: val_loss improved from inf to 468.36773681640625, saving model to ./CKPT/CKP_ep_1__loss_468.36774_.h5\n",
      "\n",
      "\n",
      "Start of epoch 2\n",
      "(2, 56000, 1)\n",
      "(2, 80000, 1)\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "Time taken >>> 1.63s <<<\n",
      "epoch: 2, Train_loss: 468.0527648925781, Train_Si-sdr: -34.70179748535156, Train_KL_loss: 0.0 \n",
      "    Valid_loss: 467.18829345703125, Valid_Si-sdr: -24.99163246154785\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 2: val_loss improved from 468.36773681640625 to 467.18829345703125, saving model to ./CKPT/CKP_ep_2__loss_467.18829_.h5\n",
      "\n",
      "\n",
      "Start of epoch 3\n",
      "(2, 56000, 1)\n",
      "(2, 80000, 1)\n",
      "\n",
      "----------------------------------------------------------------------------------\n",
      "Time taken >>> 1.69s <<<\n",
      "epoch: 3, Train_loss: 468.2825012207031, Train_Si-sdr: -40.597503662109375, Train_KL_loss: 0.0 \n",
      "    Valid_loss: 468.29705810546875, Valid_Si-sdr: -39.67472457885742\n",
      "----------------------------------------------------------------------------------\n",
      "Epoch 3: val_loss did not improve from 467.18829345703125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "previous_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over the batches of the dataset\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        x_batch_train = tf.cast(x_batch_train, dtype=tf.float32)\n",
    "        y_batch_train = tf.cast(y_batch_train, dtype=tf.float32)\n",
    "        \n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # Log every 1 batches\n",
    "#         if step % 1 == 0:\n",
    "#             print(\"Training loss (for one batch) at step %d: %.4f\" % (step, train_loss.result()))\n",
    "#             print(\"Training Si-sdr (for one batch) at step %d: %.4f\" % (step, sisdr_Metric.result()))\n",
    "#             print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "    # Run a validation loop at the end of each epoch\n",
    "    for x_batch_val, y_batch_val in valid_dataset:\n",
    "        x_batch_val = tf.cast(x_batch_val, dtype=tf.float32)\n",
    "        y_batch_val = tf.cast(y_batch_val, dtype=tf.float32)\n",
    "        \n",
    "        val_loss_value = test_step(x_batch_val, y_batch_val)\n",
    "    \n",
    "    print()\n",
    "    print('----------------------------------------------------------------------------------')\n",
    "    print(\"Time taken >>> %.2fs <<<\" % (time.time() - start_time))\n",
    "    print('epoch: {}, Train_loss: {}, Train_Si-sdr: {}, Train_KL_loss: {} \\n\\\n",
    "    Valid_loss: {}, Valid_Si-sdr: {}'.format(\n",
    "        epoch+1,\n",
    "        train_loss.result(),\n",
    "        sisdr_Metric.result(),\n",
    "        train_kl_loss.result(),\n",
    "        valid_loss.result(),\n",
    "        val_sisdr_Metric.result()))\n",
    "    print('----------------------------------------------------------------------------------')\n",
    "    \n",
    "    # Save Model\n",
    "    if valid_loss.result() < previous_loss:\n",
    "        filePath_temp = filePath.format(epoch+1, valid_loss.result())\n",
    "        \n",
    "        vq_vae.save_weights(filePath_temp)\n",
    "        print('Epoch {}: val_loss improved from {} to {}, saving model to {}'.format(\n",
    "            epoch+1,\n",
    "            previous_loss,\n",
    "            valid_loss.result(),\n",
    "            filePath_temp))\n",
    "        \n",
    "        previous_loss = valid_loss.result()\n",
    "    else:\n",
    "        print('Epoch {}: val_loss did not improve from {}'.format(\n",
    "            epoch+1,\n",
    "            previous_loss))\n",
    "    print()\n",
    "    \n",
    "    # Reset metrics at the end of each epoch\n",
    "    train_loss.reset_states()\n",
    "    sisdr_Metric.reset_states()\n",
    "    valid_loss.reset_states()\n",
    "    val_sisdr_Metric.reset_states()\n",
    "    \n",
    "    train_kl_loss.reset_states()\n",
    "    \n",
    "    # Data shuffle at the end of each epoch\n",
    "    train_dataset.on_epoch_end()\n",
    "    valid_dataset.on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adaptive-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data_generator():\n",
    "    for i in range(train_dataset.__len__()):\n",
    "        data = np.squeeze(train_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(train_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)\n",
    "\n",
    "def gen_valid_data_generator():\n",
    "    for i in range(valid_dataset.__len__()):\n",
    "        data = np.squeeze(valid_dataset.__getitem__(i)[0], axis=0)\n",
    "        label = np.squeeze(valid_dataset.__getitem__(i)[1], axis=0)\n",
    "        \n",
    "        yield (data, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-frequency",
   "metadata": {},
   "source": [
    "# 여기는 기존의 .fit() 함수를 사용해서 학습하는 부분임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "referenced-bangkok",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " softmax_4 (Softmax)         (None, None, 1024)        0         \n",
      "                                                                 \n",
      " encoder (Encoder)           (None, None, 1024)        1263776   \n",
      "                                                                 \n",
      " gumbel_softmax (GumbelSoftm  (None, None, 1024)       0         \n",
      " ax)                                                             \n",
      "                                                                 \n",
      " einsum_dense_4 (EinsumDense  (None, None, 1024)       1049600   \n",
      " )                                                               \n",
      "                                                                 \n",
      " decoder (Decoder)           (None, None, 1)           2835521   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,148,897\n",
      "Trainable params: 5,148,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/600\n",
      "    193/Unknown - 46s 151ms/step - loss: 229.1326 - Si-sdr: 3.4868\n",
      "Epoch 00001: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 51s 177ms/step - loss: 229.1326 - Si-sdr: 3.4868 - val_loss: 230.9757 - val_Si-sdr: 0.4728\n",
      "Epoch 2/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.6106 - Si-sdr: 3.5018\n",
      "Epoch 00002: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.5905 - Si-sdr: 3.5028 - val_loss: 231.4689 - val_Si-sdr: 0.4034\n",
      "Epoch 3/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.2995 - Si-sdr: 3.5168\n",
      "Epoch 00003: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.2995 - Si-sdr: 3.5168 - val_loss: 231.8879 - val_Si-sdr: 0.4492\n",
      "Epoch 4/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.5815 - Si-sdr: 3.4922\n",
      "Epoch 00004: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.5990 - Si-sdr: 3.4969 - val_loss: 232.6623 - val_Si-sdr: 0.4341\n",
      "Epoch 5/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.9782 - Si-sdr: 3.5102\n",
      "Epoch 00005: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.9782 - Si-sdr: 3.5102 - val_loss: 232.6097 - val_Si-sdr: 0.3252\n",
      "Epoch 6/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.6566 - Si-sdr: 3.4904\n",
      "Epoch 00006: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 170ms/step - loss: 228.6566 - Si-sdr: 3.4904 - val_loss: 230.9094 - val_Si-sdr: 0.4614\n",
      "Epoch 7/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.4848 - Si-sdr: 3.5079\n",
      "Epoch 00007: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 46s 169ms/step - loss: 228.3749 - Si-sdr: 3.5056 - val_loss: 230.9052 - val_Si-sdr: 0.4675\n",
      "Epoch 8/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.0110 - Si-sdr: 3.5042\n",
      "Epoch 00008: val_loss did not improve from 229.89435\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 227.9952 - Si-sdr: 3.5040 - val_loss: 230.7334 - val_Si-sdr: 0.4543\n",
      "Epoch 9/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.5295 - Si-sdr: 3.5001\n",
      "Epoch 00009: val_loss improved from 229.89435 to 229.88667, saving model to ./CKPT\\CKP_ep_9__loss_229.88667_.h5\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.5295 - Si-sdr: 3.5001 - val_loss: 229.8867 - val_Si-sdr: 0.4976\n",
      "Epoch 10/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.2856 - Si-sdr: 3.5079\n",
      "Epoch 00010: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.2856 - Si-sdr: 3.5079 - val_loss: 232.2539 - val_Si-sdr: 0.4033\n",
      "Epoch 11/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.7200 - Si-sdr: 3.4943\n",
      "Epoch 00011: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 228.7200 - Si-sdr: 3.4943 - val_loss: 231.4249 - val_Si-sdr: 0.4302\n",
      "Epoch 12/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.1724 - Si-sdr: 3.5104\n",
      "Epoch 00012: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.1724 - Si-sdr: 3.5104 - val_loss: 230.9662 - val_Si-sdr: 0.4480\n",
      "Epoch 13/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8918 - Si-sdr: 3.5148\n",
      "Epoch 00013: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 227.9340 - Si-sdr: 3.5166 - val_loss: 230.6566 - val_Si-sdr: 0.5096\n",
      "Epoch 14/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.9067 - Si-sdr: 3.5223\n",
      "Epoch 00014: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 169ms/step - loss: 227.8111 - Si-sdr: 3.5188 - val_loss: 232.2141 - val_Si-sdr: 0.3702\n",
      "Epoch 15/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.2330 - Si-sdr: 3.5050\n",
      "Epoch 00015: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.1610 - Si-sdr: 3.5073 - val_loss: 231.5536 - val_Si-sdr: 0.4352\n",
      "Epoch 16/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1295 - Si-sdr: 3.5123\n",
      "Epoch 00016: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 170ms/step - loss: 228.2469 - Si-sdr: 3.5119 - val_loss: 230.7751 - val_Si-sdr: 0.4448\n",
      "Epoch 17/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1720 - Si-sdr: 3.5140\n",
      "Epoch 00017: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 228.1389 - Si-sdr: 3.5163 - val_loss: 230.4922 - val_Si-sdr: 0.5136\n",
      "Epoch 18/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.4626 - Si-sdr: 3.4928\n",
      "Epoch 00018: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.4800 - Si-sdr: 3.4940 - val_loss: 231.3901 - val_Si-sdr: 0.4807\n",
      "Epoch 19/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.8587 - Si-sdr: 3.4949\n",
      "Epoch 00019: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.7314 - Si-sdr: 3.4896 - val_loss: 231.9074 - val_Si-sdr: 0.4659\n",
      "Epoch 20/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1937 - Si-sdr: 3.5078\n",
      "Epoch 00020: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.1432 - Si-sdr: 3.5066 - val_loss: 231.8900 - val_Si-sdr: 0.4031\n",
      "Epoch 21/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.7730 - Si-sdr: 3.4944\n",
      "Epoch 00021: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.7642 - Si-sdr: 3.4929 - val_loss: 232.5210 - val_Si-sdr: 0.4060\n",
      "Epoch 22/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.7825 - Si-sdr: 3.5154\n",
      "Epoch 00022: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 227.7825 - Si-sdr: 3.5154 - val_loss: 231.2536 - val_Si-sdr: 0.4578\n",
      "Epoch 23/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1655 - Si-sdr: 3.5114\n",
      "Epoch 00023: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 228.0974 - Si-sdr: 3.5073 - val_loss: 231.1619 - val_Si-sdr: 0.4558\n",
      "Epoch 24/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.4203 - Si-sdr: 3.5256\n",
      "Epoch 00024: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 227.4203 - Si-sdr: 3.5256 - val_loss: 230.4044 - val_Si-sdr: 0.5562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.0519 - Si-sdr: 3.5301\n",
      "Epoch 00025: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 172ms/step - loss: 228.0103 - Si-sdr: 3.5287 - val_loss: 231.3212 - val_Si-sdr: 0.4654\n",
      "Epoch 26/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.5429 - Si-sdr: 3.5263\n",
      "Epoch 00026: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.5653 - Si-sdr: 3.5245 - val_loss: 230.2857 - val_Si-sdr: 0.5716\n",
      "Epoch 27/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.9208 - Si-sdr: 3.5240\n",
      "Epoch 00027: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 227.9133 - Si-sdr: 3.5228 - val_loss: 231.2516 - val_Si-sdr: 0.4694\n",
      "Epoch 28/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8333 - Si-sdr: 3.5173\n",
      "Epoch 00028: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.9610 - Si-sdr: 3.5211 - val_loss: 231.9001 - val_Si-sdr: 0.3837\n",
      "Epoch 29/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.4903 - Si-sdr: 3.4912\n",
      "Epoch 00029: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 228.5182 - Si-sdr: 3.4940 - val_loss: 231.5403 - val_Si-sdr: 0.3920\n",
      "Epoch 30/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8581 - Si-sdr: 3.5241\n",
      "Epoch 00030: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.8992 - Si-sdr: 3.5230 - val_loss: 230.5698 - val_Si-sdr: 0.5201\n",
      "Epoch 31/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.9426 - Si-sdr: 3.5085\n",
      "Epoch 00031: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.8579 - Si-sdr: 3.5088 - val_loss: 230.6898 - val_Si-sdr: 0.4145\n",
      "Epoch 32/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1453 - Si-sdr: 3.5169\n",
      "Epoch 00032: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 170ms/step - loss: 228.0793 - Si-sdr: 3.5130 - val_loss: 230.2685 - val_Si-sdr: 0.4978\n",
      "Epoch 33/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8608 - Si-sdr: 3.5077\n",
      "Epoch 00033: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.9117 - Si-sdr: 3.5124 - val_loss: 231.0802 - val_Si-sdr: 0.4659\n",
      "Epoch 34/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.5247 - Si-sdr: 3.5040\n",
      "Epoch 00034: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 170ms/step - loss: 228.5271 - Si-sdr: 3.5073 - val_loss: 229.8920 - val_Si-sdr: 0.5853\n",
      "Epoch 35/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.1404 - Si-sdr: 3.5123\n",
      "Epoch 00035: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.1404 - Si-sdr: 3.5123 - val_loss: 231.5472 - val_Si-sdr: 0.5354\n",
      "Epoch 36/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.1096 - Si-sdr: 3.5062\n",
      "Epoch 00036: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.1096 - Si-sdr: 3.5062 - val_loss: 230.4936 - val_Si-sdr: 0.5267\n",
      "Epoch 37/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1264 - Si-sdr: 3.5066\n",
      "Epoch 00037: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.0613 - Si-sdr: 3.5088 - val_loss: 230.7847 - val_Si-sdr: 0.5260\n",
      "Epoch 38/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.0238 - Si-sdr: 3.5274\n",
      "Epoch 00038: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.9601 - Si-sdr: 3.5308 - val_loss: 230.7654 - val_Si-sdr: 0.5201\n",
      "Epoch 39/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.2861 - Si-sdr: 3.5072\n",
      "Epoch 00039: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 228.2861 - Si-sdr: 3.5072 - val_loss: 231.1607 - val_Si-sdr: 0.4360\n",
      "Epoch 40/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.5147 - Si-sdr: 3.5368\n",
      "Epoch 00040: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 169ms/step - loss: 227.4159 - Si-sdr: 3.5341 - val_loss: 230.1701 - val_Si-sdr: 0.5108\n",
      "Epoch 41/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.5296 - Si-sdr: 3.5271\n",
      "Epoch 00041: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.6672 - Si-sdr: 3.5275 - val_loss: 231.1831 - val_Si-sdr: 0.3761\n",
      "Epoch 42/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.0466 - Si-sdr: 3.5140\n",
      "Epoch 00042: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 228.0851 - Si-sdr: 3.5156 - val_loss: 230.3136 - val_Si-sdr: 0.5104\n",
      "Epoch 43/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3984 - Si-sdr: 3.5382\n",
      "Epoch 00043: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.3706 - Si-sdr: 3.5379 - val_loss: 230.1580 - val_Si-sdr: 0.4708\n",
      "Epoch 44/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.5903 - Si-sdr: 3.5274\n",
      "Epoch 00044: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.5903 - Si-sdr: 3.5274 - val_loss: 229.9734 - val_Si-sdr: 0.5578\n",
      "Epoch 45/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.7937 - Si-sdr: 3.5233\n",
      "Epoch 00045: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 45s 169ms/step - loss: 227.7937 - Si-sdr: 3.5233 - val_loss: 231.4977 - val_Si-sdr: 0.3712\n",
      "Epoch 46/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.4423 - Si-sdr: 3.5249\n",
      "Epoch 00046: val_loss did not improve from 229.88667\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.5747 - Si-sdr: 3.5250 - val_loss: 231.3332 - val_Si-sdr: 0.4112\n",
      "Epoch 47/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 228.1852 - Si-sdr: 3.5024\n",
      "Epoch 00047: val_loss improved from 229.88667 to 229.72289, saving model to ./CKPT\\CKP_ep_47__loss_229.72289_.h5\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 228.1852 - Si-sdr: 3.5024 - val_loss: 229.7229 - val_Si-sdr: 0.5190\n",
      "Epoch 48/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.2214 - Si-sdr: 3.5549\n",
      "Epoch 00048: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 227.0777 - Si-sdr: 3.5511 - val_loss: 231.4277 - val_Si-sdr: 0.4448\n",
      "Epoch 49/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.5070 - Si-sdr: 3.5307\n",
      "Epoch 00049: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 169ms/step - loss: 227.5070 - Si-sdr: 3.5307 - val_loss: 230.6506 - val_Si-sdr: 0.4927\n",
      "Epoch 50/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.7892 - Si-sdr: 3.5265\n",
      "Epoch 00050: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.7637 - Si-sdr: 3.5220 - val_loss: 229.9950 - val_Si-sdr: 0.5150\n",
      "Epoch 51/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.4545 - Si-sdr: 3.5403\n",
      "Epoch 00051: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 171ms/step - loss: 227.4545 - Si-sdr: 3.5403 - val_loss: 231.3421 - val_Si-sdr: 0.4550\n",
      "Epoch 52/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.7066 - Si-sdr: 3.5257\n",
      "Epoch 00052: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.7066 - Si-sdr: 3.5257 - val_loss: 230.6996 - val_Si-sdr: 0.4686\n",
      "Epoch 53/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8192 - Si-sdr: 3.5065\n",
      "Epoch 00053: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 227.8352 - Si-sdr: 3.5134 - val_loss: 234.1486 - val_Si-sdr: 0.3896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.7043 - Si-sdr: 3.5277\n",
      "Epoch 00054: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 46s 170ms/step - loss: 227.7043 - Si-sdr: 3.5277 - val_loss: 230.3126 - val_Si-sdr: 0.5461\n",
      "Epoch 55/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.2943 - Si-sdr: 3.5324\n",
      "Epoch 00055: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 227.3955 - Si-sdr: 3.5360 - val_loss: 232.0267 - val_Si-sdr: 0.3571\n",
      "Epoch 56/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1564 - Si-sdr: 3.5124\n",
      "Epoch 00056: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 45s 168ms/step - loss: 228.1628 - Si-sdr: 3.5118 - val_loss: 231.4371 - val_Si-sdr: 0.4807\n",
      "Epoch 57/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.9501 - Si-sdr: 3.5199\n",
      "Epoch 00057: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 227.8837 - Si-sdr: 3.5166 - val_loss: 230.4959 - val_Si-sdr: 0.4664\n",
      "Epoch 58/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8255 - Si-sdr: 3.5337\n",
      "Epoch 00058: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.7613 - Si-sdr: 3.5343 - val_loss: 232.2117 - val_Si-sdr: 0.4655\n",
      "Epoch 59/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8669 - Si-sdr: 3.5316\n",
      "Epoch 00059: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.7258 - Si-sdr: 3.5285 - val_loss: 230.9374 - val_Si-sdr: 0.4239\n",
      "Epoch 60/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0826 - Si-sdr: 3.5542\n",
      "Epoch 00060: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.0257 - Si-sdr: 3.5511 - val_loss: 231.1055 - val_Si-sdr: 0.4970\n",
      "Epoch 61/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.6635 - Si-sdr: 3.5259\n",
      "Epoch 00061: val_loss did not improve from 229.72289\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 227.6013 - Si-sdr: 3.5279 - val_loss: 231.0172 - val_Si-sdr: 0.4443\n",
      "Epoch 62/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.6616 - Si-sdr: 3.5161\n",
      "Epoch 00062: val_loss improved from 229.72289 to 228.70078, saving model to ./CKPT\\CKP_ep_62__loss_228.70078_.h5\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.6616 - Si-sdr: 3.5161 - val_loss: 228.7008 - val_Si-sdr: 0.5572\n",
      "Epoch 63/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.5235 - Si-sdr: 3.5246\n",
      "Epoch 00063: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.5235 - Si-sdr: 3.5246 - val_loss: 230.2802 - val_Si-sdr: 0.5419\n",
      "Epoch 64/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3781 - Si-sdr: 3.5524\n",
      "Epoch 00064: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.2368 - Si-sdr: 3.5542 - val_loss: 230.6219 - val_Si-sdr: 0.4204\n",
      "Epoch 65/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.4665 - Si-sdr: 3.5337\n",
      "Epoch 00065: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.5191 - Si-sdr: 3.5336 - val_loss: 231.0618 - val_Si-sdr: 0.4554\n",
      "Epoch 66/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.8633 - Si-sdr: 3.5096\n",
      "Epoch 00066: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 228.0109 - Si-sdr: 3.5091 - val_loss: 230.5144 - val_Si-sdr: 0.5771\n",
      "Epoch 67/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.6716 - Si-sdr: 3.5294\n",
      "Epoch 00067: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.6154 - Si-sdr: 3.5299 - val_loss: 231.8102 - val_Si-sdr: 0.3785\n",
      "Epoch 68/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 228.1694 - Si-sdr: 3.5141\n",
      "Epoch 00068: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 228.1857 - Si-sdr: 3.5122 - val_loss: 229.8116 - val_Si-sdr: 0.4945\n",
      "Epoch 69/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.6779 - Si-sdr: 3.5364\n",
      "Epoch 00069: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.6960 - Si-sdr: 3.5312 - val_loss: 230.7810 - val_Si-sdr: 0.5093\n",
      "Epoch 70/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.5049 - Si-sdr: 3.5370\n",
      "Epoch 00070: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.5207 - Si-sdr: 3.5387 - val_loss: 231.0403 - val_Si-sdr: 0.5089\n",
      "Epoch 71/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.7672 - Si-sdr: 3.5241\n",
      "Epoch 00071: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.7300 - Si-sdr: 3.5250 - val_loss: 230.9272 - val_Si-sdr: 0.4833\n",
      "Epoch 72/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.5129 - Si-sdr: 3.5320\n",
      "Epoch 00072: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.5129 - Si-sdr: 3.5320 - val_loss: 230.1019 - val_Si-sdr: 0.5029\n",
      "Epoch 73/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0238 - Si-sdr: 3.5470\n",
      "Epoch 00073: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 227.0888 - Si-sdr: 3.5521 - val_loss: 231.4072 - val_Si-sdr: 0.4452\n",
      "Epoch 74/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.5312 - Si-sdr: 3.5288\n",
      "Epoch 00074: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.6099 - Si-sdr: 3.5331 - val_loss: 231.6678 - val_Si-sdr: 0.4731\n",
      "Epoch 75/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.4483 - Si-sdr: 3.5311\n",
      "Epoch 00075: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.3914 - Si-sdr: 3.5319 - val_loss: 230.7065 - val_Si-sdr: 0.4828\n",
      "Epoch 76/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.8373 - Si-sdr: 3.5257\n",
      "Epoch 00076: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.8373 - Si-sdr: 3.5257 - val_loss: 230.6769 - val_Si-sdr: 0.4853\n",
      "Epoch 77/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1352 - Si-sdr: 3.5393\n",
      "Epoch 00077: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.0808 - Si-sdr: 3.5385 - val_loss: 230.8571 - val_Si-sdr: 0.4299\n",
      "Epoch 78/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.0750 - Si-sdr: 3.5407\n",
      "Epoch 00078: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.0750 - Si-sdr: 3.5407 - val_loss: 230.4022 - val_Si-sdr: 0.4456\n",
      "Epoch 79/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.4673 - Si-sdr: 3.5375\n",
      "Epoch 00079: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.4052 - Si-sdr: 3.5428 - val_loss: 230.3322 - val_Si-sdr: 0.4847\n",
      "Epoch 80/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.2812 - Si-sdr: 3.5373\n",
      "Epoch 00080: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 227.3220 - Si-sdr: 3.5337 - val_loss: 231.3558 - val_Si-sdr: 0.4064\n",
      "Epoch 81/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1738 - Si-sdr: 3.5460\n",
      "Epoch 00081: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.2256 - Si-sdr: 3.5488 - val_loss: 231.0552 - val_Si-sdr: 0.4145\n",
      "Epoch 82/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3618 - Si-sdr: 3.5413\n",
      "Epoch 00082: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.4997 - Si-sdr: 3.5406 - val_loss: 230.2389 - val_Si-sdr: 0.5151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3840 - Si-sdr: 3.5265\n",
      "Epoch 00083: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.4308 - Si-sdr: 3.5300 - val_loss: 230.3597 - val_Si-sdr: 0.5079\n",
      "Epoch 84/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.6081 - Si-sdr: 3.5236\n",
      "Epoch 00084: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 227.5999 - Si-sdr: 3.5279 - val_loss: 230.5263 - val_Si-sdr: 0.5045\n",
      "Epoch 85/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.1674 - Si-sdr: 3.5434\n",
      "Epoch 00085: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.1674 - Si-sdr: 3.5434 - val_loss: 230.4366 - val_Si-sdr: 0.4718\n",
      "Epoch 86/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.1487 - Si-sdr: 3.5357\n",
      "Epoch 00086: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 227.1487 - Si-sdr: 3.5357 - val_loss: 231.2167 - val_Si-sdr: 0.4540\n",
      "Epoch 87/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.4378 - Si-sdr: 3.5397\n",
      "Epoch 00087: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 227.2967 - Si-sdr: 3.5384 - val_loss: 229.3761 - val_Si-sdr: 0.5350\n",
      "Epoch 88/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.8437 - Si-sdr: 3.5482\n",
      "Epoch 00088: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.8196 - Si-sdr: 3.5486 - val_loss: 231.2671 - val_Si-sdr: 0.4430\n",
      "Epoch 89/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.2562 - Si-sdr: 3.5388\n",
      "Epoch 00089: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.2562 - Si-sdr: 3.5388 - val_loss: 230.6921 - val_Si-sdr: 0.3679\n",
      "Epoch 90/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.9821 - Si-sdr: 3.5424\n",
      "Epoch 00090: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.9821 - Si-sdr: 3.5424 - val_loss: 230.3779 - val_Si-sdr: 0.4579\n",
      "Epoch 91/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3594 - Si-sdr: 3.5273\n",
      "Epoch 00091: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 227.3978 - Si-sdr: 3.5271 - val_loss: 231.0115 - val_Si-sdr: 0.3703\n",
      "Epoch 92/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0306 - Si-sdr: 3.5445\n",
      "Epoch 00092: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.9510 - Si-sdr: 3.5457 - val_loss: 231.6523 - val_Si-sdr: 0.3465\n",
      "Epoch 93/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0296 - Si-sdr: 3.5468\n",
      "Epoch 00093: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.0127 - Si-sdr: 3.5475 - val_loss: 229.7245 - val_Si-sdr: 0.5555\n",
      "Epoch 94/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9155 - Si-sdr: 3.5461\n",
      "Epoch 00094: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.8565 - Si-sdr: 3.5440 - val_loss: 229.6566 - val_Si-sdr: 0.5031\n",
      "Epoch 95/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1985 - Si-sdr: 3.5385\n",
      "Epoch 00095: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.1272 - Si-sdr: 3.5381 - val_loss: 230.5302 - val_Si-sdr: 0.4592\n",
      "Epoch 96/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.0945 - Si-sdr: 3.5535\n",
      "Epoch 00096: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.0945 - Si-sdr: 3.5535 - val_loss: 230.3761 - val_Si-sdr: 0.4266\n",
      "Epoch 97/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0355 - Si-sdr: 3.5316\n",
      "Epoch 00097: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 227.1341 - Si-sdr: 3.5308 - val_loss: 230.8076 - val_Si-sdr: 0.4117\n",
      "Epoch 98/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.0061 - Si-sdr: 3.5377\n",
      "Epoch 00098: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.0061 - Si-sdr: 3.5377 - val_loss: 231.1909 - val_Si-sdr: 0.4536\n",
      "Epoch 99/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1117 - Si-sdr: 3.5501\n",
      "Epoch 00099: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.9519 - Si-sdr: 3.5482 - val_loss: 229.6646 - val_Si-sdr: 0.5088\n",
      "Epoch 100/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3401 - Si-sdr: 3.5791\n",
      "Epoch 00100: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.3564 - Si-sdr: 3.5726 - val_loss: 229.8147 - val_Si-sdr: 0.5620\n",
      "Epoch 101/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0143 - Si-sdr: 3.5610\n",
      "Epoch 00101: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.9185 - Si-sdr: 3.5557 - val_loss: 231.6302 - val_Si-sdr: 0.4223\n",
      "Epoch 102/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.4224 - Si-sdr: 3.5386\n",
      "Epoch 00102: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 227.4224 - Si-sdr: 3.5386 - val_loss: 231.3160 - val_Si-sdr: 0.4047\n",
      "Epoch 103/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.0570 - Si-sdr: 3.5418\n",
      "Epoch 00103: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.9779 - Si-sdr: 3.5424 - val_loss: 230.5817 - val_Si-sdr: 0.4811\n",
      "Epoch 104/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.8176 - Si-sdr: 3.5557\n",
      "Epoch 00104: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.8074 - Si-sdr: 3.5548 - val_loss: 229.8179 - val_Si-sdr: 0.4961\n",
      "Epoch 105/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5146 - Si-sdr: 3.5622\n",
      "Epoch 00105: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.5840 - Si-sdr: 3.5630 - val_loss: 230.6944 - val_Si-sdr: 0.5016\n",
      "Epoch 106/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.2080 - Si-sdr: 3.5357\n",
      "Epoch 00106: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.1520 - Si-sdr: 3.5352 - val_loss: 230.3531 - val_Si-sdr: 0.4623\n",
      "Epoch 107/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1450 - Si-sdr: 3.5523\n",
      "Epoch 00107: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.9833 - Si-sdr: 3.5480 - val_loss: 229.4621 - val_Si-sdr: 0.5167\n",
      "Epoch 108/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9356 - Si-sdr: 3.5395\n",
      "Epoch 00108: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 227.0282 - Si-sdr: 3.5448 - val_loss: 230.0636 - val_Si-sdr: 0.4966\n",
      "Epoch 109/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3563 - Si-sdr: 3.5486\n",
      "Epoch 00109: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.2470 - Si-sdr: 3.5454 - val_loss: 229.5354 - val_Si-sdr: 0.4840\n",
      "Epoch 110/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7381 - Si-sdr: 3.5513\n",
      "Epoch 00110: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.6982 - Si-sdr: 3.5532 - val_loss: 231.6126 - val_Si-sdr: 0.4994\n",
      "Epoch 111/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 227.2157 - Si-sdr: 3.5440\n",
      "Epoch 00111: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 227.2157 - Si-sdr: 3.5440 - val_loss: 229.6433 - val_Si-sdr: 0.5150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.8630 - Si-sdr: 3.5611\n",
      "Epoch 00112: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.7596 - Si-sdr: 3.5588 - val_loss: 230.3438 - val_Si-sdr: 0.5630\n",
      "Epoch 113/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.9063 - Si-sdr: 3.5548\n",
      "Epoch 00113: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.9063 - Si-sdr: 3.5548 - val_loss: 231.5062 - val_Si-sdr: 0.4974\n",
      "Epoch 114/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9122 - Si-sdr: 3.5563\n",
      "Epoch 00114: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.8222 - Si-sdr: 3.5541 - val_loss: 230.9958 - val_Si-sdr: 0.4945\n",
      "Epoch 115/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1419 - Si-sdr: 3.5486\n",
      "Epoch 00115: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 227.1275 - Si-sdr: 3.5483 - val_loss: 229.7615 - val_Si-sdr: 0.4562\n",
      "Epoch 116/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9007 - Si-sdr: 3.5386\n",
      "Epoch 00116: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.9293 - Si-sdr: 3.5407 - val_loss: 230.7763 - val_Si-sdr: 0.4445\n",
      "Epoch 117/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1337 - Si-sdr: 3.5499\n",
      "Epoch 00117: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 227.0500 - Si-sdr: 3.5517 - val_loss: 230.3941 - val_Si-sdr: 0.5308\n",
      "Epoch 118/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.3495 - Si-sdr: 3.5353\n",
      "Epoch 00118: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 227.3382 - Si-sdr: 3.5379 - val_loss: 230.3938 - val_Si-sdr: 0.4650\n",
      "Epoch 119/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9388 - Si-sdr: 3.5444\n",
      "Epoch 00119: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.8566 - Si-sdr: 3.5417 - val_loss: 230.4136 - val_Si-sdr: 0.5007\n",
      "Epoch 120/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3946 - Si-sdr: 3.5753\n",
      "Epoch 00120: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.3293 - Si-sdr: 3.5775 - val_loss: 229.5895 - val_Si-sdr: 0.4789\n",
      "Epoch 121/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.7237 - Si-sdr: 3.5554\n",
      "Epoch 00121: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.7237 - Si-sdr: 3.5554 - val_loss: 229.9301 - val_Si-sdr: 0.5575\n",
      "Epoch 122/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5704 - Si-sdr: 3.5496\n",
      "Epoch 00122: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.5818 - Si-sdr: 3.5515 - val_loss: 229.9939 - val_Si-sdr: 0.4196\n",
      "Epoch 123/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.8963 - Si-sdr: 3.5446\n",
      "Epoch 00123: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.8640 - Si-sdr: 3.5464 - val_loss: 230.8262 - val_Si-sdr: 0.4470\n",
      "Epoch 124/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.8301 - Si-sdr: 3.5557\n",
      "Epoch 00124: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.7537 - Si-sdr: 3.5539 - val_loss: 230.8991 - val_Si-sdr: 0.4409\n",
      "Epoch 125/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6836 - Si-sdr: 3.5574\n",
      "Epoch 00125: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.7337 - Si-sdr: 3.5571 - val_loss: 230.2643 - val_Si-sdr: 0.4423\n",
      "Epoch 126/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6069 - Si-sdr: 3.5584\n",
      "Epoch 00126: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.5532 - Si-sdr: 3.5565 - val_loss: 229.1835 - val_Si-sdr: 0.5531\n",
      "Epoch 127/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7788 - Si-sdr: 3.5619\n",
      "Epoch 00127: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.6647 - Si-sdr: 3.5614 - val_loss: 230.3357 - val_Si-sdr: 0.4022\n",
      "Epoch 128/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7315 - Si-sdr: 3.5591\n",
      "Epoch 00128: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.6103 - Si-sdr: 3.5569 - val_loss: 230.9331 - val_Si-sdr: 0.4692\n",
      "Epoch 129/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7702 - Si-sdr: 3.5473\n",
      "Epoch 00129: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.6209 - Si-sdr: 3.5435 - val_loss: 229.7815 - val_Si-sdr: 0.4189\n",
      "Epoch 130/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6657 - Si-sdr: 3.5539\n",
      "Epoch 00130: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.6232 - Si-sdr: 3.5537 - val_loss: 231.5373 - val_Si-sdr: 0.4334\n",
      "Epoch 131/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.9905 - Si-sdr: 3.5571\n",
      "Epoch 00131: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.9311 - Si-sdr: 3.5574 - val_loss: 230.2110 - val_Si-sdr: 0.5156\n",
      "Epoch 132/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4718 - Si-sdr: 3.5585\n",
      "Epoch 00132: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.4622 - Si-sdr: 3.5581 - val_loss: 230.8465 - val_Si-sdr: 0.4819\n",
      "Epoch 133/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.4470 - Si-sdr: 3.5745\n",
      "Epoch 00133: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.4470 - Si-sdr: 3.5745 - val_loss: 229.2880 - val_Si-sdr: 0.4581\n",
      "Epoch 134/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 227.1035 - Si-sdr: 3.5410\n",
      "Epoch 00134: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 227.0700 - Si-sdr: 3.5441 - val_loss: 230.2177 - val_Si-sdr: 0.4618\n",
      "Epoch 135/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6925 - Si-sdr: 3.5582\n",
      "Epoch 00135: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.5457 - Si-sdr: 3.5586 - val_loss: 229.9587 - val_Si-sdr: 0.4753\n",
      "Epoch 136/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5099 - Si-sdr: 3.5648\n",
      "Epoch 00136: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.4194 - Si-sdr: 3.5631 - val_loss: 229.6434 - val_Si-sdr: 0.5366\n",
      "Epoch 137/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5002 - Si-sdr: 3.5739\n",
      "Epoch 00137: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.4658 - Si-sdr: 3.5720 - val_loss: 230.6030 - val_Si-sdr: 0.4442\n",
      "Epoch 138/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6720 - Si-sdr: 3.5561\n",
      "Epoch 00138: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.6371 - Si-sdr: 3.5576 - val_loss: 230.4112 - val_Si-sdr: 0.4689\n",
      "Epoch 139/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5347 - Si-sdr: 3.5589\n",
      "Epoch 00139: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.5574 - Si-sdr: 3.5603 - val_loss: 230.2344 - val_Si-sdr: 0.4472\n",
      "Epoch 140/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7466 - Si-sdr: 3.5635\n",
      "Epoch 00140: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 226.7239 - Si-sdr: 3.5632 - val_loss: 230.3472 - val_Si-sdr: 0.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4908 - Si-sdr: 3.5644\n",
      "Epoch 00141: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 166ms/step - loss: 226.4609 - Si-sdr: 3.5639 - val_loss: 230.2314 - val_Si-sdr: 0.5139\n",
      "Epoch 142/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4321 - Si-sdr: 3.5601\n",
      "Epoch 00142: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.4459 - Si-sdr: 3.5641 - val_loss: 230.5309 - val_Si-sdr: 0.5068\n",
      "Epoch 143/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.5152 - Si-sdr: 3.5576\n",
      "Epoch 00143: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.5152 - Si-sdr: 3.5576 - val_loss: 229.8722 - val_Si-sdr: 0.5050\n",
      "Epoch 144/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6252 - Si-sdr: 3.5571\n",
      "Epoch 00144: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.5140 - Si-sdr: 3.5575 - val_loss: 228.7346 - val_Si-sdr: 0.5259\n",
      "Epoch 145/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3958 - Si-sdr: 3.5634\n",
      "Epoch 00145: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.4255 - Si-sdr: 3.5619 - val_loss: 229.5221 - val_Si-sdr: 0.4923\n",
      "Epoch 146/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3550 - Si-sdr: 3.5514\n",
      "Epoch 00146: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.4884 - Si-sdr: 3.5548 - val_loss: 230.7337 - val_Si-sdr: 0.4721\n",
      "Epoch 147/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.7423 - Si-sdr: 3.5486\n",
      "Epoch 00147: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.7423 - Si-sdr: 3.5486 - val_loss: 230.7813 - val_Si-sdr: 0.4528\n",
      "Epoch 148/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.7368 - Si-sdr: 3.5475\n",
      "Epoch 00148: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.7567 - Si-sdr: 3.5504 - val_loss: 231.4942 - val_Si-sdr: 0.4394\n",
      "Epoch 149/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4475 - Si-sdr: 3.5536\n",
      "Epoch 00149: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.4925 - Si-sdr: 3.5562 - val_loss: 230.5305 - val_Si-sdr: 0.4069\n",
      "Epoch 150/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.2116 - Si-sdr: 3.5764\n",
      "Epoch 00150: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.1373 - Si-sdr: 3.5755 - val_loss: 229.6110 - val_Si-sdr: 0.5593\n",
      "Epoch 151/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.2308 - Si-sdr: 3.5814\n",
      "Epoch 00151: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 226.2308 - Si-sdr: 3.5814 - val_loss: 229.9701 - val_Si-sdr: 0.4996\n",
      "Epoch 152/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.1195 - Si-sdr: 3.5638\n",
      "Epoch 00152: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.0923 - Si-sdr: 3.5653 - val_loss: 230.4169 - val_Si-sdr: 0.4861\n",
      "Epoch 153/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5032 - Si-sdr: 3.5684\n",
      "Epoch 00153: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.3965 - Si-sdr: 3.5683 - val_loss: 230.8629 - val_Si-sdr: 0.4659\n",
      "Epoch 154/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6005 - Si-sdr: 3.5471\n",
      "Epoch 00154: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.5809 - Si-sdr: 3.5475 - val_loss: 231.2671 - val_Si-sdr: 0.4472\n",
      "Epoch 155/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3491 - Si-sdr: 3.5612\n",
      "Epoch 00155: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.3414 - Si-sdr: 3.5614 - val_loss: 229.9361 - val_Si-sdr: 0.4974\n",
      "Epoch 156/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.5897 - Si-sdr: 3.5732\n",
      "Epoch 00156: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 226.4618 - Si-sdr: 3.5699 - val_loss: 230.7352 - val_Si-sdr: 0.4321\n",
      "Epoch 157/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0855 - Si-sdr: 3.5745\n",
      "Epoch 00157: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 226.0652 - Si-sdr: 3.5750 - val_loss: 229.4528 - val_Si-sdr: 0.5184\n",
      "Epoch 158/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7681 - Si-sdr: 3.5781\n",
      "Epoch 00158: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.8895 - Si-sdr: 3.5798 - val_loss: 229.3574 - val_Si-sdr: 0.5046\n",
      "Epoch 159/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4335 - Si-sdr: 3.5567\n",
      "Epoch 00159: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 226.5508 - Si-sdr: 3.5602 - val_loss: 230.3084 - val_Si-sdr: 0.4734\n",
      "Epoch 160/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.6308 - Si-sdr: 3.5482\n",
      "Epoch 00160: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.5120 - Si-sdr: 3.5468 - val_loss: 229.3144 - val_Si-sdr: 0.5498\n",
      "Epoch 161/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.0752 - Si-sdr: 3.5644\n",
      "Epoch 00161: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.0752 - Si-sdr: 3.5644 - val_loss: 229.6039 - val_Si-sdr: 0.4886\n",
      "Epoch 162/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4467 - Si-sdr: 3.5582\n",
      "Epoch 00162: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.5426 - Si-sdr: 3.5582 - val_loss: 229.8564 - val_Si-sdr: 0.5007\n",
      "Epoch 163/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3520 - Si-sdr: 3.5691\n",
      "Epoch 00163: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.3084 - Si-sdr: 3.5663 - val_loss: 229.5723 - val_Si-sdr: 0.4991\n",
      "Epoch 164/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.0745 - Si-sdr: 3.5684\n",
      "Epoch 00164: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.0745 - Si-sdr: 3.5684 - val_loss: 229.4135 - val_Si-sdr: 0.4942\n",
      "Epoch 165/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3616 - Si-sdr: 3.5544\n",
      "Epoch 00165: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.3838 - Si-sdr: 3.5581 - val_loss: 229.5143 - val_Si-sdr: 0.5863\n",
      "Epoch 166/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.2446 - Si-sdr: 3.5642\n",
      "Epoch 00166: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.2417 - Si-sdr: 3.5628 - val_loss: 230.1273 - val_Si-sdr: 0.5230\n",
      "Epoch 167/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3823 - Si-sdr: 3.5609\n",
      "Epoch 00167: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.4125 - Si-sdr: 3.5601 - val_loss: 230.4146 - val_Si-sdr: 0.5307\n",
      "Epoch 168/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9547 - Si-sdr: 3.5850\n",
      "Epoch 00168: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 225.9298 - Si-sdr: 3.5824 - val_loss: 230.6986 - val_Si-sdr: 0.4658\n",
      "Epoch 169/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.2871 - Si-sdr: 3.5609\n",
      "Epoch 00169: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 166ms/step - loss: 226.4085 - Si-sdr: 3.5638 - val_loss: 229.6111 - val_Si-sdr: 0.5387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3437 - Si-sdr: 3.5665\n",
      "Epoch 00170: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.3878 - Si-sdr: 3.5668 - val_loss: 229.8126 - val_Si-sdr: 0.5042\n",
      "Epoch 171/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.4209 - Si-sdr: 3.5665\n",
      "Epoch 00171: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.3158 - Si-sdr: 3.5635 - val_loss: 229.5727 - val_Si-sdr: 0.5726\n",
      "Epoch 172/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3431 - Si-sdr: 3.5734\n",
      "Epoch 00172: val_loss did not improve from 228.70078\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.2159 - Si-sdr: 3.5686 - val_loss: 230.1791 - val_Si-sdr: 0.4449\n",
      "Epoch 173/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0233 - Si-sdr: 3.5799\n",
      "Epoch 00173: val_loss improved from 228.70078 to 228.02232, saving model to ./CKPT\\CKP_ep_173__loss_228.02232_.h5\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.0637 - Si-sdr: 3.5818 - val_loss: 228.0223 - val_Si-sdr: 0.5890\n",
      "Epoch 174/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.9508 - Si-sdr: 3.5730\n",
      "Epoch 00174: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.9508 - Si-sdr: 3.5730 - val_loss: 228.3665 - val_Si-sdr: 0.5995\n",
      "Epoch 175/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0818 - Si-sdr: 3.5728\n",
      "Epoch 00175: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 167ms/step - loss: 226.0431 - Si-sdr: 3.5727 - val_loss: 229.8306 - val_Si-sdr: 0.4979\n",
      "Epoch 176/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7769 - Si-sdr: 3.5753\n",
      "Epoch 00176: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.7828 - Si-sdr: 3.5734 - val_loss: 229.1311 - val_Si-sdr: 0.5651\n",
      "Epoch 177/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9722 - Si-sdr: 3.5893\n",
      "Epoch 00177: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.9807 - Si-sdr: 3.5889 - val_loss: 228.9321 - val_Si-sdr: 0.5880\n",
      "Epoch 178/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.2301 - Si-sdr: 3.5554\n",
      "Epoch 00178: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 226.2072 - Si-sdr: 3.5591 - val_loss: 228.6562 - val_Si-sdr: 0.5774\n",
      "Epoch 179/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.1130 - Si-sdr: 3.5832\n",
      "Epoch 00179: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.1227 - Si-sdr: 3.5845 - val_loss: 229.5386 - val_Si-sdr: 0.5627\n",
      "Epoch 180/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6121 - Si-sdr: 3.5720\n",
      "Epoch 00180: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.7139 - Si-sdr: 3.5755 - val_loss: 229.5485 - val_Si-sdr: 0.5023\n",
      "Epoch 181/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0693 - Si-sdr: 3.5789\n",
      "Epoch 00181: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.9160 - Si-sdr: 3.5769 - val_loss: 229.0595 - val_Si-sdr: 0.5450\n",
      "Epoch 182/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.7026 - Si-sdr: 3.5895\n",
      "Epoch 00182: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.7026 - Si-sdr: 3.5895 - val_loss: 228.9666 - val_Si-sdr: 0.4974\n",
      "Epoch 183/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0047 - Si-sdr: 3.5631\n",
      "Epoch 00183: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.9549 - Si-sdr: 3.5672 - val_loss: 228.8716 - val_Si-sdr: 0.5517\n",
      "Epoch 184/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0868 - Si-sdr: 3.5790\n",
      "Epoch 00184: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.0346 - Si-sdr: 3.5733 - val_loss: 230.4422 - val_Si-sdr: 0.4709\n",
      "Epoch 185/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.3739 - Si-sdr: 3.5944\n",
      "Epoch 00185: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 225.3739 - Si-sdr: 3.5944 - val_loss: 229.3781 - val_Si-sdr: 0.4974\n",
      "Epoch 186/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.0584 - Si-sdr: 3.5957\n",
      "Epoch 00186: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.9110 - Si-sdr: 3.5903 - val_loss: 230.4038 - val_Si-sdr: 0.4511\n",
      "Epoch 187/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.1460 - Si-sdr: 3.5789\n",
      "Epoch 00187: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.0677 - Si-sdr: 3.5774 - val_loss: 228.5324 - val_Si-sdr: 0.5292\n",
      "Epoch 188/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.1821 - Si-sdr: 3.5713\n",
      "Epoch 00188: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.0479 - Si-sdr: 3.5672 - val_loss: 229.7576 - val_Si-sdr: 0.5334\n",
      "Epoch 189/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6896 - Si-sdr: 3.5898\n",
      "Epoch 00189: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.7348 - Si-sdr: 3.5938 - val_loss: 228.9801 - val_Si-sdr: 0.5230\n",
      "Epoch 190/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7910 - Si-sdr: 3.5788\n",
      "Epoch 00190: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.9350 - Si-sdr: 3.5786 - val_loss: 229.3819 - val_Si-sdr: 0.5791\n",
      "Epoch 191/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5710 - Si-sdr: 3.5793\n",
      "Epoch 00191: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 225.7853 - Si-sdr: 3.5843 - val_loss: 229.7931 - val_Si-sdr: 0.5865\n",
      "Epoch 192/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.1361 - Si-sdr: 3.5733\n",
      "Epoch 00192: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 226.1361 - Si-sdr: 3.5733 - val_loss: 229.3096 - val_Si-sdr: 0.5671\n",
      "Epoch 193/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 226.2008 - Si-sdr: 3.5750\n",
      "Epoch 00193: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 226.2008 - Si-sdr: 3.5750 - val_loss: 228.8740 - val_Si-sdr: 0.5733\n",
      "Epoch 194/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9443 - Si-sdr: 3.5874\n",
      "Epoch 00194: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.9923 - Si-sdr: 3.5888 - val_loss: 230.2143 - val_Si-sdr: 0.4720\n",
      "Epoch 195/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4765 - Si-sdr: 3.6118\n",
      "Epoch 00195: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 225.4291 - Si-sdr: 3.6109 - val_loss: 230.6359 - val_Si-sdr: 0.5384\n",
      "Epoch 196/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6433 - Si-sdr: 3.5765\n",
      "Epoch 00196: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 164ms/step - loss: 225.7446 - Si-sdr: 3.5782 - val_loss: 229.7903 - val_Si-sdr: 0.4657\n",
      "Epoch 197/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9807 - Si-sdr: 3.5665\n",
      "Epoch 00197: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 226.1088 - Si-sdr: 3.5684 - val_loss: 229.9088 - val_Si-sdr: 0.5086\n",
      "Epoch 198/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.8356 - Si-sdr: 3.5889\n",
      "Epoch 00198: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 225.8356 - Si-sdr: 3.5889 - val_loss: 229.9370 - val_Si-sdr: 0.4966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 226.3222 - Si-sdr: 3.5768\n",
      "Epoch 00199: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 226.1169 - Si-sdr: 3.5699 - val_loss: 229.4077 - val_Si-sdr: 0.5516\n",
      "Epoch 200/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5006 - Si-sdr: 3.5913\n",
      "Epoch 00200: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 225.5797 - Si-sdr: 3.5905 - val_loss: 229.0087 - val_Si-sdr: 0.5937\n",
      "Epoch 201/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5770 - Si-sdr: 3.5872\n",
      "Epoch 00201: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 225.4600 - Si-sdr: 3.5859 - val_loss: 229.5267 - val_Si-sdr: 0.5123\n",
      "Epoch 202/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3726 - Si-sdr: 3.5941\n",
      "Epoch 00202: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.4294 - Si-sdr: 3.5953 - val_loss: 230.5221 - val_Si-sdr: 0.4905\n",
      "Epoch 203/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7021 - Si-sdr: 3.5798\n",
      "Epoch 00203: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 163ms/step - loss: 225.6944 - Si-sdr: 3.5808 - val_loss: 228.7802 - val_Si-sdr: 0.5596\n",
      "Epoch 204/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7338 - Si-sdr: 3.5886\n",
      "Epoch 00204: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.7103 - Si-sdr: 3.5875 - val_loss: 229.5671 - val_Si-sdr: 0.5511\n",
      "Epoch 205/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4385 - Si-sdr: 3.5908\n",
      "Epoch 00205: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.3939 - Si-sdr: 3.5913 - val_loss: 229.8042 - val_Si-sdr: 0.4814\n",
      "Epoch 206/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6624 - Si-sdr: 3.5857\n",
      "Epoch 00206: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.6036 - Si-sdr: 3.5856 - val_loss: 228.6811 - val_Si-sdr: 0.5786\n",
      "Epoch 207/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9035 - Si-sdr: 3.5991\n",
      "Epoch 00207: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 225.8187 - Si-sdr: 3.5978 - val_loss: 229.8098 - val_Si-sdr: 0.4857\n",
      "Epoch 208/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5659 - Si-sdr: 3.5924\n",
      "Epoch 00208: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.5545 - Si-sdr: 3.5912 - val_loss: 229.6158 - val_Si-sdr: 0.5244\n",
      "Epoch 209/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9259 - Si-sdr: 3.5933\n",
      "Epoch 00209: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.8131 - Si-sdr: 3.5902 - val_loss: 229.0648 - val_Si-sdr: 0.5571\n",
      "Epoch 210/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7044 - Si-sdr: 3.5765\n",
      "Epoch 00210: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.7884 - Si-sdr: 3.5772 - val_loss: 231.7214 - val_Si-sdr: 0.4505\n",
      "Epoch 211/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7625 - Si-sdr: 3.5869\n",
      "Epoch 00211: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.7715 - Si-sdr: 3.5923 - val_loss: 229.6056 - val_Si-sdr: 0.5033\n",
      "Epoch 212/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5745 - Si-sdr: 3.5861\n",
      "Epoch 00212: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 165ms/step - loss: 225.4975 - Si-sdr: 3.5830 - val_loss: 229.3585 - val_Si-sdr: 0.5410\n",
      "Epoch 213/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7591 - Si-sdr: 3.5842\n",
      "Epoch 00213: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.6161 - Si-sdr: 3.5868 - val_loss: 230.5936 - val_Si-sdr: 0.4478\n",
      "Epoch 214/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6037 - Si-sdr: 3.5929\n",
      "Epoch 00214: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 45s 166ms/step - loss: 225.5834 - Si-sdr: 3.5911 - val_loss: 228.6872 - val_Si-sdr: 0.6109\n",
      "Epoch 215/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1418 - Si-sdr: 3.6170\n",
      "Epoch 00215: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.0929 - Si-sdr: 3.6154 - val_loss: 228.9465 - val_Si-sdr: 0.5429\n",
      "Epoch 216/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.6706 - Si-sdr: 3.5861\n",
      "Epoch 00216: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.6706 - Si-sdr: 3.5861 - val_loss: 229.8230 - val_Si-sdr: 0.5139\n",
      "Epoch 217/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.9901 - Si-sdr: 3.5703\n",
      "Epoch 00217: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.9590 - Si-sdr: 3.5713 - val_loss: 230.0874 - val_Si-sdr: 0.4517\n",
      "Epoch 218/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.2146 - Si-sdr: 3.6099\n",
      "Epoch 00218: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 166ms/step - loss: 225.1751 - Si-sdr: 3.6154 - val_loss: 229.6374 - val_Si-sdr: 0.5695\n",
      "Epoch 219/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.9048 - Si-sdr: 3.5830\n",
      "Epoch 00219: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 225.9048 - Si-sdr: 3.5830 - val_loss: 229.7215 - val_Si-sdr: 0.5156\n",
      "Epoch 220/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5465 - Si-sdr: 3.5805\n",
      "Epoch 00220: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.4567 - Si-sdr: 3.5803 - val_loss: 229.5539 - val_Si-sdr: 0.4918\n",
      "Epoch 221/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7108 - Si-sdr: 3.5834\n",
      "Epoch 00221: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.7156 - Si-sdr: 3.5837 - val_loss: 229.4075 - val_Si-sdr: 0.5960\n",
      "Epoch 222/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7725 - Si-sdr: 3.5897\n",
      "Epoch 00222: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.8401 - Si-sdr: 3.5899 - val_loss: 229.1386 - val_Si-sdr: 0.6333\n",
      "Epoch 223/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5280 - Si-sdr: 3.6097\n",
      "Epoch 00223: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.3823 - Si-sdr: 3.6028 - val_loss: 229.0806 - val_Si-sdr: 0.5207\n",
      "Epoch 224/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.2019 - Si-sdr: 3.6036\n",
      "Epoch 00224: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.1114 - Si-sdr: 3.6031 - val_loss: 229.2196 - val_Si-sdr: 0.5285\n",
      "Epoch 225/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4836 - Si-sdr: 3.5910\n",
      "Epoch 00225: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.3867 - Si-sdr: 3.5892 - val_loss: 228.3121 - val_Si-sdr: 0.5467\n",
      "Epoch 226/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3678 - Si-sdr: 3.6030\n",
      "Epoch 00226: val_loss did not improve from 228.02232\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.3073 - Si-sdr: 3.6037 - val_loss: 229.6451 - val_Si-sdr: 0.4908\n",
      "Epoch 227/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.2070 - Si-sdr: 3.6014\n",
      "Epoch 00227: val_loss improved from 228.02232 to 227.94547, saving model to ./CKPT\\CKP_ep_227__loss_227.94547_.h5\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.2070 - Si-sdr: 3.6014 - val_loss: 227.9455 - val_Si-sdr: 0.6016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6565 - Si-sdr: 3.5928\n",
      "Epoch 00228: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.5786 - Si-sdr: 3.5885 - val_loss: 229.4749 - val_Si-sdr: 0.5573\n",
      "Epoch 229/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.8131 - Si-sdr: 3.5824\n",
      "Epoch 00229: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.8590 - Si-sdr: 3.5855 - val_loss: 229.4561 - val_Si-sdr: 0.5704\n",
      "Epoch 230/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8772 - Si-sdr: 3.6178\n",
      "Epoch 00230: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.0476 - Si-sdr: 3.6183 - val_loss: 229.4787 - val_Si-sdr: 0.4778\n",
      "Epoch 231/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5134 - Si-sdr: 3.5871\n",
      "Epoch 00231: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.5370 - Si-sdr: 3.5861 - val_loss: 228.7955 - val_Si-sdr: 0.5463\n",
      "Epoch 232/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.6333 - Si-sdr: 3.5915\n",
      "Epoch 00232: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.6067 - Si-sdr: 3.5896 - val_loss: 229.4942 - val_Si-sdr: 0.4777\n",
      "Epoch 233/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.2505 - Si-sdr: 3.5979\n",
      "Epoch 00233: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.2505 - Si-sdr: 3.5979 - val_loss: 229.8026 - val_Si-sdr: 0.5808\n",
      "Epoch 234/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4296 - Si-sdr: 3.6067\n",
      "Epoch 00234: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.3265 - Si-sdr: 3.6072 - val_loss: 229.7190 - val_Si-sdr: 0.4664\n",
      "Epoch 235/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4644 - Si-sdr: 3.5938\n",
      "Epoch 00235: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.5313 - Si-sdr: 3.5920 - val_loss: 229.9450 - val_Si-sdr: 0.4817\n",
      "Epoch 236/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0883 - Si-sdr: 3.6100\n",
      "Epoch 00236: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.1472 - Si-sdr: 3.6130 - val_loss: 229.7900 - val_Si-sdr: 0.5230\n",
      "Epoch 237/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3425 - Si-sdr: 3.5946\n",
      "Epoch 00237: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.3781 - Si-sdr: 3.5971 - val_loss: 230.2561 - val_Si-sdr: 0.5233\n",
      "Epoch 238/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5724 - Si-sdr: 3.5810\n",
      "Epoch 00238: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.4578 - Si-sdr: 3.5779 - val_loss: 229.1830 - val_Si-sdr: 0.5151\n",
      "Epoch 239/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9878 - Si-sdr: 3.5965\n",
      "Epoch 00239: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.9603 - Si-sdr: 3.5998 - val_loss: 230.4095 - val_Si-sdr: 0.5345\n",
      "Epoch 240/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5211 - Si-sdr: 3.5899\n",
      "Epoch 00240: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.4237 - Si-sdr: 3.5858 - val_loss: 229.9109 - val_Si-sdr: 0.5667\n",
      "Epoch 241/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5049 - Si-sdr: 3.5923\n",
      "Epoch 00241: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.4583 - Si-sdr: 3.5953 - val_loss: 229.4796 - val_Si-sdr: 0.5154\n",
      "Epoch 242/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7088 - Si-sdr: 3.6030\n",
      "Epoch 00242: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.5680 - Si-sdr: 3.6024 - val_loss: 229.7491 - val_Si-sdr: 0.4457\n",
      "Epoch 243/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.1529 - Si-sdr: 3.6127\n",
      "Epoch 00243: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.1529 - Si-sdr: 3.6127 - val_loss: 229.3461 - val_Si-sdr: 0.5522\n",
      "Epoch 244/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9888 - Si-sdr: 3.6101\n",
      "Epoch 00244: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.9205 - Si-sdr: 3.6118 - val_loss: 229.6691 - val_Si-sdr: 0.5289\n",
      "Epoch 245/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.4792 - Si-sdr: 3.5934\n",
      "Epoch 00245: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.4792 - Si-sdr: 3.5934 - val_loss: 228.6700 - val_Si-sdr: 0.5219\n",
      "Epoch 246/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3920 - Si-sdr: 3.6055\n",
      "Epoch 00246: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.3155 - Si-sdr: 3.6021 - val_loss: 229.1559 - val_Si-sdr: 0.5137\n",
      "Epoch 247/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3096 - Si-sdr: 3.6014\n",
      "Epoch 00247: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.2303 - Si-sdr: 3.6001 - val_loss: 229.5025 - val_Si-sdr: 0.4774\n",
      "Epoch 248/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1925 - Si-sdr: 3.6128\n",
      "Epoch 00248: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.1309 - Si-sdr: 3.6142 - val_loss: 229.0959 - val_Si-sdr: 0.6183\n",
      "Epoch 249/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9069 - Si-sdr: 3.6108\n",
      "Epoch 00249: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.8033 - Si-sdr: 3.6071 - val_loss: 229.2508 - val_Si-sdr: 0.4969\n",
      "Epoch 250/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5790 - Si-sdr: 3.6239\n",
      "Epoch 00250: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.4725 - Si-sdr: 3.6192 - val_loss: 229.8599 - val_Si-sdr: 0.4780\n",
      "Epoch 251/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1395 - Si-sdr: 3.6034\n",
      "Epoch 00251: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.1585 - Si-sdr: 3.6057 - val_loss: 230.4312 - val_Si-sdr: 0.4525\n",
      "Epoch 252/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4241 - Si-sdr: 3.5875\n",
      "Epoch 00252: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.4278 - Si-sdr: 3.5856 - val_loss: 230.0054 - val_Si-sdr: 0.5368\n",
      "Epoch 253/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1460 - Si-sdr: 3.6038\n",
      "Epoch 00253: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.2372 - Si-sdr: 3.6019 - val_loss: 230.3260 - val_Si-sdr: 0.4667\n",
      "Epoch 254/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1408 - Si-sdr: 3.6041\n",
      "Epoch 00254: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.1385 - Si-sdr: 3.6023 - val_loss: 229.1818 - val_Si-sdr: 0.5385\n",
      "Epoch 255/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3949 - Si-sdr: 3.5912\n",
      "Epoch 00255: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.3889 - Si-sdr: 3.5899 - val_loss: 228.3943 - val_Si-sdr: 0.5745\n",
      "Epoch 256/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1310 - Si-sdr: 3.6158\n",
      "Epoch 00256: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.0334 - Si-sdr: 3.6173 - val_loss: 229.1565 - val_Si-sdr: 0.5177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9077 - Si-sdr: 3.6251\n",
      "Epoch 00257: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.8018 - Si-sdr: 3.6202 - val_loss: 230.0838 - val_Si-sdr: 0.4855\n",
      "Epoch 258/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.1323 - Si-sdr: 3.6034\n",
      "Epoch 00258: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.1323 - Si-sdr: 3.6034 - val_loss: 229.0191 - val_Si-sdr: 0.5134\n",
      "Epoch 259/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.7722 - Si-sdr: 3.6147\n",
      "Epoch 00259: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.7722 - Si-sdr: 3.6147 - val_loss: 229.7276 - val_Si-sdr: 0.5225\n",
      "Epoch 260/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.7289 - Si-sdr: 3.6091\n",
      "Epoch 00260: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.5749 - Si-sdr: 3.6093 - val_loss: 229.5843 - val_Si-sdr: 0.5480\n",
      "Epoch 261/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 225.2255 - Si-sdr: 3.6042\n",
      "Epoch 00261: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.2255 - Si-sdr: 3.6042 - val_loss: 228.6877 - val_Si-sdr: 0.6076\n",
      "Epoch 262/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1568 - Si-sdr: 3.5976\n",
      "Epoch 00262: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.1830 - Si-sdr: 3.5994 - val_loss: 229.3628 - val_Si-sdr: 0.5311\n",
      "Epoch 263/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0395 - Si-sdr: 3.6087\n",
      "Epoch 00263: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.0486 - Si-sdr: 3.6025 - val_loss: 228.8694 - val_Si-sdr: 0.6234\n",
      "Epoch 264/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8064 - Si-sdr: 3.6118\n",
      "Epoch 00264: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8384 - Si-sdr: 3.6156 - val_loss: 228.7491 - val_Si-sdr: 0.5053\n",
      "Epoch 265/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1846 - Si-sdr: 3.6118\n",
      "Epoch 00265: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.0979 - Si-sdr: 3.6103 - val_loss: 228.5033 - val_Si-sdr: 0.5589\n",
      "Epoch 266/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.2001 - Si-sdr: 3.6045\n",
      "Epoch 00266: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.1182 - Si-sdr: 3.6047 - val_loss: 229.4532 - val_Si-sdr: 0.5202\n",
      "Epoch 267/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.7305 - Si-sdr: 3.6194\n",
      "Epoch 00267: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.7868 - Si-sdr: 3.6218 - val_loss: 228.1983 - val_Si-sdr: 0.5712\n",
      "Epoch 268/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9923 - Si-sdr: 3.6003\n",
      "Epoch 00268: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.8875 - Si-sdr: 3.6012 - val_loss: 230.1258 - val_Si-sdr: 0.5834\n",
      "Epoch 269/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9716 - Si-sdr: 3.6093\n",
      "Epoch 00269: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 225.1228 - Si-sdr: 3.6123 - val_loss: 229.0911 - val_Si-sdr: 0.5404\n",
      "Epoch 270/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.9315 - Si-sdr: 3.6228\n",
      "Epoch 00270: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.9315 - Si-sdr: 3.6228 - val_loss: 228.8005 - val_Si-sdr: 0.5671\n",
      "Epoch 271/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9633 - Si-sdr: 3.6198\n",
      "Epoch 00271: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8630 - Si-sdr: 3.6111 - val_loss: 228.9853 - val_Si-sdr: 0.5780\n",
      "Epoch 272/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5934 - Si-sdr: 3.6324\n",
      "Epoch 00272: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.5283 - Si-sdr: 3.6331 - val_loss: 229.2928 - val_Si-sdr: 0.5718\n",
      "Epoch 273/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9347 - Si-sdr: 3.6090\n",
      "Epoch 00273: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.7962 - Si-sdr: 3.6088 - val_loss: 228.8288 - val_Si-sdr: 0.6015\n",
      "Epoch 274/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1337 - Si-sdr: 3.5871\n",
      "Epoch 00274: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 225.0976 - Si-sdr: 3.5921 - val_loss: 229.4122 - val_Si-sdr: 0.5121\n",
      "Epoch 275/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9412 - Si-sdr: 3.6087\n",
      "Epoch 00275: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8853 - Si-sdr: 3.6131 - val_loss: 229.1170 - val_Si-sdr: 0.5170\n",
      "Epoch 276/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0591 - Si-sdr: 3.6070\n",
      "Epoch 00276: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.0412 - Si-sdr: 3.6071 - val_loss: 229.1878 - val_Si-sdr: 0.5237\n",
      "Epoch 277/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9036 - Si-sdr: 3.6073\n",
      "Epoch 00277: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.9091 - Si-sdr: 3.6099 - val_loss: 229.9861 - val_Si-sdr: 0.4541\n",
      "Epoch 278/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.8728 - Si-sdr: 3.6118\n",
      "Epoch 00278: val_loss did not improve from 227.94547\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.8728 - Si-sdr: 3.6118 - val_loss: 228.2700 - val_Si-sdr: 0.6132\n",
      "Epoch 279/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0379 - Si-sdr: 3.6186\n",
      "Epoch 00279: val_loss improved from 227.94547 to 227.75038, saving model to ./CKPT\\CKP_ep_279__loss_227.75038_.h5\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.9631 - Si-sdr: 3.6132 - val_loss: 227.7504 - val_Si-sdr: 0.5509\n",
      "Epoch 280/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1755 - Si-sdr: 3.6201\n",
      "Epoch 00280: val_loss did not improve from 227.75038\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.3495 - Si-sdr: 3.6234 - val_loss: 230.7137 - val_Si-sdr: 0.4695\n",
      "Epoch 281/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.7848 - Si-sdr: 3.5994\n",
      "Epoch 00281: val_loss improved from 227.75038 to 227.27164, saving model to ./CKPT\\CKP_ep_281__loss_227.27164_.h5\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.9119 - Si-sdr: 3.6000 - val_loss: 227.2716 - val_Si-sdr: 0.6784\n",
      "Epoch 282/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.7591 - Si-sdr: 3.6192\n",
      "Epoch 00282: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.7185 - Si-sdr: 3.6163 - val_loss: 230.5447 - val_Si-sdr: 0.5195\n",
      "Epoch 283/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1161 - Si-sdr: 3.6096\n",
      "Epoch 00283: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.0545 - Si-sdr: 3.6120 - val_loss: 229.2707 - val_Si-sdr: 0.5516\n",
      "Epoch 284/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0733 - Si-sdr: 3.6135\n",
      "Epoch 00284: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.0710 - Si-sdr: 3.6165 - val_loss: 229.8470 - val_Si-sdr: 0.5313\n",
      "Epoch 285/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8570 - Si-sdr: 3.6032\n",
      "Epoch 00285: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8696 - Si-sdr: 3.6030 - val_loss: 228.1035 - val_Si-sdr: 0.5701\n",
      "Epoch 286/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0960 - Si-sdr: 3.6063\n",
      "Epoch 00286: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.1880 - Si-sdr: 3.6058 - val_loss: 229.3044 - val_Si-sdr: 0.5171\n",
      "Epoch 287/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.3182 - Si-sdr: 3.6071\n",
      "Epoch 00287: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 225.2893 - Si-sdr: 3.6047 - val_loss: 228.8392 - val_Si-sdr: 0.5701\n",
      "Epoch 288/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.0497 - Si-sdr: 3.6094\n",
      "Epoch 00288: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8875 - Si-sdr: 3.6067 - val_loss: 228.3672 - val_Si-sdr: 0.5858\n",
      "Epoch 289/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6369 - Si-sdr: 3.6296\n",
      "Epoch 00289: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.5742 - Si-sdr: 3.6288 - val_loss: 227.4857 - val_Si-sdr: 0.5871\n",
      "Epoch 290/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8310 - Si-sdr: 3.6255\n",
      "Epoch 00290: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.7331 - Si-sdr: 3.6190 - val_loss: 230.2449 - val_Si-sdr: 0.4875\n",
      "Epoch 291/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6437 - Si-sdr: 3.6391\n",
      "Epoch 00291: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.5401 - Si-sdr: 3.6335 - val_loss: 229.1580 - val_Si-sdr: 0.4926\n",
      "Epoch 292/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5893 - Si-sdr: 3.6151\n",
      "Epoch 00292: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.6312 - Si-sdr: 3.6176 - val_loss: 229.1130 - val_Si-sdr: 0.5463\n",
      "Epoch 293/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.5242 - Si-sdr: 3.5939\n",
      "Epoch 00293: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.5110 - Si-sdr: 3.5940 - val_loss: 228.9005 - val_Si-sdr: 0.5319\n",
      "Epoch 294/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.9146 - Si-sdr: 3.6156\n",
      "Epoch 00294: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.9146 - Si-sdr: 3.6156 - val_loss: 228.4614 - val_Si-sdr: 0.5469\n",
      "Epoch 295/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6953 - Si-sdr: 3.6084\n",
      "Epoch 00295: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.8128 - Si-sdr: 3.6072 - val_loss: 228.2613 - val_Si-sdr: 0.5737\n",
      "Epoch 296/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.5830 - Si-sdr: 3.6274\n",
      "Epoch 00296: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.5830 - Si-sdr: 3.6274 - val_loss: 228.0457 - val_Si-sdr: 0.5576\n",
      "Epoch 297/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1326 - Si-sdr: 3.6286\n",
      "Epoch 00297: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.1817 - Si-sdr: 3.6317 - val_loss: 229.3285 - val_Si-sdr: 0.5030\n",
      "Epoch 298/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.7167 - Si-sdr: 3.6267\n",
      "Epoch 00298: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.7204 - Si-sdr: 3.6261 - val_loss: 229.9087 - val_Si-sdr: 0.5358\n",
      "Epoch 299/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1579 - Si-sdr: 3.6234\n",
      "Epoch 00299: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.2996 - Si-sdr: 3.6254 - val_loss: 228.8769 - val_Si-sdr: 0.5302\n",
      "Epoch 300/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.4995 - Si-sdr: 3.6026\n",
      "Epoch 00300: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 225.4675 - Si-sdr: 3.6035 - val_loss: 227.8841 - val_Si-sdr: 0.6091\n",
      "Epoch 301/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.2441 - Si-sdr: 3.6350\n",
      "Epoch 00301: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.2125 - Si-sdr: 3.6375 - val_loss: 228.6412 - val_Si-sdr: 0.6095\n",
      "Epoch 302/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.7722 - Si-sdr: 3.6099\n",
      "Epoch 00302: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.7722 - Si-sdr: 3.6099 - val_loss: 228.4506 - val_Si-sdr: 0.5287\n",
      "Epoch 303/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.9427 - Si-sdr: 3.6075\n",
      "Epoch 00303: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.9021 - Si-sdr: 3.6071 - val_loss: 229.2376 - val_Si-sdr: 0.4991\n",
      "Epoch 304/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8021 - Si-sdr: 3.6076\n",
      "Epoch 00304: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.7159 - Si-sdr: 3.6104 - val_loss: 229.3668 - val_Si-sdr: 0.5522\n",
      "Epoch 305/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 225.1072 - Si-sdr: 3.6087\n",
      "Epoch 00305: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.9959 - Si-sdr: 3.6094 - val_loss: 228.3392 - val_Si-sdr: 0.5758\n",
      "Epoch 306/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.1373 - Si-sdr: 3.6407\n",
      "Epoch 00306: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.1373 - Si-sdr: 3.6407 - val_loss: 228.0248 - val_Si-sdr: 0.5394\n",
      "Epoch 307/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6377 - Si-sdr: 3.6234\n",
      "Epoch 00307: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5803 - Si-sdr: 3.6226 - val_loss: 228.1809 - val_Si-sdr: 0.5597\n",
      "Epoch 308/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4859 - Si-sdr: 3.6194\n",
      "Epoch 00308: val_loss did not improve from 227.27164\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.4740 - Si-sdr: 3.6212 - val_loss: 227.8007 - val_Si-sdr: 0.6367\n",
      "Epoch 309/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.3350 - Si-sdr: 3.6289\n",
      "Epoch 00309: val_loss improved from 227.27164 to 227.24799, saving model to ./CKPT\\CKP_ep_309__loss_227.24799_.h5\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.3350 - Si-sdr: 3.6289 - val_loss: 227.2480 - val_Si-sdr: 0.6609\n",
      "Epoch 310/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4710 - Si-sdr: 3.6184\n",
      "Epoch 00310: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5338 - Si-sdr: 3.6205 - val_loss: 228.2812 - val_Si-sdr: 0.5585\n",
      "Epoch 311/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4604 - Si-sdr: 3.6253\n",
      "Epoch 00311: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.4334 - Si-sdr: 3.6280 - val_loss: 227.3632 - val_Si-sdr: 0.6393\n",
      "Epoch 312/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6415 - Si-sdr: 3.6216\n",
      "Epoch 00312: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.7962 - Si-sdr: 3.6222 - val_loss: 228.0672 - val_Si-sdr: 0.6217\n",
      "Epoch 313/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5380 - Si-sdr: 3.6208\n",
      "Epoch 00313: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5322 - Si-sdr: 3.6238 - val_loss: 227.9367 - val_Si-sdr: 0.5598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 314/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4130 - Si-sdr: 3.6245\n",
      "Epoch 00314: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.4639 - Si-sdr: 3.6255 - val_loss: 229.2841 - val_Si-sdr: 0.4969\n",
      "Epoch 315/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.5685 - Si-sdr: 3.6199\n",
      "Epoch 00315: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.5685 - Si-sdr: 3.6199 - val_loss: 228.0717 - val_Si-sdr: 0.5117\n",
      "Epoch 316/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5692 - Si-sdr: 3.6132\n",
      "Epoch 00316: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5082 - Si-sdr: 3.6173 - val_loss: 228.5407 - val_Si-sdr: 0.5880\n",
      "Epoch 317/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.5862 - Si-sdr: 3.6268\n",
      "Epoch 00317: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5862 - Si-sdr: 3.6268 - val_loss: 228.4173 - val_Si-sdr: 0.5512\n",
      "Epoch 318/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4529 - Si-sdr: 3.6271\n",
      "Epoch 00318: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.4200 - Si-sdr: 3.6291 - val_loss: 228.3412 - val_Si-sdr: 0.5588\n",
      "Epoch 319/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4410 - Si-sdr: 3.6218\n",
      "Epoch 00319: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.6095 - Si-sdr: 3.6226 - val_loss: 229.8752 - val_Si-sdr: 0.4806\n",
      "Epoch 320/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5625 - Si-sdr: 3.6209\n",
      "Epoch 00320: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.5532 - Si-sdr: 3.6243 - val_loss: 228.1913 - val_Si-sdr: 0.6390\n",
      "Epoch 321/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5210 - Si-sdr: 3.6326\n",
      "Epoch 00321: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.4852 - Si-sdr: 3.6304 - val_loss: 227.6216 - val_Si-sdr: 0.5750\n",
      "Epoch 322/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0616 - Si-sdr: 3.6283\n",
      "Epoch 00322: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 160ms/step - loss: 224.0782 - Si-sdr: 3.6294 - val_loss: 228.9494 - val_Si-sdr: 0.4985\n",
      "Epoch 323/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1724 - Si-sdr: 3.6323\n",
      "Epoch 00323: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.0909 - Si-sdr: 3.6317 - val_loss: 227.6770 - val_Si-sdr: 0.5889\n",
      "Epoch 324/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0073 - Si-sdr: 3.6369\n",
      "Epoch 00324: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.0492 - Si-sdr: 3.6393 - val_loss: 227.8358 - val_Si-sdr: 0.6197\n",
      "Epoch 325/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.7856 - Si-sdr: 3.6085\n",
      "Epoch 00325: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.7856 - Si-sdr: 3.6085 - val_loss: 229.4017 - val_Si-sdr: 0.5147\n",
      "Epoch 326/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.7150 - Si-sdr: 3.6158\n",
      "Epoch 00326: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.7623 - Si-sdr: 3.6158 - val_loss: 229.3889 - val_Si-sdr: 0.5187\n",
      "Epoch 327/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.2819 - Si-sdr: 3.6434\n",
      "Epoch 00327: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.2819 - Si-sdr: 3.6434 - val_loss: 227.7345 - val_Si-sdr: 0.5678\n",
      "Epoch 328/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0612 - Si-sdr: 3.6378\n",
      "Epoch 00328: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.0902 - Si-sdr: 3.6364 - val_loss: 228.8403 - val_Si-sdr: 0.5459\n",
      "Epoch 329/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5537 - Si-sdr: 3.6267\n",
      "Epoch 00329: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5204 - Si-sdr: 3.6280 - val_loss: 229.2438 - val_Si-sdr: 0.5413\n",
      "Epoch 330/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4360 - Si-sdr: 3.6262\n",
      "Epoch 00330: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.4140 - Si-sdr: 3.6274 - val_loss: 228.8849 - val_Si-sdr: 0.5259\n",
      "Epoch 331/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.5217 - Si-sdr: 3.6227\n",
      "Epoch 00331: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 43s 161ms/step - loss: 224.5217 - Si-sdr: 3.6227 - val_loss: 228.5971 - val_Si-sdr: 0.5823\n",
      "Epoch 332/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.2493 - Si-sdr: 3.6393\n",
      "Epoch 00332: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.2493 - Si-sdr: 3.6393 - val_loss: 228.2740 - val_Si-sdr: 0.5814\n",
      "Epoch 333/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9107 - Si-sdr: 3.6382\n",
      "Epoch 00333: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.9247 - Si-sdr: 3.6391 - val_loss: 228.0643 - val_Si-sdr: 0.5566\n",
      "Epoch 334/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.5907 - Si-sdr: 3.6264\n",
      "Epoch 00334: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 43s 160ms/step - loss: 224.6348 - Si-sdr: 3.6238 - val_loss: 227.9840 - val_Si-sdr: 0.5878\n",
      "Epoch 335/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.4848 - Si-sdr: 3.6284\n",
      "Epoch 00335: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 224.4572 - Si-sdr: 3.6309 - val_loss: 228.0751 - val_Si-sdr: 0.5992\n",
      "Epoch 336/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.2448 - Si-sdr: 3.6306\n",
      "Epoch 00336: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.2448 - Si-sdr: 3.6306 - val_loss: 228.0744 - val_Si-sdr: 0.6042\n",
      "Epoch 337/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7549 - Si-sdr: 3.6411\n",
      "Epoch 00337: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.7942 - Si-sdr: 3.6416 - val_loss: 228.5435 - val_Si-sdr: 0.6413\n",
      "Epoch 338/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.3887 - Si-sdr: 3.6232\n",
      "Epoch 00338: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.3887 - Si-sdr: 3.6232 - val_loss: 229.6819 - val_Si-sdr: 0.5347\n",
      "Epoch 339/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.8227 - Si-sdr: 3.6174\n",
      "Epoch 00339: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.8027 - Si-sdr: 3.6139 - val_loss: 229.0002 - val_Si-sdr: 0.5294\n",
      "Epoch 340/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7667 - Si-sdr: 3.6393\n",
      "Epoch 00340: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 165ms/step - loss: 223.7509 - Si-sdr: 3.6407 - val_loss: 228.3261 - val_Si-sdr: 0.5360\n",
      "Epoch 341/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.2304 - Si-sdr: 3.6353\n",
      "Epoch 00341: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.1941 - Si-sdr: 3.6348 - val_loss: 227.7291 - val_Si-sdr: 0.5990\n",
      "Epoch 342/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.0464 - Si-sdr: 3.6377\n",
      "Epoch 00342: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.0464 - Si-sdr: 3.6377 - val_loss: 228.7962 - val_Si-sdr: 0.5235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 343/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1629 - Si-sdr: 3.6343\n",
      "Epoch 00343: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 43s 162ms/step - loss: 224.1830 - Si-sdr: 3.6298 - val_loss: 228.9620 - val_Si-sdr: 0.5429\n",
      "Epoch 344/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1147 - Si-sdr: 3.6443\n",
      "Epoch 00344: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.1017 - Si-sdr: 3.6404 - val_loss: 228.3249 - val_Si-sdr: 0.5681\n",
      "Epoch 345/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9648 - Si-sdr: 3.6473\n",
      "Epoch 00345: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.9292 - Si-sdr: 3.6478 - val_loss: 227.4759 - val_Si-sdr: 0.6348\n",
      "Epoch 346/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1808 - Si-sdr: 3.6434\n",
      "Epoch 00346: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.0600 - Si-sdr: 3.6426 - val_loss: 229.9047 - val_Si-sdr: 0.5185\n",
      "Epoch 347/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.5703 - Si-sdr: 3.6386\n",
      "Epoch 00347: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.5703 - Si-sdr: 3.6386 - val_loss: 228.1256 - val_Si-sdr: 0.5884\n",
      "Epoch 348/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.6486 - Si-sdr: 3.6358\n",
      "Epoch 00348: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.5582 - Si-sdr: 3.6354 - val_loss: 228.1895 - val_Si-sdr: 0.5459\n",
      "Epoch 349/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.2676 - Si-sdr: 3.6265\n",
      "Epoch 00349: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.2253 - Si-sdr: 3.6270 - val_loss: 227.9534 - val_Si-sdr: 0.6273\n",
      "Epoch 350/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7677 - Si-sdr: 3.6460\n",
      "Epoch 00350: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.7821 - Si-sdr: 3.6420 - val_loss: 228.5821 - val_Si-sdr: 0.4619\n",
      "Epoch 351/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.1513 - Si-sdr: 3.6273\n",
      "Epoch 00351: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.1513 - Si-sdr: 3.6273 - val_loss: 229.0204 - val_Si-sdr: 0.5882\n",
      "Epoch 352/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.1460 - Si-sdr: 3.6374\n",
      "Epoch 00352: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.1460 - Si-sdr: 3.6374 - val_loss: 227.6614 - val_Si-sdr: 0.6494\n",
      "Epoch 353/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7969 - Si-sdr: 3.6415\n",
      "Epoch 00353: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.8737 - Si-sdr: 3.6432 - val_loss: 229.2457 - val_Si-sdr: 0.5200\n",
      "Epoch 354/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.5306 - Si-sdr: 3.6405\n",
      "Epoch 00354: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.5306 - Si-sdr: 3.6405 - val_loss: 228.9608 - val_Si-sdr: 0.5406\n",
      "Epoch 355/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.8005 - Si-sdr: 3.6369\n",
      "Epoch 00355: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.8245 - Si-sdr: 3.6401 - val_loss: 227.9435 - val_Si-sdr: 0.5678\n",
      "Epoch 356/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7259 - Si-sdr: 3.6408\n",
      "Epoch 00356: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.9892 - Si-sdr: 3.6448 - val_loss: 227.9519 - val_Si-sdr: 0.5670\n",
      "Epoch 357/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.3220 - Si-sdr: 3.6433\n",
      "Epoch 00357: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.1935 - Si-sdr: 3.6409 - val_loss: 228.6879 - val_Si-sdr: 0.5260\n",
      "Epoch 358/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0739 - Si-sdr: 3.6276\n",
      "Epoch 00358: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.0624 - Si-sdr: 3.6264 - val_loss: 228.7117 - val_Si-sdr: 0.5314\n",
      "Epoch 359/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.9675 - Si-sdr: 3.6316\n",
      "Epoch 00359: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.9675 - Si-sdr: 3.6316 - val_loss: 228.6835 - val_Si-sdr: 0.5750\n",
      "Epoch 360/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.9458 - Si-sdr: 3.6484\n",
      "Epoch 00360: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.9458 - Si-sdr: 3.6484 - val_loss: 227.8027 - val_Si-sdr: 0.5882\n",
      "Epoch 361/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6546 - Si-sdr: 3.6478\n",
      "Epoch 00361: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.5907 - Si-sdr: 3.6450 - val_loss: 228.1175 - val_Si-sdr: 0.5554\n",
      "Epoch 362/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9534 - Si-sdr: 3.6338\n",
      "Epoch 00362: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.9030 - Si-sdr: 3.6363 - val_loss: 228.9097 - val_Si-sdr: 0.5756\n",
      "Epoch 363/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 224.0679 - Si-sdr: 3.6424\n",
      "Epoch 00363: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.0679 - Si-sdr: 3.6424 - val_loss: 228.0343 - val_Si-sdr: 0.5870\n",
      "Epoch 364/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7216 - Si-sdr: 3.6391\n",
      "Epoch 00364: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.7375 - Si-sdr: 3.6385 - val_loss: 227.5929 - val_Si-sdr: 0.6641\n",
      "Epoch 365/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.9018 - Si-sdr: 3.6452\n",
      "Epoch 00365: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.9018 - Si-sdr: 3.6452 - val_loss: 228.2662 - val_Si-sdr: 0.5303\n",
      "Epoch 366/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0477 - Si-sdr: 3.6438\n",
      "Epoch 00366: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.9626 - Si-sdr: 3.6413 - val_loss: 228.5846 - val_Si-sdr: 0.5760\n",
      "Epoch 367/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0499 - Si-sdr: 3.6294\n",
      "Epoch 00367: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 224.0133 - Si-sdr: 3.6311 - val_loss: 228.3030 - val_Si-sdr: 0.6428\n",
      "Epoch 368/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0448 - Si-sdr: 3.6413\n",
      "Epoch 00368: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.0643 - Si-sdr: 3.6471 - val_loss: 227.4168 - val_Si-sdr: 0.6390\n",
      "Epoch 369/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.6202 - Si-sdr: 3.6481\n",
      "Epoch 00369: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.6202 - Si-sdr: 3.6481 - val_loss: 228.0635 - val_Si-sdr: 0.5376\n",
      "Epoch 370/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.3952 - Si-sdr: 3.6253\n",
      "Epoch 00370: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.4185 - Si-sdr: 3.6241 - val_loss: 227.6285 - val_Si-sdr: 0.6695\n",
      "Epoch 371/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0073 - Si-sdr: 3.6451\n",
      "Epoch 00371: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.9340 - Si-sdr: 3.6428 - val_loss: 227.4818 - val_Si-sdr: 0.5791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 372/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9114 - Si-sdr: 3.6477\n",
      "Epoch 00372: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.7930 - Si-sdr: 3.6457 - val_loss: 228.1422 - val_Si-sdr: 0.5427\n",
      "Epoch 373/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7305 - Si-sdr: 3.6588\n",
      "Epoch 00373: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.6888 - Si-sdr: 3.6555 - val_loss: 227.4891 - val_Si-sdr: 0.6126\n",
      "Epoch 374/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.1598 - Si-sdr: 3.6433\n",
      "Epoch 00374: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.1775 - Si-sdr: 3.6434 - val_loss: 227.9794 - val_Si-sdr: 0.6011\n",
      "Epoch 375/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5472 - Si-sdr: 3.6515\n",
      "Epoch 00375: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.5714 - Si-sdr: 3.6533 - val_loss: 228.4565 - val_Si-sdr: 0.5629\n",
      "Epoch 376/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.8277 - Si-sdr: 3.6565\n",
      "Epoch 00376: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.8277 - Si-sdr: 3.6565 - val_loss: 227.4114 - val_Si-sdr: 0.6031\n",
      "Epoch 377/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9416 - Si-sdr: 3.6455\n",
      "Epoch 00377: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.9960 - Si-sdr: 3.6471 - val_loss: 228.4323 - val_Si-sdr: 0.5162\n",
      "Epoch 378/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.8262 - Si-sdr: 3.6452\n",
      "Epoch 00378: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.7789 - Si-sdr: 3.6443 - val_loss: 227.6103 - val_Si-sdr: 0.6309\n",
      "Epoch 379/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.9727 - Si-sdr: 3.6352\n",
      "Epoch 00379: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.0210 - Si-sdr: 3.6379 - val_loss: 229.1422 - val_Si-sdr: 0.5433\n",
      "Epoch 380/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.3358 - Si-sdr: 3.6221\n",
      "Epoch 00380: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.3885 - Si-sdr: 3.6230 - val_loss: 228.3997 - val_Si-sdr: 0.5428\n",
      "Epoch 381/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.0005 - Si-sdr: 3.6363\n",
      "Epoch 00381: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 224.0592 - Si-sdr: 3.6364 - val_loss: 227.2826 - val_Si-sdr: 0.6191\n",
      "Epoch 382/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5358 - Si-sdr: 3.6611\n",
      "Epoch 00382: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4520 - Si-sdr: 3.6583 - val_loss: 227.7703 - val_Si-sdr: 0.5931\n",
      "Epoch 383/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3314 - Si-sdr: 3.6522\n",
      "Epoch 00383: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.2584 - Si-sdr: 3.6525 - val_loss: 227.6356 - val_Si-sdr: 0.5626\n",
      "Epoch 384/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7386 - Si-sdr: 3.6468\n",
      "Epoch 00384: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.5854 - Si-sdr: 3.6432 - val_loss: 227.5269 - val_Si-sdr: 0.6554\n",
      "Epoch 385/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6712 - Si-sdr: 3.6700\n",
      "Epoch 00385: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.5620 - Si-sdr: 3.6675 - val_loss: 228.4102 - val_Si-sdr: 0.5348\n",
      "Epoch 386/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7150 - Si-sdr: 3.6300\n",
      "Epoch 00386: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.7581 - Si-sdr: 3.6322 - val_loss: 228.8863 - val_Si-sdr: 0.5034\n",
      "Epoch 387/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.8220 - Si-sdr: 3.6335\n",
      "Epoch 00387: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.7626 - Si-sdr: 3.6325 - val_loss: 228.7374 - val_Si-sdr: 0.5212\n",
      "Epoch 388/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3410 - Si-sdr: 3.6518\n",
      "Epoch 00388: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.3456 - Si-sdr: 3.6562 - val_loss: 227.4249 - val_Si-sdr: 0.6494\n",
      "Epoch 389/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.7803 - Si-sdr: 3.6422\n",
      "Epoch 00389: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.7803 - Si-sdr: 3.6422 - val_loss: 228.5058 - val_Si-sdr: 0.5063\n",
      "Epoch 390/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7731 - Si-sdr: 3.6523\n",
      "Epoch 00390: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.6778 - Si-sdr: 3.6555 - val_loss: 229.5574 - val_Si-sdr: 0.5614\n",
      "Epoch 391/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5588 - Si-sdr: 3.6575\n",
      "Epoch 00391: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4564 - Si-sdr: 3.6581 - val_loss: 227.7814 - val_Si-sdr: 0.6230\n",
      "Epoch 392/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6656 - Si-sdr: 3.6450\n",
      "Epoch 00392: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.6750 - Si-sdr: 3.6487 - val_loss: 229.0271 - val_Si-sdr: 0.5660\n",
      "Epoch 393/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5246 - Si-sdr: 3.6596\n",
      "Epoch 00393: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3650 - Si-sdr: 3.6559 - val_loss: 228.1883 - val_Si-sdr: 0.5518\n",
      "Epoch 394/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5882 - Si-sdr: 3.6591\n",
      "Epoch 00394: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.5966 - Si-sdr: 3.6564 - val_loss: 227.5700 - val_Si-sdr: 0.6172\n",
      "Epoch 395/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5889 - Si-sdr: 3.6582\n",
      "Epoch 00395: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.4755 - Si-sdr: 3.6552 - val_loss: 228.0126 - val_Si-sdr: 0.5503\n",
      "Epoch 396/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3680 - Si-sdr: 3.6557\n",
      "Epoch 00396: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3185 - Si-sdr: 3.6546 - val_loss: 228.7112 - val_Si-sdr: 0.5428\n",
      "Epoch 397/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.5144 - Si-sdr: 3.6498\n",
      "Epoch 00397: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.5144 - Si-sdr: 3.6498 - val_loss: 228.6657 - val_Si-sdr: 0.5582\n",
      "Epoch 398/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 224.3213 - Si-sdr: 3.6297\n",
      "Epoch 00398: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 224.3577 - Si-sdr: 3.6325 - val_loss: 227.6568 - val_Si-sdr: 0.5413\n",
      "Epoch 399/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6973 - Si-sdr: 3.6457\n",
      "Epoch 00399: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.7577 - Si-sdr: 3.6494 - val_loss: 228.4750 - val_Si-sdr: 0.6078\n",
      "Epoch 400/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7758 - Si-sdr: 3.6432\n",
      "Epoch 00400: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.6959 - Si-sdr: 3.6427 - val_loss: 228.6361 - val_Si-sdr: 0.4793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 401/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1887 - Si-sdr: 3.6685\n",
      "Epoch 00401: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.1845 - Si-sdr: 3.6626 - val_loss: 227.7565 - val_Si-sdr: 0.5372\n",
      "Epoch 402/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.3121 - Si-sdr: 3.6496\n",
      "Epoch 00402: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.3121 - Si-sdr: 3.6496 - val_loss: 227.2986 - val_Si-sdr: 0.5630\n",
      "Epoch 403/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6812 - Si-sdr: 3.6397\n",
      "Epoch 00403: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.6092 - Si-sdr: 3.6389 - val_loss: 227.5990 - val_Si-sdr: 0.6324\n",
      "Epoch 404/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.0467 - Si-sdr: 3.6644\n",
      "Epoch 00404: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1068 - Si-sdr: 3.6627 - val_loss: 228.2341 - val_Si-sdr: 0.6172\n",
      "Epoch 405/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6402 - Si-sdr: 3.6627\n",
      "Epoch 00405: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.5497 - Si-sdr: 3.6630 - val_loss: 228.7565 - val_Si-sdr: 0.6059\n",
      "Epoch 406/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5746 - Si-sdr: 3.6545\n",
      "Epoch 00406: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.5702 - Si-sdr: 3.6544 - val_loss: 227.7170 - val_Si-sdr: 0.6466\n",
      "Epoch 407/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.3673 - Si-sdr: 3.6607\n",
      "Epoch 00407: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.3673 - Si-sdr: 3.6607 - val_loss: 227.8689 - val_Si-sdr: 0.5956\n",
      "Epoch 408/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5222 - Si-sdr: 3.6578\n",
      "Epoch 00408: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4097 - Si-sdr: 3.6563 - val_loss: 228.4076 - val_Si-sdr: 0.5236\n",
      "Epoch 409/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.4723 - Si-sdr: 3.6535\n",
      "Epoch 00409: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.5322 - Si-sdr: 3.6551 - val_loss: 227.6514 - val_Si-sdr: 0.5325\n",
      "Epoch 410/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6177 - Si-sdr: 3.6487\n",
      "Epoch 00410: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4680 - Si-sdr: 3.6478 - val_loss: 227.6734 - val_Si-sdr: 0.5670\n",
      "Epoch 411/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.4829 - Si-sdr: 3.6539\n",
      "Epoch 00411: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.4315 - Si-sdr: 3.6548 - val_loss: 228.6853 - val_Si-sdr: 0.6347\n",
      "Epoch 412/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7778 - Si-sdr: 3.6564\n",
      "Epoch 00412: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.6561 - Si-sdr: 3.6563 - val_loss: 228.3311 - val_Si-sdr: 0.5410\n",
      "Epoch 413/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.2556 - Si-sdr: 3.6696\n",
      "Epoch 00413: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.2556 - Si-sdr: 3.6696 - val_loss: 227.6043 - val_Si-sdr: 0.6252\n",
      "Epoch 414/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.6472 - Si-sdr: 3.6417\n",
      "Epoch 00414: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.5361 - Si-sdr: 3.6403 - val_loss: 230.0105 - val_Si-sdr: 0.4624\n",
      "Epoch 415/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9418 - Si-sdr: 3.6684\n",
      "Epoch 00415: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.0196 - Si-sdr: 3.6705 - val_loss: 228.7825 - val_Si-sdr: 0.5227\n",
      "Epoch 416/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9790 - Si-sdr: 3.6711\n",
      "Epoch 00416: val_loss did not improve from 227.24799\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.0810 - Si-sdr: 3.6715 - val_loss: 228.6375 - val_Si-sdr: 0.5738\n",
      "Epoch 417/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2714 - Si-sdr: 3.6677\n",
      "Epoch 00417: val_loss improved from 227.24799 to 226.30409, saving model to ./CKPT\\CKP_ep_417__loss_226.30409_.h5\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.1701 - Si-sdr: 3.6655 - val_loss: 226.3041 - val_Si-sdr: 0.6643\n",
      "Epoch 418/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.1320 - Si-sdr: 3.6828\n",
      "Epoch 00418: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.1320 - Si-sdr: 3.6828 - val_loss: 227.2165 - val_Si-sdr: 0.6326\n",
      "Epoch 419/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1783 - Si-sdr: 3.6696\n",
      "Epoch 00419: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.1548 - Si-sdr: 3.6695 - val_loss: 227.4533 - val_Si-sdr: 0.5902\n",
      "Epoch 420/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.3950 - Si-sdr: 3.6587\n",
      "Epoch 00420: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.3950 - Si-sdr: 3.6587 - val_loss: 226.9365 - val_Si-sdr: 0.6254\n",
      "Epoch 421/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.4626 - Si-sdr: 3.6665\n",
      "Epoch 00421: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4300 - Si-sdr: 3.6642 - val_loss: 227.9467 - val_Si-sdr: 0.5634\n",
      "Epoch 422/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1888 - Si-sdr: 3.6535\n",
      "Epoch 00422: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1689 - Si-sdr: 3.6573 - val_loss: 228.5046 - val_Si-sdr: 0.5267\n",
      "Epoch 423/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.4735 - Si-sdr: 3.6617\n",
      "Epoch 00423: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.4735 - Si-sdr: 3.6617 - val_loss: 227.6566 - val_Si-sdr: 0.5913\n",
      "Epoch 424/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1803 - Si-sdr: 3.6660\n",
      "Epoch 00424: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.0869 - Si-sdr: 3.6669 - val_loss: 228.4044 - val_Si-sdr: 0.5873\n",
      "Epoch 425/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.2174 - Si-sdr: 3.6646\n",
      "Epoch 00425: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.2174 - Si-sdr: 3.6646 - val_loss: 227.3984 - val_Si-sdr: 0.5916\n",
      "Epoch 426/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.0318 - Si-sdr: 3.6723\n",
      "Epoch 00426: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1738 - Si-sdr: 3.6742 - val_loss: 227.8653 - val_Si-sdr: 0.5567\n",
      "Epoch 427/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2851 - Si-sdr: 3.6618\n",
      "Epoch 00427: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.2651 - Si-sdr: 3.6629 - val_loss: 226.8772 - val_Si-sdr: 0.6827\n",
      "Epoch 428/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.4286 - Si-sdr: 3.6645\n",
      "Epoch 00428: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3761 - Si-sdr: 3.6623 - val_loss: 228.1320 - val_Si-sdr: 0.5876\n",
      "Epoch 429/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1574 - Si-sdr: 3.6708\n",
      "Epoch 00429: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.0001 - Si-sdr: 3.6695 - val_loss: 226.6951 - val_Si-sdr: 0.6451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 430/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8969 - Si-sdr: 3.6632\n",
      "Epoch 00430: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.9064 - Si-sdr: 3.6660 - val_loss: 227.3979 - val_Si-sdr: 0.6277\n",
      "Epoch 431/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2939 - Si-sdr: 3.6647\n",
      "Epoch 00431: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.3076 - Si-sdr: 3.6651 - val_loss: 228.5260 - val_Si-sdr: 0.5658\n",
      "Epoch 432/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.4545 - Si-sdr: 3.6590\n",
      "Epoch 00432: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3916 - Si-sdr: 3.6620 - val_loss: 228.1724 - val_Si-sdr: 0.5928\n",
      "Epoch 433/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3542 - Si-sdr: 3.6579\n",
      "Epoch 00433: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3238 - Si-sdr: 3.6595 - val_loss: 227.2954 - val_Si-sdr: 0.5733\n",
      "Epoch 434/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7535 - Si-sdr: 3.6700\n",
      "Epoch 00434: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.7535 - Si-sdr: 3.6700 - val_loss: 227.9837 - val_Si-sdr: 0.6004\n",
      "Epoch 435/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.1882 - Si-sdr: 3.6710\n",
      "Epoch 00435: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1882 - Si-sdr: 3.6710 - val_loss: 227.9002 - val_Si-sdr: 0.5999\n",
      "Epoch 436/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3904 - Si-sdr: 3.6550\n",
      "Epoch 00436: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.5065 - Si-sdr: 3.6532 - val_loss: 227.8602 - val_Si-sdr: 0.6080\n",
      "Epoch 437/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2973 - Si-sdr: 3.6718\n",
      "Epoch 00437: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.2229 - Si-sdr: 3.6708 - val_loss: 227.2990 - val_Si-sdr: 0.6022\n",
      "Epoch 438/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1117 - Si-sdr: 3.6515\n",
      "Epoch 00438: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.2722 - Si-sdr: 3.6565 - val_loss: 227.8823 - val_Si-sdr: 0.5514\n",
      "Epoch 439/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.7787 - Si-sdr: 3.6581\n",
      "Epoch 00439: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.6177 - Si-sdr: 3.6566 - val_loss: 228.1821 - val_Si-sdr: 0.5378\n",
      "Epoch 440/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.1024 - Si-sdr: 3.6788\n",
      "Epoch 00440: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.1024 - Si-sdr: 3.6788 - val_loss: 227.1034 - val_Si-sdr: 0.6765\n",
      "Epoch 441/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.1422 - Si-sdr: 3.6696\n",
      "Epoch 00441: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1422 - Si-sdr: 3.6696 - val_loss: 227.4805 - val_Si-sdr: 0.6593\n",
      "Epoch 442/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.0491 - Si-sdr: 3.6764\n",
      "Epoch 00442: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.9766 - Si-sdr: 3.6776 - val_loss: 227.1394 - val_Si-sdr: 0.6276\n",
      "Epoch 443/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1303 - Si-sdr: 3.6623\n",
      "Epoch 00443: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.1454 - Si-sdr: 3.6655 - val_loss: 228.0965 - val_Si-sdr: 0.5303\n",
      "Epoch 444/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2228 - Si-sdr: 3.6752\n",
      "Epoch 00444: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1668 - Si-sdr: 3.6702 - val_loss: 227.2877 - val_Si-sdr: 0.7065\n",
      "Epoch 445/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.1339 - Si-sdr: 3.6750\n",
      "Epoch 00445: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1339 - Si-sdr: 3.6750 - val_loss: 227.7507 - val_Si-sdr: 0.6795\n",
      "Epoch 446/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.4334 - Si-sdr: 3.6432\n",
      "Epoch 00446: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.4334 - Si-sdr: 3.6432 - val_loss: 229.0701 - val_Si-sdr: 0.5063\n",
      "Epoch 447/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.5017 - Si-sdr: 3.6655\n",
      "Epoch 00447: val_loss did not improve from 226.30409\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3987 - Si-sdr: 3.6689 - val_loss: 227.1024 - val_Si-sdr: 0.6375\n",
      "Epoch 448/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1294 - Si-sdr: 3.6773\n",
      "Epoch 00448: val_loss improved from 226.30409 to 226.29158, saving model to ./CKPT\\CKP_ep_448__loss_226.29158_.h5\n",
      "193/193 [==============================] - 45s 162ms/step - loss: 223.0605 - Si-sdr: 3.6770 - val_loss: 226.2916 - val_Si-sdr: 0.6810\n",
      "Epoch 449/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9429 - Si-sdr: 3.6804\n",
      "Epoch 00449: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.8406 - Si-sdr: 3.6787 - val_loss: 227.0785 - val_Si-sdr: 0.6367\n",
      "Epoch 450/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.6657 - Si-sdr: 3.6941\n",
      "Epoch 00450: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.6657 - Si-sdr: 3.6941 - val_loss: 227.7299 - val_Si-sdr: 0.5633\n",
      "Epoch 451/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1579 - Si-sdr: 3.6765\n",
      "Epoch 00451: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.0197 - Si-sdr: 3.6726 - val_loss: 227.5094 - val_Si-sdr: 0.5753\n",
      "Epoch 452/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7971 - Si-sdr: 3.6762\n",
      "Epoch 00452: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7971 - Si-sdr: 3.6762 - val_loss: 226.8870 - val_Si-sdr: 0.6604\n",
      "Epoch 453/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5701 - Si-sdr: 3.6837\n",
      "Epoch 00453: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7992 - Si-sdr: 3.6832 - val_loss: 227.6820 - val_Si-sdr: 0.5735\n",
      "Epoch 454/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3205 - Si-sdr: 3.6581\n",
      "Epoch 00454: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.1962 - Si-sdr: 3.6585 - val_loss: 228.3630 - val_Si-sdr: 0.5801\n",
      "Epoch 455/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.3821 - Si-sdr: 3.6570\n",
      "Epoch 00455: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.3375 - Si-sdr: 3.6593 - val_loss: 227.2469 - val_Si-sdr: 0.6711\n",
      "Epoch 456/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5798 - Si-sdr: 3.6781\n",
      "Epoch 00456: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.6050 - Si-sdr: 3.6811 - val_loss: 226.9410 - val_Si-sdr: 0.6641\n",
      "Epoch 457/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.2057 - Si-sdr: 3.6690\n",
      "Epoch 00457: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 223.1316 - Si-sdr: 3.6691 - val_loss: 227.6911 - val_Si-sdr: 0.6038\n",
      "Epoch 458/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8490 - Si-sdr: 3.6736\n",
      "Epoch 00458: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.8445 - Si-sdr: 3.6728 - val_loss: 227.7523 - val_Si-sdr: 0.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8244 - Si-sdr: 3.6721\n",
      "Epoch 00459: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7862 - Si-sdr: 3.6751 - val_loss: 228.8154 - val_Si-sdr: 0.5187\n",
      "Epoch 460/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.9602 - Si-sdr: 3.6621\n",
      "Epoch 00460: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.9602 - Si-sdr: 3.6621 - val_loss: 227.6528 - val_Si-sdr: 0.6075\n",
      "Epoch 461/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7871 - Si-sdr: 3.6739\n",
      "Epoch 00461: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7871 - Si-sdr: 3.6739 - val_loss: 226.6615 - val_Si-sdr: 0.5794\n",
      "Epoch 462/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1110 - Si-sdr: 3.6854\n",
      "Epoch 00462: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.0115 - Si-sdr: 3.6842 - val_loss: 227.3782 - val_Si-sdr: 0.5915\n",
      "Epoch 463/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8554 - Si-sdr: 3.6593\n",
      "Epoch 00463: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.9059 - Si-sdr: 3.6613 - val_loss: 227.1495 - val_Si-sdr: 0.6618\n",
      "Epoch 464/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9467 - Si-sdr: 3.6845\n",
      "Epoch 00464: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.8572 - Si-sdr: 3.6828 - val_loss: 227.9290 - val_Si-sdr: 0.5666\n",
      "Epoch 465/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.0608 - Si-sdr: 3.6767\n",
      "Epoch 00465: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 223.0608 - Si-sdr: 3.6767 - val_loss: 226.8728 - val_Si-sdr: 0.6367\n",
      "Epoch 466/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.4727 - Si-sdr: 3.6506\n",
      "Epoch 00466: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.4727 - Si-sdr: 3.6506 - val_loss: 228.5302 - val_Si-sdr: 0.5029\n",
      "Epoch 467/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.7014 - Si-sdr: 3.6824\n",
      "Epoch 00467: val_loss did not improve from 226.29158\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.7969 - Si-sdr: 3.6859 - val_loss: 227.6105 - val_Si-sdr: 0.6405\n",
      "Epoch 468/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9278 - Si-sdr: 3.6767\n",
      "Epoch 00468: val_loss improved from 226.29158 to 226.11737, saving model to ./CKPT\\CKP_ep_468__loss_226.11737_.h5\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.9951 - Si-sdr: 3.6776 - val_loss: 226.1174 - val_Si-sdr: 0.6473\n",
      "Epoch 469/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.7244 - Si-sdr: 3.6739\n",
      "Epoch 00469: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.6879 - Si-sdr: 3.6789 - val_loss: 227.0837 - val_Si-sdr: 0.6064\n",
      "Epoch 470/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6771 - Si-sdr: 3.6779\n",
      "Epoch 00470: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.8117 - Si-sdr: 3.6801 - val_loss: 226.1590 - val_Si-sdr: 0.6565\n",
      "Epoch 471/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 223.0178 - Si-sdr: 3.6669\n",
      "Epoch 00471: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 223.0178 - Si-sdr: 3.6669 - val_loss: 228.1839 - val_Si-sdr: 0.5932\n",
      "Epoch 472/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9016 - Si-sdr: 3.6631\n",
      "Epoch 00472: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.9189 - Si-sdr: 3.6647 - val_loss: 227.9037 - val_Si-sdr: 0.6357\n",
      "Epoch 473/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.0014 - Si-sdr: 3.6743\n",
      "Epoch 00473: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.9825 - Si-sdr: 3.6731 - val_loss: 227.9921 - val_Si-sdr: 0.5951\n",
      "Epoch 474/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3583 - Si-sdr: 3.6900\n",
      "Epoch 00474: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.3763 - Si-sdr: 3.6913 - val_loss: 226.6282 - val_Si-sdr: 0.6564\n",
      "Epoch 475/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9521 - Si-sdr: 3.6774\n",
      "Epoch 00475: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.8908 - Si-sdr: 3.6776 - val_loss: 227.1138 - val_Si-sdr: 0.6981\n",
      "Epoch 476/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5857 - Si-sdr: 3.6771\n",
      "Epoch 00476: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.5065 - Si-sdr: 3.6797 - val_loss: 227.1693 - val_Si-sdr: 0.6195\n",
      "Epoch 477/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.7270 - Si-sdr: 3.6820\n",
      "Epoch 00477: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.6606 - Si-sdr: 3.6797 - val_loss: 228.3938 - val_Si-sdr: 0.4961\n",
      "Epoch 478/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3125 - Si-sdr: 3.6836\n",
      "Epoch 00478: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.3584 - Si-sdr: 3.6877 - val_loss: 228.0451 - val_Si-sdr: 0.5803\n",
      "Epoch 479/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9014 - Si-sdr: 3.6793\n",
      "Epoch 00479: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.8581 - Si-sdr: 3.6800 - val_loss: 227.1295 - val_Si-sdr: 0.6139\n",
      "Epoch 480/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8754 - Si-sdr: 3.6728\n",
      "Epoch 00480: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 43s 161ms/step - loss: 222.7818 - Si-sdr: 3.6714 - val_loss: 227.8273 - val_Si-sdr: 0.6531\n",
      "Epoch 481/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1603 - Si-sdr: 3.6877\n",
      "Epoch 00481: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.2539 - Si-sdr: 3.6913 - val_loss: 227.1784 - val_Si-sdr: 0.6033\n",
      "Epoch 482/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7182 - Si-sdr: 3.6787\n",
      "Epoch 00482: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.7182 - Si-sdr: 3.6787 - val_loss: 226.7390 - val_Si-sdr: 0.6717\n",
      "Epoch 483/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8727 - Si-sdr: 3.6851\n",
      "Epoch 00483: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.7401 - Si-sdr: 3.6850 - val_loss: 227.4470 - val_Si-sdr: 0.5815\n",
      "Epoch 484/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.8929 - Si-sdr: 3.6730\n",
      "Epoch 00484: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.8929 - Si-sdr: 3.6730 - val_loss: 226.3053 - val_Si-sdr: 0.6774\n",
      "Epoch 485/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.7305 - Si-sdr: 3.6966\n",
      "Epoch 00485: val_loss did not improve from 226.11737\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.6434 - Si-sdr: 3.6961 - val_loss: 228.5048 - val_Si-sdr: 0.6056\n",
      "Epoch 486/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6567 - Si-sdr: 3.6822\n",
      "Epoch 00486: val_loss improved from 226.11737 to 225.47810, saving model to ./CKPT\\CKP_ep_486__loss_225.47810_.h5\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.6497 - Si-sdr: 3.6860 - val_loss: 225.4781 - val_Si-sdr: 0.6658\n",
      "Epoch 487/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3809 - Si-sdr: 3.6916\n",
      "Epoch 00487: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.3782 - Si-sdr: 3.6930 - val_loss: 227.3632 - val_Si-sdr: 0.5783\n",
      "Epoch 488/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4582 - Si-sdr: 3.6759\n",
      "Epoch 00488: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.5327 - Si-sdr: 3.6795 - val_loss: 227.5502 - val_Si-sdr: 0.5497\n",
      "Epoch 489/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6588 - Si-sdr: 3.6769\n",
      "Epoch 00489: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.6471 - Si-sdr: 3.6739 - val_loss: 227.8016 - val_Si-sdr: 0.5495\n",
      "Epoch 490/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6647 - Si-sdr: 3.6912\n",
      "Epoch 00490: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.6169 - Si-sdr: 3.6893 - val_loss: 227.2855 - val_Si-sdr: 0.7093\n",
      "Epoch 491/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6922 - Si-sdr: 3.6827\n",
      "Epoch 00491: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.5264 - Si-sdr: 3.6851 - val_loss: 227.0795 - val_Si-sdr: 0.6569\n",
      "Epoch 492/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7582 - Si-sdr: 3.6899\n",
      "Epoch 00492: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7582 - Si-sdr: 3.6899 - val_loss: 225.8235 - val_Si-sdr: 0.6422\n",
      "Epoch 493/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2101 - Si-sdr: 3.6944\n",
      "Epoch 00493: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.2420 - Si-sdr: 3.6947 - val_loss: 226.7359 - val_Si-sdr: 0.6303\n",
      "Epoch 494/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6751 - Si-sdr: 3.6888\n",
      "Epoch 00494: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.5757 - Si-sdr: 3.6883 - val_loss: 227.4459 - val_Si-sdr: 0.5326\n",
      "Epoch 495/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4921 - Si-sdr: 3.6966\n",
      "Epoch 00495: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 43s 161ms/step - loss: 222.4846 - Si-sdr: 3.6988 - val_loss: 228.2286 - val_Si-sdr: 0.5270\n",
      "Epoch 496/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3089 - Si-sdr: 3.6960\n",
      "Epoch 00496: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.2864 - Si-sdr: 3.6962 - val_loss: 226.1558 - val_Si-sdr: 0.6746\n",
      "Epoch 497/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5987 - Si-sdr: 3.6885\n",
      "Epoch 00497: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.5464 - Si-sdr: 3.6900 - val_loss: 227.4041 - val_Si-sdr: 0.6514\n",
      "Epoch 498/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9963 - Si-sdr: 3.6979\n",
      "Epoch 00498: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9323 - Si-sdr: 3.6999 - val_loss: 227.4236 - val_Si-sdr: 0.5938\n",
      "Epoch 499/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.9627 - Si-sdr: 3.6755\n",
      "Epoch 00499: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.8982 - Si-sdr: 3.6743 - val_loss: 226.6355 - val_Si-sdr: 0.7035\n",
      "Epoch 500/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.1382 - Si-sdr: 3.6989\n",
      "Epoch 00500: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.1382 - Si-sdr: 3.6989 - val_loss: 227.1547 - val_Si-sdr: 0.6086\n",
      "Epoch 501/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4410 - Si-sdr: 3.6884\n",
      "Epoch 00501: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.4143 - Si-sdr: 3.6873 - val_loss: 226.3656 - val_Si-sdr: 0.6480\n",
      "Epoch 502/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.8610 - Si-sdr: 3.6756\n",
      "Epoch 00502: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.7248 - Si-sdr: 3.6725 - val_loss: 226.7870 - val_Si-sdr: 0.6163\n",
      "Epoch 503/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1937 - Si-sdr: 3.6949\n",
      "Epoch 00503: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.1892 - Si-sdr: 3.6968 - val_loss: 226.8736 - val_Si-sdr: 0.6731\n",
      "Epoch 504/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9458 - Si-sdr: 3.7021\n",
      "Epoch 00504: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.0179 - Si-sdr: 3.7050 - val_loss: 227.5541 - val_Si-sdr: 0.6313\n",
      "Epoch 505/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5011 - Si-sdr: 3.6902\n",
      "Epoch 00505: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.4305 - Si-sdr: 3.6901 - val_loss: 227.3280 - val_Si-sdr: 0.6779\n",
      "Epoch 506/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4633 - Si-sdr: 3.6841\n",
      "Epoch 00506: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.4972 - Si-sdr: 3.6840 - val_loss: 227.0048 - val_Si-sdr: 0.6426\n",
      "Epoch 507/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3826 - Si-sdr: 3.7045\n",
      "Epoch 00507: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.3644 - Si-sdr: 3.7042 - val_loss: 227.7315 - val_Si-sdr: 0.5701\n",
      "Epoch 508/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3387 - Si-sdr: 3.6852\n",
      "Epoch 00508: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.4793 - Si-sdr: 3.6912 - val_loss: 228.2006 - val_Si-sdr: 0.5855\n",
      "Epoch 509/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 223.1689 - Si-sdr: 3.6780\n",
      "Epoch 00509: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 223.0383 - Si-sdr: 3.6745 - val_loss: 228.0795 - val_Si-sdr: 0.5957\n",
      "Epoch 510/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9588 - Si-sdr: 3.6957\n",
      "Epoch 00510: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.0879 - Si-sdr: 3.6967 - val_loss: 227.6686 - val_Si-sdr: 0.6348\n",
      "Epoch 511/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.7077 - Si-sdr: 3.6802\n",
      "Epoch 00511: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.7077 - Si-sdr: 3.6802 - val_loss: 227.9006 - val_Si-sdr: 0.6102\n",
      "Epoch 512/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3960 - Si-sdr: 3.6908\n",
      "Epoch 00512: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.4085 - Si-sdr: 3.6910 - val_loss: 226.7154 - val_Si-sdr: 0.6114\n",
      "Epoch 513/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8362 - Si-sdr: 3.6956\n",
      "Epoch 00513: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.9362 - Si-sdr: 3.6972 - val_loss: 227.4448 - val_Si-sdr: 0.6219\n",
      "Epoch 514/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3995 - Si-sdr: 3.7015\n",
      "Epoch 00514: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.4223 - Si-sdr: 3.7006 - val_loss: 227.8137 - val_Si-sdr: 0.5921\n",
      "Epoch 515/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6547 - Si-sdr: 3.6792\n",
      "Epoch 00515: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.5805 - Si-sdr: 3.6748 - val_loss: 227.6288 - val_Si-sdr: 0.6018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5813 - Si-sdr: 3.6838\n",
      "Epoch 00516: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.4688 - Si-sdr: 3.6824 - val_loss: 226.5965 - val_Si-sdr: 0.6616\n",
      "Epoch 517/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.6133 - Si-sdr: 3.6878\n",
      "Epoch 00517: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.5553 - Si-sdr: 3.6872 - val_loss: 226.2166 - val_Si-sdr: 0.7024\n",
      "Epoch 518/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5023 - Si-sdr: 3.6877\n",
      "Epoch 00518: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.4747 - Si-sdr: 3.6897 - val_loss: 227.2311 - val_Si-sdr: 0.6682\n",
      "Epoch 519/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2663 - Si-sdr: 3.7075\n",
      "Epoch 00519: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.2498 - Si-sdr: 3.7023 - val_loss: 227.5605 - val_Si-sdr: 0.6604\n",
      "Epoch 520/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1724 - Si-sdr: 3.6937\n",
      "Epoch 00520: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3098 - Si-sdr: 3.6946 - val_loss: 227.9666 - val_Si-sdr: 0.5434\n",
      "Epoch 521/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4527 - Si-sdr: 3.6868\n",
      "Epoch 00521: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3882 - Si-sdr: 3.6839 - val_loss: 226.0443 - val_Si-sdr: 0.6595\n",
      "Epoch 522/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.6183 - Si-sdr: 3.7098\n",
      "Epoch 00522: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 221.6743 - Si-sdr: 3.7088 - val_loss: 225.7580 - val_Si-sdr: 0.6805\n",
      "Epoch 523/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2942 - Si-sdr: 3.7027\n",
      "Epoch 00523: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.2245 - Si-sdr: 3.7050 - val_loss: 226.6168 - val_Si-sdr: 0.6817\n",
      "Epoch 524/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1537 - Si-sdr: 3.7027\n",
      "Epoch 00524: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.0800 - Si-sdr: 3.7037 - val_loss: 226.9980 - val_Si-sdr: 0.5636\n",
      "Epoch 525/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1775 - Si-sdr: 3.6909\n",
      "Epoch 00525: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.1684 - Si-sdr: 3.6895 - val_loss: 226.5160 - val_Si-sdr: 0.6360\n",
      "Epoch 526/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2878 - Si-sdr: 3.7029\n",
      "Epoch 00526: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.2909 - Si-sdr: 3.7022 - val_loss: 227.8966 - val_Si-sdr: 0.6229\n",
      "Epoch 527/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.5882 - Si-sdr: 3.6842\n",
      "Epoch 00527: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.5269 - Si-sdr: 3.6844 - val_loss: 227.6184 - val_Si-sdr: 0.6448\n",
      "Epoch 528/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8628 - Si-sdr: 3.6978\n",
      "Epoch 00528: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.9163 - Si-sdr: 3.7005 - val_loss: 227.4678 - val_Si-sdr: 0.6422\n",
      "Epoch 529/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7032 - Si-sdr: 3.7188\n",
      "Epoch 00529: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.6270 - Si-sdr: 3.7175 - val_loss: 227.9823 - val_Si-sdr: 0.5558\n",
      "Epoch 530/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4427 - Si-sdr: 3.6972\n",
      "Epoch 00530: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.3671 - Si-sdr: 3.6924 - val_loss: 227.5141 - val_Si-sdr: 0.6229\n",
      "Epoch 531/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3887 - Si-sdr: 3.6843\n",
      "Epoch 00531: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3642 - Si-sdr: 3.6898 - val_loss: 226.2159 - val_Si-sdr: 0.6366\n",
      "Epoch 532/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2873 - Si-sdr: 3.6831\n",
      "Epoch 00532: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.3417 - Si-sdr: 3.6836 - val_loss: 226.7699 - val_Si-sdr: 0.6240\n",
      "Epoch 533/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2741 - Si-sdr: 3.6924\n",
      "Epoch 00533: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.1703 - Si-sdr: 3.6907 - val_loss: 227.0943 - val_Si-sdr: 0.6164\n",
      "Epoch 534/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4200 - Si-sdr: 3.6866\n",
      "Epoch 00534: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3370 - Si-sdr: 3.6839 - val_loss: 225.8344 - val_Si-sdr: 0.6545\n",
      "Epoch 535/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.6462 - Si-sdr: 3.7123\n",
      "Epoch 00535: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.6160 - Si-sdr: 3.7093 - val_loss: 227.0526 - val_Si-sdr: 0.6592\n",
      "Epoch 536/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.9883 - Si-sdr: 3.6995\n",
      "Epoch 00536: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9883 - Si-sdr: 3.6995 - val_loss: 227.1298 - val_Si-sdr: 0.6341\n",
      "Epoch 537/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1323 - Si-sdr: 3.7015\n",
      "Epoch 00537: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.1466 - Si-sdr: 3.7044 - val_loss: 227.1785 - val_Si-sdr: 0.6375\n",
      "Epoch 538/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.0153 - Si-sdr: 3.7049\n",
      "Epoch 00538: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9721 - Si-sdr: 3.7025 - val_loss: 226.9673 - val_Si-sdr: 0.6527\n",
      "Epoch 539/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9339 - Si-sdr: 3.6974\n",
      "Epoch 00539: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.9207 - Si-sdr: 3.7014 - val_loss: 227.4054 - val_Si-sdr: 0.5657\n",
      "Epoch 540/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.8067 - Si-sdr: 3.7101\n",
      "Epoch 00540: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.8067 - Si-sdr: 3.7101 - val_loss: 226.5561 - val_Si-sdr: 0.5804\n",
      "Epoch 541/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9872 - Si-sdr: 3.7107\n",
      "Epoch 00541: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.0873 - Si-sdr: 3.7103 - val_loss: 226.0228 - val_Si-sdr: 0.6263\n",
      "Epoch 542/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2496 - Si-sdr: 3.7122\n",
      "Epoch 00542: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.1032 - Si-sdr: 3.7088 - val_loss: 227.6587 - val_Si-sdr: 0.6116\n",
      "Epoch 543/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7590 - Si-sdr: 3.7040\n",
      "Epoch 00543: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 43s 160ms/step - loss: 221.7551 - Si-sdr: 3.7066 - val_loss: 226.2378 - val_Si-sdr: 0.6574\n",
      "Epoch 544/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1045 - Si-sdr: 3.7016\n",
      "Epoch 00544: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 43s 160ms/step - loss: 222.0410 - Si-sdr: 3.7029 - val_loss: 226.9648 - val_Si-sdr: 0.6580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 545/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.6013 - Si-sdr: 3.7148\n",
      "Epoch 00545: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.6013 - Si-sdr: 3.7148 - val_loss: 226.4340 - val_Si-sdr: 0.6538\n",
      "Epoch 546/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.0207 - Si-sdr: 3.6982\n",
      "Epoch 00546: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.8725 - Si-sdr: 3.6958 - val_loss: 227.8376 - val_Si-sdr: 0.4934\n",
      "Epoch 547/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9624 - Si-sdr: 3.7036\n",
      "Epoch 00547: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.8866 - Si-sdr: 3.7070 - val_loss: 226.0787 - val_Si-sdr: 0.6655\n",
      "Epoch 548/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.0774 - Si-sdr: 3.6983\n",
      "Epoch 00548: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.0774 - Si-sdr: 3.6983 - val_loss: 226.4095 - val_Si-sdr: 0.6947\n",
      "Epoch 549/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4421 - Si-sdr: 3.6866\n",
      "Epoch 00549: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.2898 - Si-sdr: 3.6865 - val_loss: 227.3425 - val_Si-sdr: 0.6133\n",
      "Epoch 550/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.4690 - Si-sdr: 3.6909\n",
      "Epoch 00550: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.4682 - Si-sdr: 3.6870 - val_loss: 226.6198 - val_Si-sdr: 0.6753\n",
      "Epoch 551/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2790 - Si-sdr: 3.6960\n",
      "Epoch 00551: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3048 - Si-sdr: 3.6974 - val_loss: 225.9987 - val_Si-sdr: 0.6527\n",
      "Epoch 552/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9922 - Si-sdr: 3.6996\n",
      "Epoch 00552: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9893 - Si-sdr: 3.7023 - val_loss: 227.5787 - val_Si-sdr: 0.6626\n",
      "Epoch 553/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8668 - Si-sdr: 3.7110\n",
      "Epoch 00553: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9907 - Si-sdr: 3.7142 - val_loss: 226.8922 - val_Si-sdr: 0.5984\n",
      "Epoch 554/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2732 - Si-sdr: 3.6969\n",
      "Epoch 00554: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 43s 161ms/step - loss: 222.2287 - Si-sdr: 3.6987 - val_loss: 227.7638 - val_Si-sdr: 0.5513\n",
      "Epoch 555/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8176 - Si-sdr: 3.7126\n",
      "Epoch 00555: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.7050 - Si-sdr: 3.7101 - val_loss: 227.8715 - val_Si-sdr: 0.5966\n",
      "Epoch 556/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7471 - Si-sdr: 3.7078\n",
      "Epoch 00556: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.7470 - Si-sdr: 3.7065 - val_loss: 226.6605 - val_Si-sdr: 0.6530\n",
      "Epoch 557/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2073 - Si-sdr: 3.6982\n",
      "Epoch 00557: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.1275 - Si-sdr: 3.6949 - val_loss: 227.3852 - val_Si-sdr: 0.5958\n",
      "Epoch 558/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.8921 - Si-sdr: 3.7169\n",
      "Epoch 00558: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.8921 - Si-sdr: 3.7169 - val_loss: 225.9932 - val_Si-sdr: 0.6500\n",
      "Epoch 559/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.5883 - Si-sdr: 3.7138\n",
      "Epoch 00559: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.5883 - Si-sdr: 3.7138 - val_loss: 226.7712 - val_Si-sdr: 0.7139\n",
      "Epoch 560/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.0264 - Si-sdr: 3.7081\n",
      "Epoch 00560: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9612 - Si-sdr: 3.7066 - val_loss: 226.1394 - val_Si-sdr: 0.6863\n",
      "Epoch 561/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9208 - Si-sdr: 3.7180\n",
      "Epoch 00561: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.9008 - Si-sdr: 3.7220 - val_loss: 226.4323 - val_Si-sdr: 0.6765\n",
      "Epoch 562/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 222.0055 - Si-sdr: 3.6902\n",
      "Epoch 00562: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.0055 - Si-sdr: 3.6902 - val_loss: 227.0595 - val_Si-sdr: 0.6272\n",
      "Epoch 563/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1671 - Si-sdr: 3.6980\n",
      "Epoch 00563: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.2253 - Si-sdr: 3.7001 - val_loss: 226.0969 - val_Si-sdr: 0.7145\n",
      "Epoch 564/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9672 - Si-sdr: 3.7128\n",
      "Epoch 00564: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9039 - Si-sdr: 3.7067 - val_loss: 226.1231 - val_Si-sdr: 0.7071\n",
      "Epoch 565/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1518 - Si-sdr: 3.6894\n",
      "Epoch 00565: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.2050 - Si-sdr: 3.6912 - val_loss: 227.7736 - val_Si-sdr: 0.5680\n",
      "Epoch 566/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3810 - Si-sdr: 3.7037\n",
      "Epoch 00566: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 222.3210 - Si-sdr: 3.7029 - val_loss: 226.7056 - val_Si-sdr: 0.6610\n",
      "Epoch 567/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9858 - Si-sdr: 3.7066\n",
      "Epoch 00567: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.9409 - Si-sdr: 3.7079 - val_loss: 226.8214 - val_Si-sdr: 0.5944\n",
      "Epoch 568/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2031 - Si-sdr: 3.7131\n",
      "Epoch 00568: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.1134 - Si-sdr: 3.7128 - val_loss: 226.6410 - val_Si-sdr: 0.6318\n",
      "Epoch 569/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.5876 - Si-sdr: 3.7117\n",
      "Epoch 00569: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.6059 - Si-sdr: 3.7153 - val_loss: 227.1008 - val_Si-sdr: 0.6590\n",
      "Epoch 570/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.3797 - Si-sdr: 3.7042\n",
      "Epoch 00570: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 222.3544 - Si-sdr: 3.7009 - val_loss: 226.1167 - val_Si-sdr: 0.6992\n",
      "Epoch 571/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8910 - Si-sdr: 3.7183\n",
      "Epoch 00571: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.8013 - Si-sdr: 3.7155 - val_loss: 225.5916 - val_Si-sdr: 0.6041\n",
      "Epoch 572/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.5843 - Si-sdr: 3.7102\n",
      "Epoch 00572: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.4713 - Si-sdr: 3.7089 - val_loss: 225.8927 - val_Si-sdr: 0.6553\n",
      "Epoch 573/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8328 - Si-sdr: 3.7128\n",
      "Epoch 00573: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.8106 - Si-sdr: 3.7135 - val_loss: 225.4854 - val_Si-sdr: 0.7201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 574/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2201 - Si-sdr: 3.7014\n",
      "Epoch 00574: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 222.1210 - Si-sdr: 3.7006 - val_loss: 226.7151 - val_Si-sdr: 0.6734\n",
      "Epoch 575/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.5916 - Si-sdr: 3.7226\n",
      "Epoch 00575: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.5916 - Si-sdr: 3.7226 - val_loss: 226.7723 - val_Si-sdr: 0.6230\n",
      "Epoch 576/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.4703 - Si-sdr: 3.7132\n",
      "Epoch 00576: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.5535 - Si-sdr: 3.7167 - val_loss: 226.7544 - val_Si-sdr: 0.6291\n",
      "Epoch 577/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1826 - Si-sdr: 3.7029\n",
      "Epoch 00577: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.1189 - Si-sdr: 3.7022 - val_loss: 226.8967 - val_Si-sdr: 0.6158\n",
      "Epoch 578/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8235 - Si-sdr: 3.7155\n",
      "Epoch 00578: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.8509 - Si-sdr: 3.7173 - val_loss: 225.5572 - val_Si-sdr: 0.6959\n",
      "Epoch 579/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.5712 - Si-sdr: 3.7184\n",
      "Epoch 00579: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.6610 - Si-sdr: 3.7176 - val_loss: 225.8515 - val_Si-sdr: 0.6792\n",
      "Epoch 580/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7197 - Si-sdr: 3.7264\n",
      "Epoch 00580: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.6901 - Si-sdr: 3.7270 - val_loss: 226.5146 - val_Si-sdr: 0.6618\n",
      "Epoch 581/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.9144 - Si-sdr: 3.6986\n",
      "Epoch 00581: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.8656 - Si-sdr: 3.6963 - val_loss: 226.4532 - val_Si-sdr: 0.6528\n",
      "Epoch 582/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.4683 - Si-sdr: 3.7324\n",
      "Epoch 00582: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.4597 - Si-sdr: 3.7296 - val_loss: 227.3931 - val_Si-sdr: 0.6337\n",
      "Epoch 583/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.5000 - Si-sdr: 3.7259\n",
      "Epoch 00583: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.4109 - Si-sdr: 3.7247 - val_loss: 227.4627 - val_Si-sdr: 0.6436\n",
      "Epoch 584/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.3602 - Si-sdr: 3.7221\n",
      "Epoch 00584: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.3602 - Si-sdr: 3.7221 - val_loss: 225.9807 - val_Si-sdr: 0.7105\n",
      "Epoch 585/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.6441 - Si-sdr: 3.7226\n",
      "Epoch 00585: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.6781 - Si-sdr: 3.7190 - val_loss: 225.8441 - val_Si-sdr: 0.7213\n",
      "Epoch 586/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7372 - Si-sdr: 3.7142\n",
      "Epoch 00586: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.6246 - Si-sdr: 3.7135 - val_loss: 226.3846 - val_Si-sdr: 0.6533\n",
      "Epoch 587/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.0092 - Si-sdr: 3.7151\n",
      "Epoch 00587: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.8697 - Si-sdr: 3.7128 - val_loss: 226.4910 - val_Si-sdr: 0.7115\n",
      "Epoch 588/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.1202 - Si-sdr: 3.7295\n",
      "Epoch 00588: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 164ms/step - loss: 221.1429 - Si-sdr: 3.7324 - val_loss: 227.0050 - val_Si-sdr: 0.5870\n",
      "Epoch 589/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.3379 - Si-sdr: 3.7225\n",
      "Epoch 00589: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.3379 - Si-sdr: 3.7225 - val_loss: 227.7020 - val_Si-sdr: 0.5927\n",
      "Epoch 590/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.4247 - Si-sdr: 3.7209\n",
      "Epoch 00590: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.4247 - Si-sdr: 3.7209 - val_loss: 225.8107 - val_Si-sdr: 0.6496\n",
      "Epoch 591/600\n",
      "193/193 [==============================] - ETA: 0s - loss: 221.8209 - Si-sdr: 3.7082\n",
      "Epoch 00591: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 43s 160ms/step - loss: 221.8209 - Si-sdr: 3.7082 - val_loss: 226.5087 - val_Si-sdr: 0.6574\n",
      "Epoch 592/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.5465 - Si-sdr: 3.7084\n",
      "Epoch 00592: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.6092 - Si-sdr: 3.7130 - val_loss: 227.5701 - val_Si-sdr: 0.6406\n",
      "Epoch 593/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.4578 - Si-sdr: 3.7356\n",
      "Epoch 00593: val_loss did not improve from 225.47810\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.4065 - Si-sdr: 3.7307 - val_loss: 227.1926 - val_Si-sdr: 0.6568\n",
      "Epoch 594/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.4478 - Si-sdr: 3.7193\n",
      "Epoch 00594: val_loss improved from 225.47810 to 225.11702, saving model to ./CKPT\\CKP_ep_594__loss_225.11702_.h5\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.4515 - Si-sdr: 3.7187 - val_loss: 225.1170 - val_Si-sdr: 0.7527\n",
      "Epoch 595/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.6354 - Si-sdr: 3.6999\n",
      "Epoch 00595: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 221.7017 - Si-sdr: 3.7045 - val_loss: 227.3887 - val_Si-sdr: 0.6207\n",
      "Epoch 596/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.2683 - Si-sdr: 3.6908\n",
      "Epoch 00596: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 43s 161ms/step - loss: 222.1794 - Si-sdr: 3.6925 - val_loss: 227.2086 - val_Si-sdr: 0.6489\n",
      "Epoch 597/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.7545 - Si-sdr: 3.7161\n",
      "Epoch 00597: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 44s 161ms/step - loss: 221.7180 - Si-sdr: 3.7160 - val_loss: 227.5873 - val_Si-sdr: 0.5779\n",
      "Epoch 598/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.8267 - Si-sdr: 3.7164\n",
      "Epoch 00598: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.7442 - Si-sdr: 3.7189 - val_loss: 227.2274 - val_Si-sdr: 0.6652\n",
      "Epoch 599/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 222.1234 - Si-sdr: 3.7135\n",
      "Epoch 00599: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 44s 162ms/step - loss: 222.0374 - Si-sdr: 3.7094 - val_loss: 226.7304 - val_Si-sdr: 0.6623\n",
      "Epoch 600/600\n",
      "192/193 [============================>.] - ETA: 0s - loss: 221.4635 - Si-sdr: 3.7186\n",
      "Epoch 00600: val_loss did not improve from 225.11702\n",
      "193/193 [==============================] - 44s 163ms/step - loss: 221.5575 - Si-sdr: 3.7230 - val_loss: 226.3455 - val_Si-sdr: 0.6874\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "batch_size = 10\n",
    "latent_size = 1024\n",
    "epoch = 600\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['/gpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_594__loss_229.89435_.h5'\n",
    "    \n",
    "    loss_fun = custom_mse\n",
    "#     loss_fun = custom_sisdr_loss\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    vq_vae.compile(optimizer, loss=loss_fun, metrics=[SiSdr()])\n",
    "    \n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "    vq_vae.load_weights(model_path)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    tf.executing_eagerly()\n",
    "\n",
    "    \n",
    "train_data = tf.data.Dataset.from_generator(gen_train_data_generator, output_signature=(\n",
    "                                            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "                                            tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "train_data = train_data.shuffle(train_dataset.__len__()).padded_batch(batch_size)\n",
    "\n",
    "val_data = tf.data.Dataset.from_generator(gen_valid_data_generator, output_signature=(\n",
    "                                            tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "                                            tf.TensorSpec(shape=(None, 1), dtype=tf.float32)))\n",
    "val_data = val_data.padded_batch(batch_size)\n",
    "\n",
    "# Disable AutoShard.\n",
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "train_data = train_data.with_options(options)\n",
    "val_data = val_data.with_options(options)\n",
    "    \n",
    "history = vq_vae.fit(\n",
    "    train_data,\n",
    "    epochs=epoch,\n",
    "    validation_data=val_data,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb],\n",
    ")\n",
    "\n",
    "# history = vq_vae.fit_generator(\n",
    "#     generator=train_dataset,\n",
    "#     validation_data=valid_dataset,\n",
    "#     epochs=epoch,\n",
    "#     use_multiprocessing=False,\n",
    "#     shuffle=True,\n",
    "#     callbacks=[checkpoint_cb],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-overhead",
   "metadata": {},
   "source": [
    "## 2.2. Encoder 부르는 방법, Decoder에 값 넣는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "annoying-architect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_25 (Softmax)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 512)         517248    \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           516737    \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma (None, None, 512)         0         \n",
      "=================================================================\n",
      "Total params: 1,033,985\n",
      "Trainable params: 1,033,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_size = 512\n",
    "epoch = 200\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['cpu:0'])\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    model_path = './CKPT/CKP_ep_283__loss_141.77045_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, gumbel_hard=False)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    \n",
    "    vq_vae.load_weights(model_path)\n",
    "    \n",
    "    # 이렇게 하면, transforer의 input으로 들어가는 one-hot 형식의 값을 얻을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "    \n",
    "    # 이렇게 하면, transformer의 output을 vq-vae의 decoder 입력으로 넣을 수 있음\n",
    "    for inputs, label in train_dataset:\n",
    "        encode = vq_vae.encoder(inputs).numpy()\n",
    "        encode_onehot = tf.cast(tf.equal(encode, tf.math.reduce_max(encode, 2, keepdims=True)), encode.dtype)\n",
    "        \n",
    "        # 이렇게 이전 layer의 출렫을 넣으면 됨\n",
    "        decode = vq_vae.decoder(encode_onehot).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf7a69",
   "metadata": {},
   "source": [
    "# 3. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c06eb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create .//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e73b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64be7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7cc186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vqvae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "softmax_3 (Softmax)          (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "encoder (Encoder)            (None, None, 1024)        1263776   \n",
      "_________________________________________________________________\n",
      "gumbel_softmax (GumbelSoftma multiple                  0 (unused)\n",
      "_________________________________________________________________\n",
      "einsum_dense_3 (EinsumDense) (None, None, 1024)        1049600   \n",
      "_________________________________________________________________\n",
      "decoder (Decoder)            (None, None, 1)           2835521   \n",
      "=================================================================\n",
      "Total params: 5,148,897\n",
      "Trainable params: 5,148,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028397106CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028397106CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    latent_size = 1024\n",
    "    sample_rate = 8000\n",
    "    model_path = './CKPT/CKP_ep_293__loss_49.28763_.h5'\n",
    "    \n",
    "    vq_vae = Vq_vae(latent_size, for_predict=True)\n",
    "    vq_vae(0, True)\n",
    "    vq_vae.summary()\n",
    "    vq_vae.load_weights(model_path)\n",
    "\n",
    "    for batch in test_dataset:\n",
    "        input_batch, length_batch, name = batch\n",
    "\n",
    "        result = vq_vae.predict(input_batch)\n",
    "        \n",
    "        wav_name = './test_wav/' + name[0][:-5] + '_s1.wav'\n",
    "        audiowrite(result[0], wav_name, sample_rate, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7745f214",
   "metadata": {},
   "source": [
    "# 여기 밑에는 연습장임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "temporal-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(None, 1)))\n",
    "model.add(layers.Conv1D(filters=512, kernel_size=3, padding='same'))\n",
    "\n",
    "input_array = np.random.randn(2, 10, 1)\n",
    "input_array2 = np.random.randn(2, 9, 1)\n",
    "with tf.device('/cpu:0'):\n",
    "    model.compile('rmsprop', 'mse')\n",
    "\n",
    "    output_array = model.predict(input_array)\n",
    "    output_array2 = model.predict(input_array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "baking-highlight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3.5056162, 3.214589 ], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(output_array, axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "separate-circular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.05127785,  0.04667316,  0.02635628, ...,  0.04573939,\n",
       "          0.01375185, -0.01533533],\n",
       "        [ 0.01121127,  0.06675138,  0.0021627 , ...,  0.07490488,\n",
       "          0.07613068,  0.02180689]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.08263874,  0.03258345,  0.01568549, ...,  0.03589562,\n",
       "         -0.01625197, -0.03177397],\n",
       "        [ 0.13691738,  0.09397386, -0.01036285, ..., -0.00614451,\n",
       "          0.05317175, -0.01284541]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07028106,  0.00527185,  0.01043324, ...,  0.0617263 ,\n",
       "         -0.03244079, -0.01786789],\n",
       "        [ 0.10520705,  0.042405  ,  0.03726472, ...,  0.07816655,\n",
       "          0.03663039, -0.03122897]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07235897,  0.00636261, -0.03309729, ...,  0.05743181,\n",
       "         -0.01674554, -0.01104611],\n",
       "        [ 0.11989237,  0.04727669,  0.04978402, ...,  0.10254725,\n",
       "         -0.0322747 , -0.04379546]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.06034716,  0.02196031, -0.03286755, ...,  0.04720719,\n",
       "          0.02011653,  0.01457734],\n",
       "        [ 0.08640987,  0.05295723, -0.05331329, ...,  0.05878888,\n",
       "         -0.11385743, -0.11239739]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.03044085,  0.01922614, -0.01506157, ...,  0.0429484 ,\n",
       "          0.03525994,  0.01746666],\n",
       "        [ 0.1105442 ,  0.06415009, -0.11616933, ...,  0.12055975,\n",
       "         -0.0501071 , -0.02813609]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 5.68038262e-02,  5.34826368e-02,  1.02059479e-04, ...,\n",
       "          2.20218338e-02,  5.25824353e-02,  1.43096512e-02],\n",
       "        [ 1.03822105e-01,  5.07594347e-02, -1.07106149e-01, ...,\n",
       "          1.95540398e-01,  1.40878245e-01,  3.50491852e-02]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[-0.01148595,  0.03309207, -0.02289026, ...,  0.02078854,\n",
       "         -0.06813245, -0.09970599],\n",
       "        [ 0.04621019,  0.06871783,  0.00798892, ...,  0.01603857,\n",
       "          0.04825678,  0.01381727]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.07415138,  0.0991153 , -0.03032039, ...,  0.01240078,\n",
       "         -0.04231954, -0.04340101],\n",
       "        [ 0.0583101 ,  0.0205835 , -0.01229584, ..., -0.01952541,\n",
       "          0.00030562, -0.05487346]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
       " array([[ 0.02075784,  0.00618587, -0.07708566, ...,  0.10809869,\n",
       "          0.01363952,  0.02574195],\n",
       "        [-0.00304264, -0.00650905, -0.00512165, ...,  0.01797911,\n",
       "         -0.00719647, -0.02386911]], dtype=float32)>]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.unstack(output_array, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ba32a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.05127785  0.04667316  0.02635628 ...  0.04573939  0.01375185\n",
      "   -0.01533533]\n",
      "  [ 0.08263874  0.03258345  0.01568549 ...  0.03589562 -0.01625197\n",
      "   -0.03177397]\n",
      "  [ 0.07028106  0.00527185  0.01043324 ...  0.0617263  -0.03244079\n",
      "   -0.01786789]\n",
      "  ...\n",
      "  [-0.01148595  0.03309207 -0.02289026 ...  0.02078854 -0.06813245\n",
      "   -0.09970599]\n",
      "  [ 0.07415138  0.0991153  -0.03032039 ...  0.01240078 -0.04231954\n",
      "   -0.04340101]\n",
      "  [ 0.02075784  0.00618587 -0.07708566 ...  0.10809869  0.01363952\n",
      "    0.02574195]]\n",
      "\n",
      " [[ 0.01121127  0.06675138  0.0021627  ...  0.07490488  0.07613068\n",
      "    0.02180689]\n",
      "  [ 0.13691738  0.09397386 -0.01036285 ... -0.00614451  0.05317175\n",
      "   -0.01284541]\n",
      "  [ 0.10520705  0.042405    0.03726472 ...  0.07816655  0.03663039\n",
      "   -0.03122897]\n",
      "  ...\n",
      "  [ 0.04621019  0.06871783  0.00798892 ...  0.01603857  0.04825678\n",
      "    0.01381727]\n",
      "  [ 0.0583101   0.0205835  -0.01229584 ... -0.01952541  0.00030562\n",
      "   -0.05487346]\n",
      "  [-0.00304264 -0.00650905 -0.00512165 ...  0.01797911 -0.00719647\n",
      "   -0.02386911]]]\n",
      "(2, 10, 512)\n",
      "(2, 9, 512)\n"
     ]
    }
   ],
   "source": [
    "print(output_array)\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "backed-dispatch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[259 259  34  50   0 110   0 305 200 287]\n",
      " [257   0 200 259  34 200 509 110 200 287]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype), axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)\n",
    "# layers.Embedding(512, 512)(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "liable-coupon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10)\n",
      "tf.Tensor(\n",
      "[[228 249 283 206  20 206 435  32 270  30]\n",
      " [428 206  20 244 357 289 324 249 498 134]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "one_hot = tf.math.argmax(output_array, axis=-1)\n",
    "print(one_hot.shape)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bacterial-trading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.012074914, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "qy = tf.nn.softmax(output_array)\n",
    "log_qy = tf.math.log(qy + 1e-10)\n",
    "log_uniform = qy * (log_qy - tf.math.log(1.0 / 512))\n",
    "kl_loss = tf.reduce_sum(log_uniform, axis=[1, 2])\n",
    "kl_loss = tf.reduce_mean(kl_loss)\n",
    "\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc8e699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 9, 1)\n",
      "(2, 9, 1)\n",
      "tf.Tensor(2.8309882, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(output_array)[0]\n",
    "array1_size = tf.shape(output_array)[1]\n",
    "array2_size = tf.shape(output_array2)[1]\n",
    "feature_size = tf.shape(output_array)[-1]\n",
    "\n",
    "if array1_size < array2_size:\n",
    "#     append_size = array1_size - array2_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array2 = tf.concat([output_array2, append_zeros], axis=1)\n",
    "    output_array2 = tf.slice(output_array2, [0, 0, 0], [-1, array1_size, -1])\n",
    "elif array1_size > array2_size:\n",
    "#     append_size = array2_size - array1_size\n",
    "#     append_zeros = tf.zeros([batch_size, append_size, feature_size])\n",
    "#     append_zeros = tf.Variable(initial_value=tf.zeros((batch_size, append_size, feature_size)))\n",
    "#     output_array = tf.concat([output_array, append_zeros], axis=1)\n",
    "    output_array = tf.slice(output_array, [0, 0, 0], [-1, array2_size, -1])\n",
    "\n",
    "print(output_array.shape)\n",
    "print(output_array2.shape)\n",
    "# output_array0 = output_array[1]\n",
    "# output_array20 = output_array2[1]\n",
    "# target = np.sum(output_array20 * output_array0) * output_array0 / np.square(np.linalg.norm(output_array0, ord=2))\n",
    "# noise = output_array20 - target\n",
    "# npnp = 10 * np.log10(np.square(np.linalg.norm(target, ord=2)) / np.square(np.linalg.norm(noise, ord=2)))\n",
    "# print(npnp)\n",
    "\n",
    "target = tf.linalg.matmul(output_array2, output_array, transpose_a=True) * output_array / tf.expand_dims(tf.experimental.numpy.square(tf.norm(output_array, axis=1)), axis=-1)\n",
    "noise = output_array2 - target\n",
    "si_sdr = 10 * tf.experimental.numpy.log10(tf.experimental.numpy.square(tf.norm(target, axis=1)) / tf.experimental.numpy.square(tf.norm(noise, axis=1)))\n",
    "si_sdr = tf.reduce_mean(si_sdr)\n",
    "print(si_sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "clean-drink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.cast(tf.equal(output_array, tf.math.reduce_max(output_array, 2, keepdims=True)), output_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "153f9e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 9, 4), dtype=float32, numpy=\n",
       "array([[[-0.03009652, -0.03612775, -0.06680483, -0.03670201],\n",
       "        [-0.04768711, -0.12344762, -0.03924457, -0.11762322],\n",
       "        [ 0.01808495, -0.16106637, -0.19467078, -0.15282159],\n",
       "        [-0.0986427 , -0.08625205, -0.12661007, -0.16366175],\n",
       "        [-0.09758376, -0.08886974, -0.0433558 , -0.19985165],\n",
       "        [-0.06933096, -0.03154394, -0.13725929, -0.20143284],\n",
       "        [ 0.03375649,  0.00182091, -0.01022564, -0.35924646],\n",
       "        [-0.01645333, -0.10466891, -0.13975918, -0.12066491],\n",
       "        [-0.13588801, -0.08173112, -0.00253745, -0.28615874]],\n",
       "\n",
       "       [[ 0.04865369, -0.02880372, -0.06414615, -0.07730438],\n",
       "        [-0.08225074, -0.03192509, -0.06216412, -0.08035193],\n",
       "        [-0.09515338,  0.04221668,  0.14230826, -0.23082384],\n",
       "        [-0.00094383,  0.05597762, -0.09290768, -0.08630683],\n",
       "        [-0.09894791, -0.04727853, -0.01004983, -0.30325216],\n",
       "        [ 0.01705559, -0.16948727, -0.08829505, -0.16453639],\n",
       "        [-0.07230186, -0.15348263, -0.06832955, -0.09588489],\n",
       "        [-0.1258373 ,  0.02068143,  0.07559443, -0.19079593],\n",
       "        [-0.08558049, -0.07712768, -0.07924994, -0.07352428]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5044d110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_softmax = tf.nn.softmax(output_array)\n",
    "output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4122a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape = tf.reshape(output_softmax, [-1, 4])\n",
    "output_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d31fcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0.24628553, 0.282701  , 0.2385324 , 0.23248109],\n",
       "        [0.2298986 , 0.23856457, 0.25392184, 0.27761498],\n",
       "        [0.2101039 , 0.2444843 , 0.26824066, 0.27717113],\n",
       "        [0.22202027, 0.29728216, 0.2501223 , 0.23057525],\n",
       "        [0.24084595, 0.2724257 , 0.25000373, 0.23672463]],\n",
       "\n",
       "       [[0.23988546, 0.27957046, 0.23915865, 0.24138539],\n",
       "        [0.24975686, 0.27135593, 0.24479878, 0.23408844],\n",
       "        [0.24278015, 0.26340333, 0.24202275, 0.25179377],\n",
       "        [0.23139507, 0.262904  , 0.25970972, 0.2459912 ],\n",
       "        [0.2601803 , 0.25627998, 0.24524413, 0.23829558]]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(tf.nn.softmax(output_array), [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01c293c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.cast(tf.equal(y, tf.reduce_max(y,1,keep_dims=True)), y.dtype)\n",
    "output_hard = tf.cast(tf.equal(output_reshape, tf.math.reduce_max(output_reshape, 1, keepdims=True)), output_softmax.dtype)\n",
    "output_hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7703002f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5, 4), dtype=float32, numpy=\n",
       "array([[[0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(output_hard, [-1, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "constitutional-coral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 5])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(output_array, perm=[0, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "prompt-distinction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.advanced_activations.Softmax at 0x26f14e934c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers.Softmax(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "treated-flash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fifth-citizen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302585092994046"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac75a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
