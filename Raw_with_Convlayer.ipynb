{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8961e9",
   "metadata": {},
   "source": [
    "# 1. Data Gnerator\n",
    "- Raw data를 읽어와 세그먼트로 나누고 각 배치별로 패딩해주는 부분\n",
    "- 여기서 만들어진 데이터는 모델의 입력으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b8cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawDataGenerator(Sequence):\n",
    "    def __init__(self, Mix, wav_dir, files, batch_size=10, shuffle=True):\n",
    "        self.Mix = Mix\n",
    "        self.wav_dir = wav_dir\n",
    "        self.files = files\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.Mix))\n",
    "        \n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __audioread__(self, path, offset=0.0, duration=None, sample_rate=16000):\n",
    "        signal = librosa.load(path, sr=sample_rate, mono=False, offset=offset, duration=duration)\n",
    "\n",
    "        return signal[0]\n",
    "    \n",
    "    def __padding__(self, data):\n",
    "        n_batch = len(data)\n",
    "        max_len = max([d.shape[0] for d in data])\n",
    "        pad = np.zeros((n_batch, max_len, data[0].shape[1]))\n",
    "        \n",
    "        for i in range(n_batch):\n",
    "            pad[i, :data[i].shape[0]] = data[i]\n",
    "        \n",
    "        return pad\n",
    "        \n",
    "    def __data_generation__(self, Mix_list):\n",
    "        sample_rate = 8000\n",
    "        L = 40\n",
    "        \n",
    "        mix_wav_list = []\n",
    "        label_wav_list = []\n",
    "        for name in Mix_list:\n",
    "            name = name.strip('\\n')\n",
    "            \n",
    "            mix_wav_name = self.wav_dir + self.files + '/mix/' + name\n",
    "            s1_wav_name = self.wav_dir + self.files + '/s1/' + name\n",
    "            s2_wav_name = self.wav_dir + self.files + '/s2/' + name\n",
    "            \n",
    "            # ------- AUDIO READ -------\n",
    "            mix_wav = (self.__audioread__(mix_wav_name, offset=0.0, duration=None, sample_rate=sample_rate))\n",
    "            s1_wav = (self.__audioread__(s1_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate))\n",
    "            s2_wav = (self.__audioread__(s2_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate))\n",
    "            # --------------------------\n",
    "            \n",
    "            # ------- TIME AXIS CALCULATE -------\n",
    "            K = int(np.ceil(len(mix_wav) / L))\n",
    "            # -----------------------------------\n",
    "            \n",
    "            # ------- PADDING -------\n",
    "            pad_len = K * L\n",
    "            pad_mix = np.concatenate([mix_wav, np.zeros([pad_len - len(mix_wav)])])\n",
    "            pad_s1 = np.concatenate([s1_wav, np.zeros([pad_len - len(s1_wav)])])\n",
    "            pad_s2 = np.concatenate([s2_wav, np.zeros([pad_len - len(s2_wav)])])\n",
    "            # -----------------------\n",
    "            \n",
    "            # ------- RESHAPE -------\n",
    "            mix = np.reshape(pad_mix, [K, L])\n",
    "            s1 = np.reshape(pad_s1, [K, L])\n",
    "            s2 = np.reshape(pad_s2, [K, L])\n",
    "            # -----------------------\n",
    "            \n",
    "            # ------- CONCAT S1 S2 -------\n",
    "            s = np.concatenate([s1, s2], axis=-1) # [K x 2L]\n",
    "            # ----------------------------\n",
    "            \n",
    "            mix_wav_list.append(mix)\n",
    "            label_wav_list.append(s)\n",
    "        \n",
    "        return mix_wav_list, label_wav_list\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.Mix) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        Mix_list = [self.Mix[k] for k in indexes]\n",
    "        \n",
    "        if self.files is not 'tt':\n",
    "            mix, labels = self.__data_generation__(Mix_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in mix])\n",
    "            tiled = np.tile(np.expand_dims(lengths, 1), [1, labels[0].shape[1]])\n",
    "            tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            mix_pad = self.__padding__(mix) # [Batch, Time_step, Dimension]\n",
    "            label_pad = self.__padding__(labels) # [Batch, Time_step, Dimension * 2]\n",
    "            \n",
    "            return mix_pad, np.concatenate([label_pad, tiled], axis=1)\n",
    "        else:\n",
    "            mix, labels = self.__data_generation__(Mix_list)\n",
    "            \n",
    "            # Get Lengths(K value of each batch)\n",
    "            lengths = np.array([m.shape[0] for m in mix])\n",
    "            tiled = np.tile(np.expand_dims(lengths, 1), [1, labels[0].shape[1]])\n",
    "            tiled = np.expand_dims(tiled, 1)\n",
    "            \n",
    "            # Padding\n",
    "            mix_pad = self.__padding__(mix) # [Batch, Time_step, Dimension]\n",
    "            \n",
    "            return mix_pad, tiled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e28a0",
   "metadata": {},
   "source": [
    "## Data를 어떻게 읽는지에 대한 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "analyzed-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "WAV_DIR = './mycode/wsj0_2mix/use_this/'\n",
    "LIST_DIR = './mycode/wsj0_2mix/use_this/lists/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tropical-offense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "# Directory List file create\n",
    "\n",
    "wav_dir = WAV_DIR\n",
    "output_lst = LIST_DIR\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "addressed-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "train_dataset = 0\n",
    "valid_dataset = 0\n",
    "test_dataset = 0\n",
    "\n",
    "name_list = []\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    # --- Lead lst file ---\n",
    "    output_lst_files = LIST_DIR + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    # ---------------------\n",
    "    \n",
    "    if files == 'tr':\n",
    "        train_dataset = RawDataGenerator(lines, WAV_DIR, files, batch_size)\n",
    "    elif files == 'cv':\n",
    "        valid_dataset = RawDataGenerator(lines, WAV_DIR, files, batch_size)\n",
    "    else:\n",
    "        test_dataset = RawDataGenerator(lines, WAV_DIR, files, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26871488",
   "metadata": {},
   "source": [
    "## 2. Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89ab227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys, os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, LSTM, Concatenate, Multiply, Bidirectional, Dropout, Conv1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d31a56c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181f55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39577cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44cb53ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom PIT loss\n",
    "\n",
    "# def pit_with_outputsize(output_size):\n",
    "#     def pit_loss(y_true, y_pred):\n",
    "#         ori_length = tf.shape(y_true)[1]\n",
    "        \n",
    "#         # Label & Length divide\n",
    "#         labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "#         lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "#         # Label value slice\n",
    "#         labels1 = tf.slice(labels, [0, 0, 0], [-1, -1, output_size])\n",
    "#         labels2 = tf.slice(labels, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "#         # Predict value slice\n",
    "#         pred1 = tf.slice(y_pred, [0, 0, 0], [-1, -1, output_size])\n",
    "#         pred2 = tf.slice(y_pred, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "#         # Permute calculate\n",
    "#         cost1 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred1 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred2 - labels2, 2), 1), 1)\n",
    "#         cost2 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred2 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred1 - labels2, 2), 1), 1)\n",
    "\n",
    "#         idx = tf.cast(cost1 > cost2, tf.float32) \n",
    "#         pit_loss = tf.reduce_sum(idx * cost2 + (1 - idx) * cost1)\n",
    "        \n",
    "#         return pit_loss\n",
    "    \n",
    "#     return pit_loss\n",
    "\n",
    "def pit_with_outputsize(output_size):\n",
    "    def pit_loss(y_true, y_pred):\n",
    "        ori_length = tf.shape(y_true)[1]\n",
    "\n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, ori_length-1, -1]) # [batch_size, length_size, 129]\n",
    "        lengths = tf.slice(y_true, [0, ori_length-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "\n",
    "        mask = tf.cast(tf.sequence_mask(tf.squeeze(lengths), tf.shape(y_pred)[1]), tf.float32)\n",
    "        mask = tf.expand_dims(mask, axis=-1)\n",
    "        mask = tf.tile(mask, [1, 1, output_size])\n",
    "        print(tf.shape(mask))\n",
    "\n",
    "        # Label value slice\n",
    "        labels1 = tf.slice(labels, [0, 0, 0], [-1, -1, output_size])\n",
    "        labels2 = tf.slice(labels, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Predict value slice\n",
    "        pred1 = tf.slice(y_pred, [0, 0, 0], [-1, -1, output_size])\n",
    "        pred2 = tf.slice(y_pred, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Masking\n",
    "        mask_pred1 = pred1 * mask\n",
    "        mask_pred2 = pred2 * mask\n",
    "\n",
    "        # Permute calculate (batch, seqlen, 258) mask = (batch, seq_len)\n",
    "        cost1 = tf.reduce_sum(tf.pow(mask_pred1 - labels1, 2), 1) + tf.reduce_sum(tf.pow(mask_pred2 - labels2, 2), 1)\n",
    "        cost1 = tf.reduce_sum(cost1, 1) / tf.squeeze(lengths)\n",
    "        cost2 = tf.reduce_sum(tf.pow(mask_pred2 - labels1, 2), 1) + tf.reduce_sum(tf.pow(mask_pred1 - labels2, 2), 1)\n",
    "        cost2 = tf.reduce_sum(cost2, 1) / tf.squeeze(lengths)\n",
    "\n",
    "        idx = tf.cast(cost1 > cost2, tf.float32) \n",
    "        pit_loss = tf.reduce_sum(idx * cost2 + (1 - idx) * cost1)\n",
    "\n",
    "        return pit_loss\n",
    "        \n",
    "    return pit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4b1de7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "\n",
    "def uPIT(input_size, output_size, batch):\n",
    "    inputs = Input(shape=(None, input_size))\n",
    "    \n",
    "    outputs = Conv1D(filters=129, kernel_size=2, activation = 'sigmoid', padding='same')(inputs)\n",
    "    \n",
    "    outputs = Bidirectional(LSTM(129, activation = 'tanh', return_sequences=True),\n",
    "                           input_shape=(None, 129,))(outputs)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(129, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(129, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    \n",
    "    pred1 = Dense(output_size, activation = 'relu')(drop)\n",
    "    pred2 = Dense(output_size, activation = 'relu')(drop)\n",
    "    \n",
    "    cleaned1 = Multiply()([pred1, inputs])\n",
    "    cleaned2 = Multiply()([pred2, inputs])\n",
    "    \n",
    "    model = Concatenate()([cleaned1, cleaned2])\n",
    "    \n",
    "    model = keras.Model(inputs, model)\n",
    "    \n",
    "    model.summary()\n",
    "    adam = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    model.compile(loss=pit_with_outputsize(output_size), optimizer=adam)\n",
    "#     model.compile(loss=keras.losses.mean_squared_error, optimizer=adam)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dc2239",
   "metadata": {},
   "source": [
    "## 3. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf06c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 40)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 129)    10449       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, None, 258)    267288      conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 258)    0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 258)    400416      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 258)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 258)    400416      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 258)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 40)     10360       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 40)     10360       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, None, 40)     0           dense[0][0]                      \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 40)     0           dense_1[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 80)     0           multiply[0][0]                   \n",
      "                                                                 multiply_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,099,289\n",
      "Trainable params: 1,099,289\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "Tensor(\"pit_loss/Shape_2:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "Tensor(\"pit_loss/Shape_2:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5722Tensor(\"pit_loss/Shape_2:0\", shape=(3,), dtype=int32, device=/job:localhost/replica:0/task:0/device:CPU:0)\n",
      "2/2 [==============================] - 14s 5s/step - loss: 0.5722 - val_loss: 0.4886\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48857, saving model to ./CKPT\\CKP_ep_1__loss_0.48857_.h5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 3s/step - loss: 0.5361 - val_loss: 0.4411\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48857 to 0.44110, saving model to ./CKPT\\CKP_ep_2__loss_0.44110_.h5\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.5229 - val_loss: 0.4237\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.44110 to 0.42372, saving model to ./CKPT\\CKP_ep_3__loss_0.42372_.h5\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.5076 - val_loss: 0.4162\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42372 to 0.41617, saving model to ./CKPT\\CKP_ep_4__loss_0.41617_.h5\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.4964 - val_loss: 0.4087\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.41617 to 0.40866, saving model to ./CKPT\\CKP_ep_5__loss_0.40866_.h5\n"
     ]
    }
   ],
   "source": [
    "# Training part\n",
    "\n",
    "epoch = 5\n",
    "L = 40\n",
    "OUTPUT_SIZE = 40\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     model = load_model('./CKPT/CKP_ep_29__loss_102.63367_.h5', custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "    \n",
    "    model = uPIT(L, OUTPUT_SIZE, BATCH_SIZE)\n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809f9735",
   "metadata": {},
   "source": [
    "## 4. Training and validation loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3fb2b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and validation loss graph\n",
    "\n",
    "def graph_util(history):\n",
    "    fig = plt.figure()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.plot(history.history['loss'], c='b')\n",
    "    plt.plot(history.history['val_loss'], c='r')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training loss', 'validation loss'], loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-synthesis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
