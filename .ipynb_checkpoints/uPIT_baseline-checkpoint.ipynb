{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-imperial",
   "metadata": {},
   "source": [
    "# Make wav list\n",
    "- 파일을 tr, cv, tt 폴더별로 무지성으로 읽어온 다음 각 폴더별 list를 .lst 파일로 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sound-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate wav file to .lst done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "tr_wav.lst format:\n",
    "...\n",
    "447o030v_0.1232_050c0109_-0.1232.wav\n",
    "447o030v_1.7882_444o0310_-1.7882.wav\n",
    "447o030w_0.52605_446o030e_-0.52605.wav\n",
    "447o030w_1.9272_420c0203_-1.9272.wav\n",
    "447o030x_0.03457_441c0209_-0.03457.wav\n",
    "447o030x_0.70879_420o0307_-0.70879.wav\n",
    "447o030x_0.98832_441o0308_-0.98832.wav\n",
    "447o030x_1.4783_422o030p_-1.4783.wav\n",
    "...\n",
    "\"\"\"\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "list_dir = './mycode/wsj0_2mix/use_this/lists/'\n",
    "\n",
    "wav_dir = wav_dir\n",
    "output_lst = list_dir\n",
    "\n",
    "for folder in ['tr', 'cv', 'tt']:\n",
    "    wav_files = os.listdir(wav_dir + folder + '/mix')\n",
    "    output_lst_files = output_lst + folder + '_wav.lst'\n",
    "    with open(output_lst_files, 'w') as f:\n",
    "        for file in wav_files:\n",
    "            f.write(file + \"\\n\")\n",
    "\n",
    "print(\"Generate wav file to .lst done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-desperate",
   "metadata": {},
   "source": [
    "# Make TFRecord file\n",
    "\n",
    "- 여기서는 위에서 만든 리스트 파일을 가지고 tfrecord data로 변환함\n",
    "- 이 섹션의 맨 아래 블럭의 코드가 본 코드인데, 읽어들인 raw data를 stft하고, stft한거를 입력으로 세팅함\n",
    "- 그리고 mix된 stft data를 통해서 label들을 뽑아내고, 따로 gender값도 읽어들임\n",
    "- 이렇게 얻게된 3개의 값(mix_stft, labels, gender)을 TFRecord 형식으로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eastern-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "from scipy import signal\n",
    "import argparse\n",
    "import os, sys\n",
    "from numpy.fft import rfft, irfft\n",
    "from scipy.io.wavfile import write as wav_write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "severe-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_p(path):\n",
    "    \"\"\" Creates a path recursively without throwing an error if it already exists\n",
    "    :param path: path to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entitled-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_axis(a, length, overlap=0, axis=None, end='cut', endvalue=0):\n",
    "    \"\"\"Generate a new array that chops the given array along the given axis into overlapping frames.\n",
    "    example:\n",
    "    >>> segment_axis(np.arange(10), 4, 2)\n",
    "    array([[0, 1, 2, 3],\n",
    "           [2, 3, 4, 5],\n",
    "           [4, 5, 6, 7],\n",
    "           [6, 7, 8, 9]])\n",
    "    arguments:\n",
    "    a       The array to segment\n",
    "    length  The length of each frame\n",
    "    overlap The number of array elements by which the frames should overlap\n",
    "    axis    The axis to operate on; if None, act on the flattened array\n",
    "    end     What to do with the last frame, if the array is not evenly\n",
    "            divisible into pieces. Options are:\n",
    "            'cut'   Simply discard the extra values\n",
    "            'wrap'  Copy values from the beginning of the array\n",
    "            'pad'   Pad with a constant value\n",
    "    endvalue    The value to use for end='pad'\n",
    "    The array is not copied unless necessary (either because it is\n",
    "    unevenly strided and being flattened or because end is set to\n",
    "    'pad' or 'wrap').\n",
    "    \"\"\"\n",
    "\n",
    "    if axis is None:\n",
    "        a = np.ravel(a)  # may copy\n",
    "        axis = 0\n",
    "\n",
    "    l = a.shape[axis]\n",
    "\n",
    "    if overlap >= length: raise ValueError(\n",
    "        \"frames cannot overlap by more than 100%\")\n",
    "    if overlap < 0 or length <= 0: raise ValueError(\n",
    "        \"overlap must be nonnegative and length must be positive\")\n",
    "\n",
    "    if l < length or (l - length) % (length - overlap):\n",
    "        if l > length:\n",
    "            roundup = length + (1 + (l - length) // (length - overlap)) * (\n",
    "                    length - overlap)\n",
    "            rounddown = length + ((l - length) // (length - overlap)) * (\n",
    "                    length - overlap)\n",
    "        else:\n",
    "            roundup = length\n",
    "            rounddown = 0\n",
    "        assert rounddown < l < roundup\n",
    "        assert roundup == rounddown + (length - overlap) or (\n",
    "                roundup == length and rounddown == 0)\n",
    "        a = a.swapaxes(-1, axis)\n",
    "\n",
    "        if end == 'cut':\n",
    "            a = a[..., :rounddown]\n",
    "        elif end in ['pad', 'wrap']:  # copying will be necessary\n",
    "            s = list(a.shape)\n",
    "            s[-1] = roundup\n",
    "            b = np.empty(s, dtype=a.dtype)\n",
    "            b[..., :l] = a\n",
    "            if end == 'pad':\n",
    "                b[..., l:] = endvalue\n",
    "            elif end == 'wrap':\n",
    "                b[..., l:] = a[..., :roundup - l]\n",
    "            a = b\n",
    "\n",
    "        a = a.swapaxes(-1, axis)\n",
    "\n",
    "    l = a.shape[axis]\n",
    "    if l == 0: raise ValueError(\n",
    "        \"Not enough data points to segment array in 'cut' mode; try 'pad' or 'wrap'\")\n",
    "    assert l >= length\n",
    "    assert (l - length) % (length - overlap) == 0\n",
    "    n = 1 + (l - length) // (length - overlap)\n",
    "    s = a.strides[axis]\n",
    "    newshape = a.shape[:axis] + (n, length) + a.shape[axis + 1:]\n",
    "    newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "\n",
    "    if not a.flags.contiguous:\n",
    "        a = a.copy()\n",
    "        newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)\n",
    "\n",
    "    try:\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)\n",
    "    except TypeError or ValueError:\n",
    "        warnings.warn(\"Problem with ndarray creation forces copy.\")\n",
    "        a = a.copy()\n",
    "        # Shape doesn't change but strides does\n",
    "        newstrides = a.strides[:axis] + ((length - overlap) * s, s) + a.strides[axis + 1:]\n",
    "        return np.ndarray.__new__(np.ndarray, strides=newstrides, shape=newshape, buffer=a, dtype=a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pleased-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _samples_to_stft_frames(samples, size, shift):\n",
    "    \"\"\"\n",
    "    Calculates STFT frames from samples in time domain.\n",
    "    :param samples: Number of samples in time domain.\n",
    "    :param size: FFT size.\n",
    "    :param shift: Hop in samples.\n",
    "    :return: Number of STFT frames.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.ceil((float(samples) - size + shift) / shift).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stft_frames_to_samples(frames, size, shift):\n",
    "    \"\"\"\n",
    "    Calculates samples in time domain from STFT frames\n",
    "    :param frames: Number of STFT frames.\n",
    "    :param size: FFT size.\n",
    "    :param shift: Hop in samples.\n",
    "    :return: Number of samples in time domain.\n",
    "    \"\"\"\n",
    "    return frames * shift + size - shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "handled-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(time_signal, time_dim=None, size=1024, shift=256,\n",
    "         window=signal.blackman, fading=True, window_length=None):\n",
    "    \"\"\"\n",
    "    Calculates the short time Fourier transformation of a multi channel multi\n",
    "    speaker time signal. It is able to add additional zeros for fade-in and\n",
    "    fade out and should yield an STFT signal which allows perfect\n",
    "    reconstruction.\n",
    "    :param time_signal: multi channel time signal.\n",
    "    :param time_dim: Scalar dim of time.\n",
    "        Default: None means the biggest dimension\n",
    "    :param size: Scalar FFT-size.\n",
    "    :param shift: Scalar FFT-shift. Typically shift is a fraction of size.\n",
    "    :param window: Window function handle.\n",
    "    :param fading: Pads the signal with zeros for better reconstruction.\n",
    "    :param window_length: Sometimes one desires to use a shorter window than\n",
    "        the fft size. In that case, the window is padded with zeros.\n",
    "        The default is to use the fft-size as a window size.\n",
    "    :return: Single channel complex STFT signal\n",
    "        with dimensions frames times size/2+1.\n",
    "    \"\"\"\n",
    "    if time_dim is None:\n",
    "        time_dim = np.argmax(time_signal.shape)\n",
    "\n",
    "    # Pad with zeros to have enough samples for the window function to fade.\n",
    "    if fading:\n",
    "        pad = [(0, 0)] * time_signal.ndim\n",
    "        pad[time_dim] = [size - shift, size - shift]\n",
    "        time_signal = np.pad(time_signal, pad, mode='constant')\n",
    "\n",
    "    # Pad with trailing zeros, to have an integral number of frames.\n",
    "    frames = _samples_to_stft_frames(time_signal.shape[time_dim], size, shift)\n",
    "    samples = _stft_frames_to_samples(frames, size, shift)\n",
    "    pad = [(0, 0)] * time_signal.ndim\n",
    "    pad[time_dim] = [0, samples - time_signal.shape[time_dim]]\n",
    "    time_signal = np.pad(time_signal, pad, mode='constant')\n",
    "    \n",
    "\n",
    "    if window_length is None:\n",
    "        window = window(size)\n",
    "    else:\n",
    "        window = window(window_length)\n",
    "        window = np.pad(window, (0, size - window_length), mode='constant')\n",
    "\n",
    "    time_signal_seg = segment_axis(time_signal, size,\n",
    "                                   size - shift, axis=time_dim)\n",
    "\n",
    "    letters = string.ascii_lowercase\n",
    "    mapping = letters[:time_signal_seg.ndim] + ',' + letters[time_dim + 1] \\\n",
    "              + '->' + letters[:time_signal_seg.ndim]\n",
    "\n",
    "    return rfft(np.einsum(mapping, time_signal_seg, window), axis=time_dim + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioread(path, offset=0.0, duration=None, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Reads a wav file, converts it to 32 bit float values and reshapes accoring\n",
    "    to the number of channels.\n",
    "    Now, this is a wrapper of librosa with our common defaults.\n",
    "    :param path: Absolute or relative file path to audio file.\n",
    "    :type: String.\n",
    "    :param offset: Begin of loaded audio.\n",
    "    :type: Scalar in seconds.\n",
    "    :param duration: Duration of loaded audio.\n",
    "    :type: Scalar in seconds.\n",
    "    :param sample_rate: Sample rate of audio\n",
    "    :type: scalar in number of samples per second\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    signal = librosa.load(path, sr=sample_rate, mono=False, offset=offset, duration=duration)\n",
    "    \n",
    "    return signal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "yellow-jackson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tr_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/cv_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_0.62948_441c0212_-0.62948.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_1.3388_22ho010i_-1.3388.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0302_2.1067_422o030k_-2.1067.tfrecords\n",
      "INFO:tensorflow:Writing utterance ./mycode/tfrecords/tt_tfrecord\\447o0303_0.14144_441c0212_-0.14144.tfrecords\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import librosa.display\n",
    "from math import ceil\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "list_dir = './mycode/wsj0_2mix/use_this/lists/'\n",
    "tfrecord_dir = './mycode/tfrecords/'\n",
    "gender_list = './wsj0-train-spkrinfo.txt'\n",
    "process_num = 8\n",
    "\n",
    "CASE = 'mixed' # mixed or signal\n",
    "\n",
    "mkdir_p(tfrecord_dir) # tfrecord_dir 폴더 만드는 코드\n",
    "sample_rate = 8000\n",
    "window_size = 256\n",
    "window_shift = 128\n",
    "\n",
    "# if gender_list is not '':\n",
    "#     apply_gender_info = True\n",
    "#     gender_dict = {}\n",
    "#     fid = open(gender_list, 'r')\n",
    "#     lines = fid.readlines()\n",
    "#     fid.close()\n",
    "#     for line in lines:\n",
    "#         spk = line.strip('\\n').split(' ')[0]\n",
    "#         gender = line.strip('\\n').split(' ')[1]\n",
    "#         if gender.lower() == 'm':\n",
    "#             gender_dict[spk] = 1\n",
    "#         else:\n",
    "#             gender_dict[spk] = 0\n",
    "\n",
    "\n",
    "def max_length(file, name, mix_or_not):\n",
    "    max_len = 0\n",
    "    \n",
    "    for name in lines:\n",
    "        name = name.strip('\\n')\n",
    "\n",
    "        wav_name = wav_dir + file + '/' + mix_or_not + '/' + name\n",
    "        audio_wav = audioread(wav_name, offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        mix_len = len(audio_wav)\n",
    "\n",
    "        if mix_len > max_len:\n",
    "            max_len = mix_len\n",
    "    \n",
    "    # 초 맞춰주는 부분\n",
    "    max_len = ceil(max_len / sample_rate) * sample_rate\n",
    "    \n",
    "    return max_len\n",
    "\n",
    "\n",
    "def make_sequence_example(inputs, labels, length, name, genders=False):\n",
    "    input_features = [tf.train.Feature(float_list=tf.train.FloatList(value=input_)) for input_ in inputs]\n",
    "    label_features = [tf.train.Feature(float_list=tf.train.FloatList(value=label)) for label in labels]\n",
    "    len_feature = [tf.train.Feature(float_list=tf.train.FloatList(value=[length]))]\n",
    "    name_feature = [tf.train.Feature(bytes_list=tf.train.BytesList(value=[name.encode('utf-8')]))]\n",
    "#     gender_features = [tf.train.Feature(float_list=tf.train.FloatList(value=genders))]\n",
    "    \n",
    "    feature_list = {\n",
    "        'inputs': tf.train.FeatureList(feature=input_features),\n",
    "        'labels': tf.train.FeatureList(feature=label_features),\n",
    "        'length': tf.train.FeatureList(feature=len_feature),\n",
    "        'name' : tf.train.FeatureList(feature=name_feature)\n",
    "#         'genders': tf.train.FeatureList(feature=gender_features)\n",
    "    }\n",
    "    feature_lists = tf.train.FeatureLists(feature_list=feature_list)\n",
    "    \n",
    "    return tf.train.SequenceExample(feature_lists=feature_lists)\n",
    "\n",
    "\n",
    "def gen_feats(wav_name, sample_rate, window_size, window_shift, file, max_len, case='mixed'):\n",
    "    mix_wav_name = wav_dir + file + '/mix/' + wav_name\n",
    "    s1_wav_name  = wav_dir + file + '/s1/' + wav_name\n",
    "    s2_wav_name  = wav_dir + file + '/s2/' + wav_name\n",
    "\n",
    "    # value initiallization\n",
    "    mix_wav = 0\n",
    "    s1_wav = 0\n",
    "    s2_wav = 0\n",
    "    mix_stft = 0\n",
    "    s1_stft = 0\n",
    "    s2_stft = 0\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        # ------- AUDIO READ -------\n",
    "        mix_wav = audioread(mix_wav_name, offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s1_wav  = audioread(s1_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s2_wav  = audioread(s2_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        # --------------------------\n",
    "        \n",
    "        # ------- AUDIO PAD -------\n",
    "        mix_wav_pad = np.pad(mix_wav, (0, max_len - len(mix_wav)), 'constant', constant_values=(0))\n",
    "        s1_wav_pad = np.pad(s1_wav, (0, max_len - len(s1_wav)), 'constant', constant_values=(0))\n",
    "        s2_wav_pad = np.pad(s2_wav, (0, max_len - len(s2_wav)), 'constant', constant_values=(0))\n",
    "        # -------------------------\n",
    "\n",
    "        # ------- STFT -------\n",
    "        mix_stft = stft(mix_wav, time_dim=0, size=window_size, shift=window_shift)\n",
    "        \n",
    "        mix_stft_pad = stft(mix_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s1_stft_pad = stft(s1_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft_pad = stft(s2_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        # --------------------\n",
    "        \n",
    "        part_name = os.path.splitext(wav_name)[0]\n",
    "        tfrecords_name = tfrecord_dir + file + '_tfrecord\\\\' + part_name + '.tfrecords'\n",
    "        \n",
    "        with tf.io.TFRecordWriter(tfrecords_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_name)\n",
    "\n",
    "            mix_abs = np.abs(mix_stft_pad)\n",
    "            mix_angle = np.angle(mix_stft_pad)\n",
    "\n",
    "            s1_abs = np.abs(s1_stft_pad)\n",
    "            s1_angle = np.angle(s1_stft_pad)\n",
    "\n",
    "            s2_abs = np.abs(s2_stft_pad)\n",
    "            s2_angle = np.angle(s2_stft_pad)\n",
    "\n",
    "            inputs = np.concatenate((mix_abs, mix_angle), axis=1)\n",
    "            labels = np.concatenate((s1_abs * np.cos(mix_angle - s1_angle), s2_abs * np.cos(mix_angle - s2_angle)), axis=1)\n",
    "            \n",
    "            ex = make_sequence_example(inputs, labels, mix_stft.shape[0], part_name)\n",
    "            writer.write(ex.SerializeToString())\n",
    "    else:\n",
    "        # ------- AUDIO READ -------\n",
    "        s1_wav  = audioread(s1_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        s2_wav  = audioread(s2_wav_name,  offset=0.0, duration=None, sample_rate=sample_rate)\n",
    "        # --------------------------\n",
    "        \n",
    "        # ------- AUDIO PAD -------\n",
    "        s1_wav_pad = np.pad(s1_wav, (0, max_len - len(s1_wav)), 'constant', constant_values=(0))\n",
    "        s2_wav_pad = np.pad(s2_wav, (0, max_len - len(s2_wav)), 'constant', constant_values=(0))\n",
    "        # -------------------------\n",
    "        \n",
    "        # ------- STFT -------\n",
    "        s1_stft  = stft(s1_wav,  time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft  = stft(s2_wav,  time_dim=0, size=window_size, shift=window_shift)\n",
    "        \n",
    "        s1_stft_pad = stft(s1_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        s2_stft_pad = stft(s2_wav_pad, time_dim=0, size=window_size, shift=window_shift)\n",
    "        # --------------------\n",
    "        \n",
    "        part_name = os.path.splitext(wav_name)[0]\n",
    "        tfrecords_s1_name = tfrecord_dir + file + '_one_source_tfrecord\\\\' + part_name + '_s1.tfrecords'\n",
    "        tfrecords_s2_name = tfrecord_dir + file + '_one_source_tfrecord\\\\' + part_name + '_s2.tfrecords'\n",
    "        \n",
    "        with tf.io.TFRecordWriter(tfrecords_s1_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_s1_name)\n",
    "            \n",
    "            s1_abs = np.abs(s1_stft_pad)\n",
    "            s1_angle = np.angle(s1_stft_pad)\n",
    "            \n",
    "            ex = make_sequence_example(s1_abs, s1_angle, s1_stft.shape[0], part_name + '_s1')\n",
    "            writer.write(ex.SerializeToString())\n",
    "\n",
    "        with tf.io.TFRecordWriter(tfrecords_s2_name) as writer:\n",
    "            tf.compat.v1.logging.info(\"Writing utterance %s\" %tfrecords_s2_name)\n",
    "            \n",
    "            s2_abs = np.abs(s2_stft_pad)\n",
    "            s2_angle = np.angle(s2_stft_pad)\n",
    "            \n",
    "            ex = make_sequence_example(s2_abs, s2_angle, s2_stft.shape[0], part_name + '_s2')\n",
    "            writer.write(ex.SerializeToString())\n",
    "\n",
    "\n",
    "# 여기 멀티프로세싱 pool 적용 어케하는지 모르게씀\n",
    "for files in ['tr', 'cv', 'tt']:\n",
    "    max_len = 0\n",
    "    \n",
    "    output_lst_files = list_dir + files + '_wav.lst'\n",
    "    fid = open(output_lst_files, 'r')\n",
    "    lines = fid.readlines()\n",
    "    fid.close()\n",
    "    \n",
    "    \n",
    "    if CASE == 'mixed':\n",
    "        for name in lines:\n",
    "            name = name.strip('\\n')\n",
    "            max_len = max_length(files, name, 'mix')\n",
    "    else:\n",
    "        for name in lines:\n",
    "            name = name.strip('\\n')\n",
    "            max_len1 = max_length(files, name, 's1')\n",
    "            max_len2 = max_length(files, name, 's2')\n",
    "        \n",
    "        if max_len1 >= max_len2:\n",
    "            max_len = max_len1\n",
    "        else:\n",
    "            max_len = max_len2\n",
    "    \n",
    "    \n",
    "    mkdir_p(tfrecord_dir + files + '_tfrecord') # tfrecord_dir 폴더 만드는 코드\n",
    "    mkdir_p(tfrecord_dir + files + '_one_source_tfrecord') # one_source_tfrecord 폴더 만드는 코드\n",
    "    for name in lines:\n",
    "        name = name.strip('\\n')\n",
    "        gen_feats(name, sample_rate, window_size, window_shift, files, max_len, CASE)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-cuisine",
   "metadata": {},
   "source": [
    "# Deep learning part\n",
    "## 1. Data Loader\n",
    "- Data를 시바 읽어오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "urban-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 2\n",
    "INPUT_SIZE = 129\n",
    "OUTPUT_SIZE = 129\n",
    "\n",
    "CASE = 'mixed' # mixed or signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "guilty-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "if CASE == 'mixed':\n",
    "    tr_path = './mycode/tfrecords/tr_tfrecord/*.tfrecords'\n",
    "    val_path = './mycode/tfrecords/cv_tfrecord/*.tfrecords'\n",
    "    tt_path = './mycode/tfrecords/tt_tfrecord/*.tfrecords'\n",
    "else:\n",
    "    tr_path = './mycode/tfrecords/tr_one_source_tfrecord/*.tfrecords'\n",
    "    val_path = './mycode/tfrecords/cv_one_source_tfrecord/*.tfrecords'\n",
    "    tt_path = './mycode/tfrecords/tt_one_source_tfrecord/*.tfrecords'\n",
    "\n",
    "FILENAMES_TRAINING = tf.io.gfile.glob(tr_path)\n",
    "FILENAMES_VALIDATION = tf.io.gfile.glob(val_path)\n",
    "FILENAMES_TEST = tf.io.gfile.glob(tt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "satellite-system",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TFRecord Files: 4\n",
      "Validation TFRecord Files: 4\n",
      "Test TFRecord Files: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_0.62948_441c0212_-0.62948.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_1.3388_22ho010i_-1.3388.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0302_2.1067_422o030k_-2.1067.tfrecords',\n",
       " '.\\\\mycode\\\\tfrecords\\\\tt_tfrecord\\\\447o0303_0.14144_441c0212_-0.14144.tfrecords']"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train TFRecord Files:\", len(FILENAMES_TRAINING))\n",
    "print(\"Validation TFRecord Files:\", len(FILENAMES_VALIDATION))\n",
    "print(\"Test TFRecord Files:\", len(FILENAMES_TEST))\n",
    "FILENAMES_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "economic-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data, check, input_size=129*2, output_size=129*2):\n",
    "    if check == 'inputs':\n",
    "        inputs = tf.slice(data, [0, 0], [-1, input_size//2])\n",
    "        angle = tf.slice(data, [0, input_size//2], [-1, -1])\n",
    "        \n",
    "        return inputs, angle\n",
    "    \n",
    "    elif check == 'labels':\n",
    "        label1 = tf.slice(data, [0, 0], [-1, output_size//2])\n",
    "        label2 = tf.slice(data, [0, output_size//2], [-1, -1])\n",
    "        \n",
    "        return label1, label2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "historic-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord(example, input_size=129*2, output_size=129*2, case='mixed'):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'inputs': tf.io.FixedLenSequenceFeature(shape=[input_size], dtype=tf.float32),\n",
    "            'labels': tf.io.FixedLenSequenceFeature(shape=[output_size], dtype=tf.float32),\n",
    "            'length': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32),\n",
    "            'name': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.string),\n",
    "#             'genders': tf.io.FixedLenSequenceFeature(shape=[2], dtype=tf.float32, allow_missing=True)\n",
    "        }\n",
    "    )\n",
    "    _, example = tf.io.parse_single_sequence_example(example, sequence_features=tfrecord_format)\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        inputs, angle = data_preprocessing(example[\"inputs\"], 'inputs', input_size)\n",
    "    #     label1, label2 = data_preprocessing(example[\"labels\"], 'labels', input_size)\n",
    "\n",
    "        tiled = tf.tile(tf.expand_dims(example['length'], 1), [1, input_size])\n",
    "        \n",
    "        # 여기가 변경된 부분(length와 label을 concat 힘)\n",
    "        return inputs, tf.concat([example['labels'], tiled], 0)\n",
    "#         return inputs, example['labels']\n",
    "    \n",
    "    else:\n",
    "        return example['inputs'], example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "gross-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tfrecord_test(example, input_size=129*2, output_size=129*2, case='mixed'):\n",
    "    tfrecord_format = (\n",
    "        {\n",
    "            'inputs': tf.io.FixedLenSequenceFeature(shape=[input_size], dtype=tf.float32),\n",
    "            'labels': tf.io.FixedLenSequenceFeature(shape=[output_size], dtype=tf.float32),\n",
    "            'length': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.float32),\n",
    "            'name': tf.io.FixedLenSequenceFeature(shape=[], dtype=tf.string),\n",
    "#             'genders': tf.io.FixedLenSequenceFeature(shape=[2], dtype=tf.float32, allow_missing=True)\n",
    "        }\n",
    "    )\n",
    "    _, example = tf.io.parse_single_sequence_example(example, sequence_features=tfrecord_format)\n",
    "    \n",
    "    if case == 'mixed':\n",
    "        inputs, angle = data_preprocessing(example[\"inputs\"], 'inputs', input_size)\n",
    "\n",
    "        return inputs, angle, example['labels'], example['name']\n",
    "    \n",
    "    else:\n",
    "        return example['inputs'], example['labels'], example['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "agricultural-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filenames, input_size=129*2, output_size=129*2, check='train', case='mixed'):\n",
    "    ignore_order = tf.data.Options()\n",
    "    \n",
    "    if check == 'train':\n",
    "        ignore_order.experimental_deterministic = False  # disable order, increase speed\n",
    "    else:\n",
    "        ignore_order.experimental_deterministic = True\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(\n",
    "        filenames\n",
    "    )  # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(\n",
    "        ignore_order\n",
    "    )  # uses data as soon as it streams in, rather than in its original order\n",
    "    \n",
    "    if check == 'train':\n",
    "        if case == 'mixed':\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord, input_size=input_size, output_size=output_size), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord, input_size=input_size//2, output_size=output_size//2, case=case), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "    else:\n",
    "        if case == 'mixed':\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord_test, input_size=input_size, output_size=output_size), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "        else:\n",
    "            dataset = dataset.map(\n",
    "                partial(read_tfrecord_test, input_size=input_size//2, output_size=output_size//2, case=case), num_parallel_calls=AUTOTUNE\n",
    "            )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "interracial-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(filenames, input_size=129*2, output_size=129*2):\n",
    "    dataset = load_dataset(filenames, input_size, output_size, case=CASE)\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(None))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "essential-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_for_test(filenames, input_size=129*2, output_size=129*2):\n",
    "    dataset = load_dataset(filenames, input_size, output_size, check='test', case=CASE)\n",
    "    dataset = dataset.repeat(1)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "#     dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(None))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "comprehensive-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(FILENAMES_TRAINING, INPUT_SIZE*2, OUTPUT_SIZE*2)\n",
    "valid_dataset = get_dataset(FILENAMES_VALIDATION, INPUT_SIZE*2, OUTPUT_SIZE*2)\n",
    "\n",
    "test_dataset = get_dataset_for_test(FILENAMES_TEST, INPUT_SIZE*2, OUTPUT_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "breathing-shareware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 627, 258) (2, 626, 258) (2, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "a, b = next(iter(train_dataset))\n",
    "c1 = tf.slice(b, [0, 0, 0], [-1, b.shape[1]-1, -1])\n",
    "d1 = tf.slice(b, [0, b.shape[1]-1, 0], [-1, -1, 1])\n",
    "\n",
    "# tf.reduce_mean(tf.cast(tf.math.equal(lab, c1), dtype=tf.float32))\n",
    "print(b.shape, c1.shape, d1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-grenada",
   "metadata": {},
   "source": [
    "## 2. Building model\n",
    "\n",
    "- 이제 우리 모델을 시바 개쩔게 만들자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "surface-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, sys, os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, LSTM, Concatenate, Multiply, Bidirectional, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "sunrise-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./CKPT/') # model check point 폴더 만드는 코드\n",
    "filepath = \"./CKPT/CKP_ep_{epoch:d}__loss_{val_loss:.5f}_.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "qualified-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom PIT loss\n",
    "\n",
    "def pit_with_outputsize(output_size):\n",
    "    def pit_loss(y_true, y_pred):\n",
    "        # Label & Length divide\n",
    "        labels = tf.slice(y_true, [0, 0, 0], [-1, 627-1, -1]) # [batch_size, length_size, 129]\n",
    "        lengths = tf.slice(y_true, [0, 627-1, 0], [-1, -1, 1]) # [batch_size, 1, 1]\n",
    "        \n",
    "        # Label value slice\n",
    "        labels1 = tf.slice(labels, [0, 0, 0], [-1, -1, output_size])\n",
    "        labels2 = tf.slice(labels, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Predict value slice\n",
    "        pred1 = tf.slice(y_pred, [0, 0, 0], [-1, -1, output_size])\n",
    "        pred2 = tf.slice(y_pred, [0, 0, output_size], [-1, -1, -1])\n",
    "\n",
    "        # Permute calculate\n",
    "        cost1 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred1 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred2 - labels2, 2), 1), 1)\n",
    "        cost2 = tf.reduce_mean(tf.reduce_sum(tf.pow(pred2 - labels1, 2), 1) + tf.reduce_sum(tf.pow(pred1 - labels2, 2), 1), 1)\n",
    "\n",
    "        idx = tf.cast(cost1 > cost2, tf.float32) \n",
    "        pit_loss = tf.reduce_sum(idx * cost2 + (1 - idx) * cost1)\n",
    "        \n",
    "        return pit_loss\n",
    "    \n",
    "    return pit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "colonial-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "\n",
    "def uPIT(input_size, output_size, batch):\n",
    "    inputs = Input(shape=(None, input_size))\n",
    "    \n",
    "    outputs = Dense(496, activation = 'tanh')(inputs)\n",
    "    \n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True),\n",
    "                           input_shape=(None, 496,))(outputs)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    outputs = Bidirectional(LSTM(496, activation = 'tanh', return_sequences=True))(drop)\n",
    "    drop = Dropout(rate=0.8)(outputs)\n",
    "    \n",
    "    pred1 = Dense(output_size, activation = 'relu')(drop)\n",
    "    pred2 = Dense(output_size, activation = 'relu')(drop)\n",
    "    \n",
    "    cleaned1 = Multiply()([pred1, inputs])\n",
    "    cleaned2 = Multiply()([pred2, inputs])\n",
    "    \n",
    "    model = Concatenate()([cleaned1, cleaned2])\n",
    "    \n",
    "    model = keras.Model(inputs, model)\n",
    "    \n",
    "    model.summary()\n",
    "    adam = tf.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss=pit_with_outputsize(output_size), optimizer=adam)\n",
    "#     model.compile(loss=keras.losses.mean_squared_error, optimizer=adam)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-jumping",
   "metadata": {},
   "source": [
    "## 3. Training model\n",
    "- 구축한 모델을 기반으로 딥러닝 학습을 진행하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "ordered-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "\n",
    "# learning rate를 점점 줄이는 부분\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "# validation loss에 대해서 좋은 것만 저장됨\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    ")\n",
    "\n",
    "# early stop 하는 부분인데, validation loss에 대해서 제일 좋은 모델이 저장됨\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', mode='min', verbose=1, patience=50, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "dress-issue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "장치의 수: 1\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, None, 129)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, None, 496)    64480       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional (None, None, 992)    3940224     dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, None, 992)    0           bidirectional_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_28 (Bidirectional (None, None, 992)    5908352     dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, None, 992)    0           bidirectional_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional (None, None, 992)    5908352     dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, None, 992)    0           bidirectional_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, None, 129)    128097      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, None, 129)    128097      dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_18 (Multiply)          (None, None, 129)    0           dense_28[0][0]                   \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_19 (Multiply)          (None, None, 129)    0           dense_29[0][0]                   \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, 258)    0           multiply_18[0][0]                \n",
      "                                                                 multiply_19[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 16,077,602\n",
      "Trainable params: 16,077,602\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 48s 24s/step - loss: 508.5292 - val_loss: 430.0889\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 430.08887, saving model to ./CKPT\\CKP_ep_1__loss_430.08887_.h5\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 39s 22s/step - loss: 472.1697 - val_loss: 408.5830\n",
      "\n",
      "Epoch 00002: val_loss improved from 430.08887 to 408.58304, saving model to ./CKPT\\CKP_ep_2__loss_408.58304_.h5\n"
     ]
    }
   ],
   "source": [
    "# Training part\n",
    "\n",
    "epoch = 2\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('장치의 수: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "with strategy.scope():\n",
    "    # 사용 안할 때는 load_model 주석 처리 하자\n",
    "#     model = load_model('./CKPT/CKP_ep_29__loss_102.63367_.h5', custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "    \n",
    "    model = uPIT(INPUT_SIZE, OUTPUT_SIZE, BATCH_SIZE)\n",
    "    tf.executing_eagerly()\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epoch,\n",
    "    validation_data=valid_dataset,\n",
    "    shuffle=True,\n",
    "    callbacks=[checkpoint_cb, early_stopping_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-boutique",
   "metadata": {},
   "source": [
    "## 4. Training and validation loss plot\n",
    "- 학습한 모델의 loss값을 그래프로 그려봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "southeast-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training and validation loss graph\n",
    "\n",
    "def graph_util(history):\n",
    "    fig = plt.figure()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(18.5, 10.5)\n",
    "    plt.plot(history.history['loss'], c='b')\n",
    "    plt.plot(history.history['val_loss'], c='r')\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training loss', 'validation loss'], loc='upper left')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "important-disney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1332x756 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_util(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-royal",
   "metadata": {},
   "source": [
    "## 5. Test Model\n",
    "- 학습된 모델을 사용해서 테스트를 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handed-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "window_size = 256\n",
    "window_shift = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dramatic-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('./test_wav/') # Result wav 폴더 만드는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "attempted-playing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _biorthogonal_window_loopy(analysis_window, shift):\n",
    "    \"\"\"\n",
    "    This version of the synthesis calculation is as close as possible to the\n",
    "    Matlab impelementation in terms of variable names.\n",
    "    The results are equal.\n",
    "    The implementation follows equation A.92 in\n",
    "    Krueger, A. Modellbasierte Merkmalsverbesserung zur robusten automatischen\n",
    "    Spracherkennung in Gegenwart von Nachhall und Hintergrundstoerungen\n",
    "    Paderborn, Universitaet Paderborn, Diss., 2011, 2011\n",
    "    \"\"\"\n",
    "    fft_size = len(analysis_window)\n",
    "    assert np.mod(fft_size, shift) == 0\n",
    "    number_of_shifts = len(analysis_window) // shift\n",
    "\n",
    "    sum_of_squares = np.zeros(shift)\n",
    "    for synthesis_index in range(0, shift):\n",
    "        for sample_index in range(0, number_of_shifts + 1):\n",
    "            analysis_index = synthesis_index + sample_index * shift\n",
    "\n",
    "            if analysis_index + 1 < fft_size:\n",
    "                sum_of_squares[synthesis_index] \\\n",
    "                    += analysis_window[analysis_index] ** 2\n",
    "\n",
    "    sum_of_squares = np.kron(np.ones(number_of_shifts), sum_of_squares)\n",
    "    synthesis_window = analysis_window / sum_of_squares / fft_size\n",
    "    return synthesis_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "north-cedar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def istft(stft_signal, size=1024, shift=256,\n",
    "          window=signal.blackman, fading=True, window_length=None):\n",
    "    \"\"\"\n",
    "    Calculated the inverse short time Fourier transform to exactly reconstruct\n",
    "    the time signal.\n",
    "    :param stft_signal: Single channel complex STFT signal\n",
    "        with dimensions frames times size/2+1.\n",
    "    :param size: Scalar FFT-size.\n",
    "    :param shift: Scalar FFT-shift. Typically shift is a fraction of size.\n",
    "    :param window: Window function handle.\n",
    "    :param fading: Removes the additional padding, if done during STFT.\n",
    "    :param window_length: Sometimes one desires to use a shorter window than\n",
    "        the fft size. In that case, the window is padded with zeros.\n",
    "        The default is to use the fft-size as a window size.\n",
    "    :return: Single channel complex STFT signal\n",
    "    :return: Single channel time signal.\n",
    "    \"\"\"\n",
    "    assert stft_signal.shape[1] == size // 2 + 1\n",
    "\n",
    "    if window_length is None:\n",
    "        window = window(size)\n",
    "    else:\n",
    "        window = window(window_length)\n",
    "        window = np.pad(window, (0, size - window_length), mode='constant')\n",
    "\n",
    "    window = _biorthogonal_window_loopy(window, shift)\n",
    "\n",
    "    # Why? Line created by Hai, Lukas does not know, why it exists.\n",
    "    window *= size\n",
    "    time_signal = scipy.zeros(stft_signal.shape[0] * shift + size - shift)\n",
    "\n",
    "    for j, i in enumerate(range(0, len(time_signal) - size + shift, shift)):\n",
    "        time_signal[i:i + size] += window * np.real(irfft(stft_signal[j]))\n",
    "\n",
    "    # Compensate fade-in and fade-out\n",
    "    if fading:\n",
    "        time_signal = time_signal[size - shift:len(time_signal) - (size - shift)]\n",
    "\n",
    "    return time_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "detailed-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiowrite(data, path, samplerate=16000, normalize=False, threaded=True):\n",
    "    \"\"\" Write the audio data ``data`` to the wav file ``path``\n",
    "    The file can be written in a threaded mode. In this case, the writing\n",
    "    process will be started at a separate thread. Consequently, the file will\n",
    "    not be written when this function exits.\n",
    "    :param data: A numpy array with the audio data\n",
    "    :param path: The wav file the data should be written to\n",
    "    :param samplerate: Samplerate of the audio data\n",
    "    :param normalize: Normalize the audio first so that the values are within\n",
    "        the range of [INTMIN, INTMAX]. E.g. no clipping occurs\n",
    "    :param threaded: If true, the write process will be started as a separate\n",
    "        thread\n",
    "    :return: The number of clipped samples\n",
    "    \"\"\"\n",
    "    data = data.copy()\n",
    "    int16_max = np.iinfo(np.int16).max\n",
    "    int16_min = np.iinfo(np.int16).min\n",
    "\n",
    "    if normalize:\n",
    "        if not data.dtype.kind == 'f':\n",
    "            data = data.astype(np.float)\n",
    "        data /= np.max(np.abs(data))\n",
    "\n",
    "    if data.dtype.kind == 'f':\n",
    "        data *= int16_max\n",
    "\n",
    "    sample_to_clip = np.sum(data > int16_max)\n",
    "    if sample_to_clip > 0:\n",
    "        print('Warning, clipping {} samples'.format(sample_to_clip))\n",
    "    data = np.clip(data, int16_min, int16_max)\n",
    "    data = data.astype(np.int16)\n",
    "\n",
    "    if threaded:\n",
    "        threading.Thread(target=wav_write, args=(path, samplerate, data)).start()\n",
    "    else:\n",
    "        wav_write(path, samplerate, data)\n",
    "\n",
    "    return sample_to_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "textile-patch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6236: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  _ctx, \"Mul\", name, x, y)\n",
      "c:\\users\\moon\\anaconda3\\envs\\study\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: scipy.zeros is deprecated and will be removed in SciPy 2.0.0, use numpy.zeros instead\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    model_path = './CKPT/CKP_ep_2__loss_411.15891_.h5'\n",
    "    model = load_model(model_path, custom_objects={'pit_loss': pit_with_outputsize(OUTPUT_SIZE)})\n",
    "\n",
    "    cnt = 0\n",
    "    check = 0\n",
    "    for batch in test_dataset:\n",
    "        input_batch, angle_batch, label_batch, name = batch\n",
    "        tf.executing_eagerly() # requires r1.7\n",
    "        angle_numpy = tf.constant(angle_batch)\n",
    "        angle_numpy = angle_numpy.numpy()\n",
    "\n",
    "        result = model.predict(input_batch)\n",
    "        label1 = tf.slice(result, [0, 0, 0], [-1, -1, OUTPUT_SIZE])\n",
    "        label2 = tf.slice(result, [0, 0, OUTPUT_SIZE], [-1, -1, -1])\n",
    "        spec1 = label1 * np.exp(angle_numpy * 1j)\n",
    "        spec2 = label2 * np.exp(angle_numpy * 1j)\n",
    "\n",
    "        num = cnt * BATCH_SIZE\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if i >= input_batch.shape[0]:\n",
    "                check = -1\n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                wav_name = name[i][0].numpy().decode('utf-8')\n",
    "\n",
    "                wav_name1 = './test_wav/' + wav_name + '_s1.wav'\n",
    "                wav_name2 = './test_wav/' + wav_name + '_s2.wav'\n",
    "                wav1 = istft(spec1[i, 0:input_batch[i].shape[0], :], size=window_size, shift=window_shift)\n",
    "                wav2 = istft(spec2[i, 0:input_batch[i].shape[0], :], size=window_size, shift=window_shift)\n",
    "                audiowrite(wav1, wav_name1, sample_rate, True, True)\n",
    "                audiowrite(wav2, wav_name2, sample_rate, True, True)\n",
    "        \n",
    "        if check == -1:\n",
    "            break\n",
    "\n",
    "        if (cnt + 1) % 10 == 0:\n",
    "            print((cnt + 1) * BATCH_SIZE)\n",
    "\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-africa",
   "metadata": {},
   "source": [
    "- 원본(ref)과 모델을 통해 만들어진 파일(est)을 SI-SDR SDR과 같이 정확도를 비교한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "natural-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "agreed-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = './mycode/wsj0_2mix/use_this/'\n",
    "test_dir = './test_wav/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liable-arrangement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SI-SDR (db) : -7.791285775601864\n",
      "The SDR (db) : -2.3582835316785915\n"
     ]
    }
   ],
   "source": [
    "si_sdr = evaluate_metrics.eval_si_sdr(wav_dir, test_dir)\n",
    "sdr = evaluate_metrics.eval_sdr(wav_dir, test_dir)\n",
    "\n",
    "print(\"The SI-SDR (db) :\", si_sdr)\n",
    "print(\"The SDR (db) :\", sdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-naples",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
